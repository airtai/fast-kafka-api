{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FastKafkaAPI","text":"<p>Effortless Kafka integration for your web services</p> <p> </p> <p> </p> <p></p> <p>FastKafkaAPI is a powerful and easy-to-use Python library for building asynchronous web services that interact with Kafka topics. Built on top of FastAPI, Starlette, Pydantic, and AIOKafka, FastKafkaAPI simplifies the process of writing producers and consumers for Kafka topics, handling all the parsing, networking, and task scheduling automatically. With FastKafkaAPI, you can quickly prototype and develop high-performance Kafka-based services with minimal code, making it an ideal choice for developers looking to streamline their workflow and accelerate their projects.</p>"},{"location":"#install","title":"Install","text":"<p>This command installs the FastKafkaAPI package from the Python Package Index (PyPI) using the pip package manager.</p> <p><code>pip</code> is a command-line tool that allows you to install and manage Python packages, including FastKafkaAPI. When you run the <code>pip install</code> command with the name of a package (in this case, \u201cfast-kafka-api\u201d), pip will download the package from PyPI, along with any dependencies that the package requires, and install it on your system.</p> <p>After running this command, you will be able to import and use the FastKafkaAPI package in your Python code. For example, you might use it to initialize a FastKafkaAPI application, as shown in the example bellow, and to use the <code>@consumes</code> and <code>@produces</code> decorators to define Kafka consumers and producers in your application.</p> <p>Installing FastKafkaAPI from PyPI using <code>pip</code> is the recommended way to install the package, as it makes it easy to manage the package and its dependencies. If you prefer, you can also install FastKafkaAPI from the source code by cloning the repository and running <code>pip install .</code> in the root directory of the project.</p> <pre><code>pip install fast-kafka-api\n</code></pre>"},{"location":"#how-to-use","title":"How to use","text":"<p>Here is an example python script using FastKafkaAPI that takes data from an input Kafka topic, makes a prediction using a predictive model, and outputs the prediction to an output Kafka topic.</p>"},{"location":"#messages","title":"Messages","text":"<p>FastKafkaAPI uses Pydantic to parse input JSON-encoded data into Python objects, making it easy to work with structured data in your Kafka-based applications. Pydantic\u2019s <code>BaseModel</code> class allows you to define messages using a declarative syntax, making it easy to specify the fields and types of your messages.</p> <p>This example defines two message classes for use in a FastKafkaAPI application: <code>InputData</code> and <code>Prediction</code>.</p> <p>The <code>InputData</code> class is used to represent input data for a predictive model. It has three fields: <code>user_id</code>, <code>feature_1</code>, and <code>feature_2</code>. The <code>user_id</code> field is of type <code>NonNegativeInt</code>, which is a subclass of int that only allows non-negative integers. The <code>feature_1</code> and <code>feature_2</code> fields are both lists of floating-point numbers and integers, respectively. These fields are used to represent input features for the predictive model.</p> <p>The <code>Prediction</code> class is used to represent the output of the predictive model. It has two fields: <code>user_id</code> and <code>score</code>. The <code>user_id</code> field is of type <code>NonNegativeInt</code>, and the <code>score</code> field is a floating-point number. The <code>score</code> field represents the prediction made by the model, such as the probability of churn in the next 28 days.</p> <p>These message classes will be used to parse and validate incoming data in Kafka consumers and producers. Using these message classes in combination with FastKafkaAPI makes it easy to work with structured data in your Kafka-based applications.</p> <pre><code>from typing import List\n\nfrom pydantic import BaseModel, Field, NonNegativeInt\n\n\nclass InputData(BaseModel):\n    user_id: NonNegativeInt = Field(..., example=202020, description=\"ID of a user\")\n    feature_1: List[float] = Field(\n        ...,\n        example=[1.2, 2.3, 4.5, 6.7, 0.1],\n        description=\"input feature 1\",\n    )\n    feature_2: List[int] = Field(\n        ...,\n        example=[2, 4, 3, 1, 0],\n        description=\"input feature 2\",\n    )\n\n\nclass Prediction(BaseModel):\n    user_id: NonNegativeInt = Field(..., example=202020, description=\"ID of a user\")\n    score: float = Field(\n        ...,\n        example=0.4321,\n        description=\"Prediction score (e.g. the probability of churn in the next 28 days)\",\n        ge=0.0,\n        le=1.0,\n    )\n</code></pre> <p>These message classes will be used to parse and validate incoming data in a Kafka consumer and to produce a JSON-encoded message in a producer. Using Pydantic\u2019s BaseModel in combination with FastKafkaAPI makes it easy to work with structured data in your Kafka-based applications.</p>"},{"location":"#application","title":"Application","text":"<p>This example shows how to initialize a FastKafkaAPI application. It starts by defining two environment variables: <code>KAFKA_HOSTNAME</code> and <code>KAFKA_PORT</code>, which are used to specify the hostname and port of the Kafka broker.</p> <p>Next, it defines a dictionary called <code>kafka_brokers</code>, which contains two entries: \u201clocalhost\u201d and \u201cproduction\u201d. Each entry specifies the URL, port, and other details of a Kafka broker. This dictionary is used to define the available Kafka brokers that can be used in the application.</p> <p>The <code>kafka_config</code> dictionary specifies the configuration options for the Kafka broker, such as the <code>bootstrap_servers</code> setting, which specifies the hostname and port of the Kafka broker.</p> <p>Finally, the FastKafkaAPI class is initialized with several arguments: <code>title</code>, <code>contact</code>, <code>version</code>, <code>description</code>, <code>kafka_brokers</code>, and <code>kafka_config</code>. These arguments are used to configure various aspects of the application, such as the title, version, and description of the application, as well as the available Kafka brokers and the Kafka configuration options. The resulting <code>FastKafkaAPI</code> object, which is stored in the <code>app</code> variable, represents the initialized FastKafkaAPI application.</p> <pre><code>from os import environ\n\nfrom fastapi import FastAPI\nfrom fast_kafka_api.application import FastKafkaAPI\n\nkafka_server_url = environ[\"KAFKA_HOSTNAME\"]\nkafka_server_port = environ[\"KAFKA_PORT\"]\n\nkafka_brokers = {\n    \"localhost\": {\n        \"url\": \"kafka\",\n        \"description\": \"local development kafka broker\",\n        \"port\": 9092,\n    },\n    \"production\": {\n        \"url\": \"kafka.acme.com\",\n        \"description\": \"production kafka broker\",\n        \"port\": 9092,\n        \"protocol\": \"kafka-secure\",\n        \"security\": {\"type\": \"plain\"},\n    },\n}\n\nkafka_config = {\n    \"bootstrap_servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n}\n\napp = FastAPI(\n    title=\"FastKafkaAPI Example\",\n    contact={\"name\": \"airt.ai\", \"url\": \"https://airt.ai\", \"email\": \"info@airt.ai\"},\n    version=\"0.0.1\",\n    description=\"A simple example on how to use FastKafkaAPI\",\n)\n\nkafka_app = FastKafkaAPI(\n    app,\n    kafka_brokers=kafka_brokers,\n    **kafka_config,\n)\n</code></pre>"},{"location":"#function-decorators","title":"Function decorators","text":"<p>FastKafkaAPI provides convenient function decorators called <code>@consumes</code> and <code>@produces</code> to allow you to delegate the actual processing of data to user-defined functions. These decorators make it easy to specify the processing logic for your Kafka consumers and producers, allowing you to focus on the core business logic of your application without worrying about the underlying Kafka integration.</p> <p>This example shows how to use the <code>@consumes</code> and <code>@produces</code> decorators in a FastKafkaAPI application.</p> <p>The <code>@consumes</code> decorator is applied to the <code>on_input_data</code> function, which specifies that this function should be called whenever a message is received on the \u201cinput_data\u201d Kafka topic. The <code>on_input_data</code> function takes a single argument, <code>msg</code>, which is expected to be an instance of the <code>InputData</code> message class.</p> <p>Inside the <code>on_input_data</code> function, the <code>model.predict</code> function is called with the <code>feature_1</code> and <code>feature_2</code> fields from the <code>msg</code> argument. This function returns a prediction score, which is then passed to the <code>to_predictions</code> function along with the <code>user_id</code> field from the <code>msg</code> argument.</p> <p>The <code>@produces</code> decorator is applied to the <code>to_predictions</code> function, which specifies that this function should produce a message to the \u201cpredictions\u201d Kafka topic whenever it is called. The <code>to_predictions</code> function takes two arguments: <code>user_id</code> and <code>score</code>. It creates a new <code>Prediction</code> message with these values and then returns it.</p> <p>In summary, this example shows how to use the <code>@consumes</code> and <code>@produces</code> decorators to specify the processing logic for Kafka consumers and producers in a FastKafkaAPI application. The <code>@consumes</code> decorator is applied to functions that should be called when a message is received on a Kafka topic, and the <code>@produces</code> decorator is applied to functions that should produce a message to a Kafka topic. These decorators make it easy to specify the processing logic for your Kafka consumers and producers, allowing you to focus on the core business logic of your application without worrying about the underlying Kafka integration.</p> <pre><code>@kafka_app.consumes(topic=\"input_data\")\nasync def on_input_data(msg: InputData):\n    print(f\"msg={msg}\")\n    score = await model.predict(feature_1=msg.feature_1, feature_2=msg.feature_2)\n    await to_predictions(user_id=msg.user_id, score=score)\n\n\n@kafka_app.produces(topic=\"predictions\")\nasync def to_predictions(user_id: int, score: float) -&gt; Prediction:\n    prediction = Prediction(user_id=user_id, score=score)\n    print(f\"prediction={prediction}\")\n    return prediction\n</code></pre>"},{"location":"#running-the-service","title":"Running the service","text":"<p>This example shows how to start the FastKafkaAPI service using the uvicorn library. The <code>uvicorn.run</code> function is called with the <code>app</code> argument (which represents the FastKafkaAPI application) and the <code>host</code> and <code>port</code> arguments, which specify the hostname and port on which the service should listen for incoming requests.</p> <p>When the service is started, several log messages are printed to the console, including information about the application startup, AsyncAPI specification generation, and consumer loop status.</p> <p>During the lifetime of the service, incoming requests will be processed by the FastKafkaAPI application and appropriate actions will be taken based on the defined Kafka consumers and producers. For example, if a message is received on the \u201cinput_data\u201d Kafka topic, the <code>on_input_data</code> function will be called to process the message, and if the <code>to_predictions</code> function is called, it will produce a message to the \u201cpredictions\u201d Kafka topic. The service will continue to run until it is shut down, at which point the application shutdown process will be initiated and the service will stop.</p> <pre><code>import uvicorn\n\nuvicorn.run(app, host=\"0.0.0.0\", port=4000)\n</code></pre> <pre><code>INFO:     Started server process [32157]\nINFO:     Waiting for application startup.\n\n[INFO] fast_kafka_api._components.asyncapi: Old async specifications at '/work/fast-kafka-api/nbs/asyncapi/spec/asyncapi.yml' does not exist.\n[INFO] fast_kafka_api._components.asyncapi: New async specifications generated at: 'asyncapi/spec/asyncapi.yml'\n[INFO] fast_kafka_api._components.asyncapi: Async docs generated at 'asyncapi/docs'\n[INFO] fast_kafka_api._components.asyncapi: Output of '$ npx -y -p @asyncapi/generator ag asyncapi/spec/asyncapi.yml @asyncapi/html-template -o asyncapi/docs --force-write'\n\nDone! \u2728\nCheck out your shiny new generated files at /work/fast-kafka-api/nbs/asyncapi/docs.\n\n\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting..\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created.\n\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\n\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'input_data'})\n[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'input_data'}\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'input_data': 1}.\n\nINFO:     Shutting down\nINFO:     Waiting for application shutdown.\n\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n\nINFO:     Application shutdown complete.\nINFO:     Finished server process [32157]\n</code></pre>"},{"location":"000_FastKafkaAPI/","title":"000 FastKafkaAPI","text":"<pre><code>supress_timestamps()\nlogger = get_logger(__name__, level=20)\nlogger.info(\"ok\")\n</code></pre> <pre><code>[INFO] __main__: ok\n</code></pre> <pre><code>import shutil\nimport unittest.mock\nfrom dataclasses import dataclass\n\nimport nest_asyncio\nimport pytest\nimport uvicorn\nimport yaml\nfrom fastapi.testclient import TestClient\nfrom rich.pretty import pprint\nfrom starlette.datastructures import Headers\n\nfrom fast_kafka_api.testing import mock_AIOKafkaProducer_send, true_after\n</code></pre> <pre><code># allows async calls in notebooks\n\nimport nest_asyncio\n</code></pre> <pre><code>nest_asyncio.apply()\n</code></pre> <pre><code>kafka_server_url = environ[\"KAFKA_HOSTNAME\"]\nkafka_server_port = environ[\"KAFKA_PORT\"]\n\nkafka_config = {\n    \"bootstrap_servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n    \"group_id\": f\"{kafka_server_url}:{kafka_server_port}_group\",\n    \"auto_offset_reset\": \"earliest\",\n}\n</code></pre>"},{"location":"000_FastKafkaAPI/#constructor-utilities","title":"Constructor utilities","text":"<pre><code>app = _get_fast_api_app()\nassert isinstance(app, FastAPI)\nprint(\"\\n\".join([f\"{k}={v}\" for k, v in app.__dict__.items()]))\n</code></pre> <pre><code>_debug=False\ntitle=FastAPI\ndescription=\nversion=0.1.0\nterms_of_service=None\ncontact=None\nlicense_info=None\nopenapi_url=/openapi.json\nopenapi_tags=None\nroot_path_in_servers=True\ndocs_url=/docs\nredoc_url=/redoc\nswagger_ui_oauth2_redirect_url=/docs/oauth2-redirect\nswagger_ui_init_oauth=None\nswagger_ui_parameters=None\nservers=[]\nextra={}\nopenapi_version=3.0.2\nopenapi_schema=None\nroot_path=\nstate=&lt;starlette.datastructures.State object&gt;\ndependency_overrides={}\nrouter=&lt;fastapi.routing.APIRouter object&gt;\nexception_handlers={&lt;class 'starlette.exceptions.HTTPException'&gt;: &lt;function http_exception_handler&gt;, &lt;class 'fastapi.exceptions.RequestValidationError'&gt;: &lt;function request_validation_exception_handler&gt;}\nuser_middleware=[]\nmiddleware_stack=&lt;starlette.middleware.errors.ServerErrorMiddleware object&gt;\n</code></pre> <pre><code>f = _get_func_with_combined_sig([AIOKafkaConsumer.__init__, AIOKafkaProducer.__init__])\nassert len(signature(f).parameters.keys()) == 48\n</code></pre> <pre><code>f = _get_func_with_combined_sig([AIOKafkaConsumer.__init__, AIOKafkaProducer.__init__], keep=True)\nassert len(signature(f).parameters.keys()) == 49\n</code></pre> <pre><code>assert _get_kafka_config() == {\n    \"bootstrap_servers\": \"localhost:9092\",\n    \"auto_offset_reset\": \"earliest\",\n    \"max_poll_records\": 100,\n}\n\nassert _get_kafka_config(max_poll_records=1_000) == {\n    \"bootstrap_servers\": \"localhost:9092\",\n    \"auto_offset_reset\": \"earliest\",\n    \"max_poll_records\": 1_000,\n}\n</code></pre> <pre><code>with pytest.raises(ValueError) as e:\n    _get_kafka_config(random_key=1_000, whatever=\"whocares\")\nassert e.value.args == (\"Unallowed key arguments passed: 'random_key', 'whatever'\",)\n</code></pre> <pre><code>assert (\n    _get_kafka_brokers(None).json()\n    == '{\"brokers\": {\"localhost\": {\"url\": \"https://localhost\", \"description\": \"Local (dev) Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n)\n\nassert (\n    _get_kafka_brokers(dict(localhost=dict(url=\"localhost\"))).json()\n    == '{\"brokers\": {\"localhost\": {\"url\": \"localhost\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n)\n\nassert (\n    _get_kafka_brokers(\n        dict(localhost=dict(url=\"localhost\"), staging=dict(url=\"staging.airt.ai\"))\n    ).json()\n    == '{\"brokers\": {\"localhost\": {\"url\": \"localhost\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}, \"staging\": {\"url\": \"staging.airt.ai\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n)\n</code></pre> <pre><code>def on_topic_name_1():\n    pass\n\n\nassert _get_topic_name(on_topic_name_1) == \"topic_name_1\"\n\nassert _get_topic_name(on_topic_name_1, prefix=\"on_topic_\") == \"name_1\"\n</code></pre> <pre><code>_get_contact_info()\n</code></pre> <pre><code>ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')\n</code></pre> <p>source</p>"},{"location":"000_FastKafkaAPI/#fastkafkaapi","title":"FastKafkaAPI","text":"<pre><code> FastKafkaAPI (fast_api_app:fastapi.applications.FastAPI,\n               asyncapi_route:Optional[str]='/asyncapi',\n               title:Optional[str]=None, description:Optional[str]=None,\n               version:Optional[str]=None,\n               contact:Optional[Dict[str,str]]=None,\n               kafka_brokers:Optional[Dict[str,Any]]=None,\n               root_path:Union[pathlib.Path,str,NoneType]=None, loop=None,\n               bootstrap_servers='localhost', client_id='aiokafka-0.8.0',\n               group_id=None, key_deserializer=None,\n               value_deserializer=None, fetch_max_wait_ms=500,\n               fetch_max_bytes=52428800, fetch_min_bytes=1,\n               max_partition_fetch_bytes=1048576,\n               request_timeout_ms=40000, retry_backoff_ms=100,\n               auto_offset_reset='latest', enable_auto_commit=True,\n               auto_commit_interval_ms=5000, check_crcs=True,\n               metadata_max_age_ms=300000,\n               partition_assignment_strategy=(&lt;class 'kafka.coordinator.as\n               signors.roundrobin.RoundRobinPartitionAssignor'&gt;,),\n               max_poll_interval_ms=300000, rebalance_timeout_ms=None,\n               session_timeout_ms=10000, heartbeat_interval_ms=3000,\n               consumer_timeout_ms=200, max_poll_records=None,\n               ssl_context=None, security_protocol='PLAINTEXT',\n               api_version='auto', exclude_internal_topics=True,\n               connections_max_idle_ms=540000,\n               isolation_level='read_uncommitted', sasl_mechanism='PLAIN',\n               sasl_plain_password=None, sasl_plain_username=None,\n               sasl_kerberos_service_name='kafka',\n               sasl_kerberos_domain_name=None,\n               sasl_oauth_token_provider=None, acks=&lt;object object at\n               0x7fa65c7e6ff0&gt;, key_serializer=None,\n               value_serializer=None, compression_type=None,\n               max_batch_size=16384,\n               partitioner=&lt;kafka.partitioner.default.DefaultPartitioner\n               object at 0x7fa65a8bd280&gt;, max_request_size=1048576,\n               linger_ms=0, send_backoff_ms=100, enable_idempotence=False,\n               transactional_id=None, transaction_timeout_ms=60000)\n</code></pre> <p>Combined REST and Kafka service</p> <p>Params: fast_api_app: the FastAPI app, if None, one will be created asyncapi_route: the route to where to mount the documentation. If None, the docs will not be mounted. title: optional title for the documentation. If None, the title of passed fast_api_app will be used description: optional description for the documentation. If None, the description of passed fast_api_app will be used version: optional version for the documentation. If None, the version of passed fast_api_app will be used contact: optional contact for the documentation. If None, the contact of passed fast_api_app will be used kafka_brokers: dictionary describing kafka brokers used for generating documentation root_path: path to where documentation will be created</p> <pre><code>app = FastAPI()\nkafka_app = FastKafkaAPI(app)\nkafka_app.__dict__\n</code></pre> <pre><code>{'_fast_api_app': &lt;fastapi.applications.FastAPI&gt;,\n '_title': 'FastAPI',\n '_description': '',\n '_version': '0.1.0',\n '_contact_info': ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com'),\n '_kafka_service_info': KafkaServiceInfo(title='FastAPI', version='0.1.0', description='', contact=ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')),\n '_kafka_brokers': KafkaBrokers(brokers={'localhost': KafkaBroker(url='https://localhost', description='Local (dev) Kafka broker', port='9092', protocol='kafka', security=None)}),\n '_root_path': PosixPath('.'),\n '_kafka_config': {'bootstrap_servers': 'localhost:9092',\n  'auto_offset_reset': 'earliest',\n  'max_poll_records': 100},\n '_consumers_store': {},\n '_producers_list': [],\n '_producers_store': {},\n '_scheduled_bg_tasks': [],\n '_bg_task_group_generator': None,\n '_on_error_topic': None,\n '_asyncapi_path': PosixPath('asyncapi'),\n '_is_shutting_down': False,\n '_kafka_consumer_tasks': [],\n '_kafka_producer_tasks': []}\n</code></pre> <pre><code>app = FastAPI(contact=\"Davor\")\nkafka_app = FastKafkaAPI(app)\nkafka_app.__dict__\n</code></pre> <pre><code>{'_fast_api_app': &lt;fastapi.applications.FastAPI&gt;,\n '_title': 'FastAPI',\n '_description': '',\n '_version': '0.1.0',\n '_contact_info': ContactInfo(name='Davor', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com'),\n '_kafka_service_info': KafkaServiceInfo(title='FastAPI', version='0.1.0', description='', contact=ContactInfo(name='Davor', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')),\n '_kafka_brokers': KafkaBrokers(brokers={'localhost': KafkaBroker(url='https://localhost', description='Local (dev) Kafka broker', port='9092', protocol='kafka', security=None)}),\n '_root_path': PosixPath('.'),\n '_kafka_config': {'bootstrap_servers': 'localhost:9092',\n  'auto_offset_reset': 'earliest',\n  'max_poll_records': 100},\n '_consumers_store': {},\n '_producers_list': [],\n '_producers_store': {},\n '_scheduled_bg_tasks': [],\n '_bg_task_group_generator': None,\n '_on_error_topic': None,\n '_asyncapi_path': PosixPath('asyncapi'),\n '_is_shutting_down': False,\n '_kafka_consumer_tasks': [],\n '_kafka_producer_tasks': []}\n</code></pre> <pre><code>def create_testing_app():\n    root_path = \"/tmp/000_FastKafkaAPI\"\n    if Path(root_path).exists():\n        shutil.rmtree(root_path)\n\n    app = FastAPI()\n    kafka_app = FastKafkaAPI(\n        app,\n        kafka_brokers={\n            \"local\": {\n                \"url\": \"kafka\",\n                \"name\": \"development\",\n                \"description\": \"Local (dev) Kafka broker\",\n                \"port\": 9092,\n            }\n        },\n        root_path=root_path,\n        **kafka_config,\n    )\n\n    return kafka_app\n</code></pre> <pre><code>app = create_testing_app()\napp\n</code></pre> <pre><code>&lt;__main__.FastKafkaAPI&gt;\n</code></pre> <p>source</p>"},{"location":"000_FastKafkaAPI/#fastkafkaapiconsumes","title":"FastKafkaAPI.consumes","text":"<pre><code> FastKafkaAPI.consumes (topic:Optional[str]=None, prefix:str='on_',\n                        **kwargs:Dict[str,Any])\n</code></pre> <p>Decorator registering the callback called when a message is received in a topic.</p> <p>This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.</p> <p>Params: topic: Kafka topic that the consumer will subscribe to and execute the decorated function when it receives a message from the topic, default: None If the topic is not specified, topic name will be inferred from the decorated function name by stripping the defined prefix prefix: Prefix stripped from the decorated function to define a topic name if the topic argument is not passed, default: \u201con_\u201d If the decorated function name is not prefixed with the defined prefix and topic argument is not passed, then this method will throw ValueError **kwargs: Keyword arguments that will be passed to AIOKafkaConsumer, used to configure the consumer</p> <p>Returns: A function returning the same function</p> <p>Throws: ValueError</p> <pre><code>app = create_testing_app()\n\n# Basic check\n@app.consumes()\ndef on_my_topic_1(msg: BaseModel) -&gt; None:\n    pass\n\n\nassert app._consumers_store[\"my_topic_1\"] == (on_my_topic_1, {}), app._consumers_store\n\n# Check topic setting\n@app.consumes(topic=\"test_topic_2\")\ndef some_func_name(msg: BaseModel) -&gt; None:\n    pass\n\n\nassert app._consumers_store[\"test_topic_2\"] == (\n    some_func_name,\n    {},\n), app._consumers_store\n\n# Check prefix change\n@app.consumes(prefix=\"for_\")\ndef for_test_topic_3(msg: BaseModel) -&gt; None:\n    pass\n\n\nassert app._consumers_store[\"test_topic_3\"] == (\n    for_test_topic_3,\n    {},\n), app._consumers_store\n\n# Check passing of kwargs\nkwargs = {\"arg1\": \"val1\", \"arg2\": 2}\n\n\n@app.consumes(topic=\"test_topic\", **kwargs)\ndef for_test_kwargs(msg: BaseModel):\n    pass\n\n\nassert app._consumers_store[\"test_topic\"] == (\n    for_test_kwargs,\n    kwargs,\n), app._consumers_store\n</code></pre> <pre><code>assert _to_json_utf8({\"a\": 1, \"b\": [2, 3]}) == b'{\"a\": 1, \"b\": [2, 3]}'\n\n\nclass A(BaseModel):\n    name: str = Field()\n    age: int\n\n\nassert _to_json_utf8(A(name=\"Davor\", age=12)) == b'{\"name\": \"Davor\", \"age\": 12}'\n</code></pre> <p>source</p>"},{"location":"000_FastKafkaAPI/#produce_decorator","title":"produce_decorator","text":"<pre><code> produce_decorator (func:Callable[...,Union[Awaitable[pydantic.main.BaseMo\n                    del],pydantic.main.BaseModel]], topic:str)\n</code></pre> <pre><code>async def test_me(is_async: bool):\n    with mock_AIOKafkaProducer_send() as send_mock:\n        topic = \"test_topic\"\n\n        class MockMsg(BaseModel):\n            name: str = \"Micky Mouse\"\n            id: int = 123\n\n        if is_async:\n\n            async def func(mock_msg: MockMsg) -&gt; MockMsg:\n                return mock_msg\n\n        else:\n\n            def func(mock_msg: MockMsg) -&gt; MockMsg:\n                return mock_msg\n\n        producer = AIOKafkaProducer(bootstrap_servers=kafka_config[\"bootstrap_servers\"])\n        if not is_async:\n            producer = AIOKafkaProducerManager(producer)\n\n        await producer.start()\n        try:\n            app = unittest.mock.Mock()\n            app._producers_store = {topic: (func, producer, {})}\n\n            test_func = produce_decorator(app, func, topic)\n            assert iscoroutinefunction(test_func) == is_async\n\n            mock_msg = MockMsg()\n            if not is_async:\n                value = test_func(mock_msg)\n                await asyncio.sleep(1)\n            else:\n                value = await test_func(mock_msg)\n\n            send_mock.assert_called_once_with(topic, mock_msg.json().encode(\"utf-8\"))\n            assert value == mock_msg\n\n        finally:\n            await producer.stop()\n\n\nfor is_async in [True, False]:\n    await test_me(is_async)\n\nprint(\"ok\")\n</code></pre> <pre><code>[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\nok\n</code></pre> <p>source</p>"},{"location":"000_FastKafkaAPI/#fastkafkaapiproduces","title":"FastKafkaAPI.produces","text":"<pre><code> FastKafkaAPI.produces (topic:Optional[str]=None, prefix:str='to_',\n                        producer:aiokafka.producer.producer.AIOKafkaProduc\n                        er=None, **kwargs:Dict[str,Any])\n</code></pre> <p>Decorator registering the callback called when delivery report for a produced message is received</p> <p>This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.</p> <p>Params: topic: Kafka topic that the producer will send returned values from the decorated function to, default: None If the topic is not specified, topic name will be inferred from the decorated function name by stripping the defined prefix prefix: Prefix stripped from the decorated function to define a topic name if the topic argument is not passed, default: \u201cto_\u201d If the decorated function name is not prefixed with the defined prefix and topic argument is not passed, then this method will throw ValueError producer: **kwargs: Keyword arguments that will be passed to AIOKafkaProducer, used to configure the producer</p> <p>Returns: A function returning the same function</p> <p>Throws: ValueError</p> <pre><code>app = create_testing_app()\n\n# Basic check\nasync def to_my_topic_1(msg: BaseModel) -&gt; None:\n    pass\n\n\n# Must be done without sugar to keep the original function reference\ncheck_func = to_my_topic_1\nto_my_topic_1 = app.produces()(to_my_topic_1)\n\nassert app._producers_store[\"my_topic_1\"] == (\n    check_func,\n    None,\n    {},\n), f\"{app._producers_store}, {to_my_topic_1}\"\n\n# Check topic setting\ndef some_func_name(msg: BaseModel) -&gt; None:\n    pass\n\n\ncheck_func = some_func_name\nsome_func_name = app.produces(topic=\"test_topic_2\")(some_func_name)\n\nassert app._producers_store[\"test_topic_2\"] == (\n    check_func,\n    None,\n    {},\n), app._producers_store\n\n# Check prefix change\n@app.produces(prefix=\"for_\")\ndef for_test_topic_3(msg: BaseModel) -&gt; None:\n    pass\n\n\ncheck_func = for_test_topic_3\nsome_func_name = app.produces(prefix=\"for_\")(for_test_topic_3)\n\nassert app._producers_store[\"test_topic_3\"] == (\n    check_func,\n    None,\n    {},\n), app._producers_store\n\n# Check passing of kwargs\nkwargs = {\"arg1\": \"val1\", \"arg2\": 2}\n\n\nasync def for_test_kwargs(msg: BaseModel):\n    pass\n\n\ncheck_func = for_test_kwargs\nfor_test_kwargs = app.produces(topic=\"test_topic\", **kwargs)(for_test_kwargs)\n\nassert app._producers_store[\"test_topic\"] == (\n    check_func,\n    None,\n    kwargs,\n), app._producers_store\n\n# Check passing of custom Producer\nasync def test_me():\n    kwargs = {\"arg1\": \"val1\", \"arg2\": 2}\n\n    async def for_test_producer(msg: BaseModel):\n        pass\n\n    check_func = for_test_producer\n    producer = AIOKafkaProducer()\n    for_test_producer = app.produces(\n        topic=\"test_producer\", producer=producer, **kwargs\n    )(for_test_producer)\n\n    assert app._producers_store[\"test_producer\"] == (\n        check_func,\n        producer,\n        kwargs,\n    ), app._producers_store\n\n\nawait test_me()\n</code></pre> <p>source</p>"},{"location":"000_FastKafkaAPI/#fastkafkaapirun_in_background","title":"FastKafkaAPI.run_in_background","text":"<pre><code> FastKafkaAPI.run_in_background ()\n</code></pre> <p>Decorator to schedule a task to be run in the background.</p> <p>This decorator is used to schedule a task to be run in the background when the app\u2019s <code>_on_startup</code> event is triggered.</p> <p>Returns: Callable[None, None]: A decorator function that takes a background task as an input and stores it to be run in the backround.</p> <pre><code># Check if the background job is getting registered\n\napp = create_testing_app()\n\n@app.run_in_background()\nasync def async_background_job():\n    pass\n\nassert app._scheduled_bg_tasks[0] == async_background_job, app._scheduled_bg_tasks[0]\nassert app._scheduled_bg_tasks.__len__() == 1\n</code></pre> <pre><code>class MyInfo(BaseModel):\n    mobile: str = Field(..., example=\"+385987654321\")\n    name: str = Field(..., example=\"James Bond\")\n\n\nclass MyMsgUrl(BaseModel):\n    info: MyInfo = Field(..., example=dict(mobile=\"+385987654321\", name=\"James Bond\"))\n    url: HttpUrl = Field(..., example=\"https://sis.gov.uk/agents/007\")\n\n\nclass MyMsgEmail(BaseModel):\n    msg_url: MyMsgUrl = Field(\n        ...,\n        example=dict(\n            info=dict(mobile=\"+385987654321\", name=\"James Bond\"),\n            url=\"https://sis.gov.uk/agents/007\",\n        ),\n    )\n    email: EmailStr = Field(..., example=\"agent-007@sis.gov.uk\")\n\n\ndef setup_testing_app():\n    app = create_testing_app()\n\n    @app.consumes(\"my_topic_1\")\n    def on_my_topic_one(msg: MyMsgUrl):\n        logger.debug(f\"on_my_topic_one(msg={msg},)\")\n\n    @app.consumes()\n    async def on_my_topic_2(msg: MyMsgEmail):\n        logger.debug(f\"on_my_topic_2(msg={msg},)\")\n\n    with pytest.raises(ValueError) as e:\n\n        @app.consumes()\n        def my_topic_3(msg: MyMsgEmail):\n            raise NotImplemented\n\n    @app.produces()\n    def to_my_topic_3(url: str) -&gt; MyMsgUrl:\n        logger.debug(f\"on_my_topic_3(msg={url}\")\n        return MyMsgUrl(info=MyInfo(\"+3851987654321\", \"Sean Connery\"), url=url)\n\n    @app.produces()\n    async def to_my_topic_4(msg: MyMsgEmail) -&gt; MyMsgEmail:\n        logger.debug(f\"on_my_topic_4(msg={msg}\")\n        return msg\n\n    @app.produces()\n    def to_my_topic_5(url: str) -&gt; MyMsgUrl:\n        logger.debug(f\"on_my_topic_5(msg={url}\")\n        return MyMsgUrl(info=MyInfo(\"+3859123456789\", \"John Wayne\"), url=url)\n\n    @app.run_in_background()\n    async def long_bg_job():\n        await asyncio.sleep(100) \n\n    return app\n</code></pre> <pre><code>app = setup_testing_app()\n\nassert set(app._consumers_store.keys()) == set([\"my_topic_1\", \"my_topic_2\"])\nassert set(app._producers_store.keys()) == set(\n    [\"my_topic_3\", \"my_topic_4\", \"my_topic_5\"]\n)\n\nprint(f\"app._kafka_service_info={app._kafka_service_info}\")\nprint(f\"app._kafka_brokers={app._kafka_brokers}\")\n</code></pre> <pre><code>app._kafka_service_info=title='FastAPI' version='0.1.0' description='' contact=ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')\napp._kafka_brokers=brokers={'local': KafkaBroker(url='kafka', description='Local (dev) Kafka broker', port='9092', protocol='kafka', security=None)}\n</code></pre> <p>source</p>"},{"location":"000_FastKafkaAPI/#filter_using_signature","title":"filter_using_signature","text":"<pre><code> filter_using_signature (f:Callable, **kwargs:Dict[str,Any])\n</code></pre> <pre><code>def f(a: int, *, b: str):\n    pass\n\n\nassert filter_using_signature(f, a=1, c=3) == {\"a\": 1}\n</code></pre> <pre><code>app = setup_testing_app()\napp._populate_consumers(is_shutting_down_f=true_after(1))\nassert len(app._kafka_consumer_tasks) == 2\n\nawait app._shutdown_consumers()\n\nassert all([t.done() for t in app._kafka_consumer_tasks])\n</code></pre> <pre><code>[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100, 'group_id': 'davor-fast-kafka-api-kafka-1:9092_group'}\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100, 'group_id': 'davor-fast-kafka-api-kafka-1:9092_group'}\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_2'})\n[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_2'}\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_1'})\n[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_1'}\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n[INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: (Re-)joining group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: (Re-)joining group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: Joined group 'davor-fast-kafka-api-kafka-1:9092_group' (generation 35) with member_id aiokafka-0.8.0-153f9135-a8f1-427a-9294-9502ecdffd63\n[INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n[INFO] aiokafka.consumer.group_coordinator: (Re-)joining group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: Joined group 'davor-fast-kafka-api-kafka-1:9092_group' (generation 36) with member_id aiokafka-0.8.0-153f9135-a8f1-427a-9294-9502ecdffd63\n[INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n[INFO] aiokafka.consumer.group_coordinator: Joined group 'davor-fast-kafka-api-kafka-1:9092_group' (generation 36) with member_id aiokafka-0.8.0-89c1de18-adff-4ecd-841d-17d743aed8c6\n[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {'my_topic_2': 1} to {'my_topic_1': 1, 'my_topic_2': 1}. \n[INFO] aiokafka.consumer.group_coordinator: Successfully synced group davor-fast-kafka-api-kafka-1:9092_group with generation 36\n[INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='my_topic_2', partition=0)} for group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: Successfully synced group davor-fast-kafka-api-kafka-1:9092_group with generation 36\n[INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='my_topic_1', partition=0)} for group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n[INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n</code></pre> <pre><code>app = setup_testing_app()\nawait app._populate_producers()\nawait app._shutdown_producers()\nassert len(app._producers_list) == 3\n\napp._producers_list\n</code></pre> <pre><code>[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092'}'\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092'}'\n[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092'}'\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n\n[&lt;fast_kafka_api._components.aiokafka_producer_manager.AIOKafkaProducerManager&gt;,\n &lt;aiokafka.producer.producer.AIOKafkaProducer&gt;,\n &lt;fast_kafka_api._components.aiokafka_producer_manager.AIOKafkaProducerManager&gt;]\n</code></pre> <pre><code>app = setup_testing_app()\nawait app._populate_bg_tasks()\nassert len(app._scheduled_bg_tasks) == 1\nassert app._bg_task_group_generator is not None\nassert app._bg_tasks_group is not None\n\nawait app._shutdown_bg_tasks()\n</code></pre> <p>source</p>"},{"location":"000_FastKafkaAPI/#fastkafkaapigenerate_async_spec","title":"FastKafkaAPI.generate_async_spec","text":"<pre><code> FastKafkaAPI.generate_async_spec ()\n</code></pre> <pre><code>app = setup_testing_app()\napp.generate_async_spec()\n</code></pre> <pre><code>[INFO] fast_kafka_api._components.asyncapi: Old async specifications at '/tmp/000_FastKafkaAPI/asyncapi/spec/asyncapi.yml' does not exist.\n[INFO] fast_kafka_api._components.asyncapi: New async specifications generated at: '/tmp/000_FastKafkaAPI/asyncapi/spec/asyncapi.yml'\n[INFO] fast_kafka_api._components.asyncapi: Async docs generated at '/tmp/000_FastKafkaAPI/asyncapi/docs'\n[INFO] fast_kafka_api._components.asyncapi: Output of '$ npx -y -p @asyncapi/generator ag /tmp/000_FastKafkaAPI/asyncapi/spec/asyncapi.yml @asyncapi/html-template -o /tmp/000_FastKafkaAPI/asyncapi/docs --force-write'\n\nDone! \u2728\nCheck out your shiny new generated files at /tmp/000_FastKafkaAPI/asyncapi/docs.\n</code></pre> <pre><code>@asynccontextmanager\nasync def start_test_app():\n    app = setup_testing_app()\n    try:\n        await app._on_startup()\n        yield app\n    finally:\n        await app._on_shutdown()\n</code></pre> <pre><code>expected = \"\"\"asyncapi: 2.5.0\ninfo:\n  description: ''\n  title: FastAPI\n  version: 0.1.0\n  contact:\n    email: noreply@gmail.com\n    name: Author\n    url: https://www.google.com\nservers:\n  local:\n    description: Local (dev) Kafka broker\n    protocol: kafka\n    url: kafka\n    variables:\n      port:\n        default: '9092'\nchannels:\n  my_topic_1:\n    subscribe:\n      message:\n        $ref: '#/components/messages/MyMsgUrl'\n  my_topic_2:\n    subscribe:\n      message:\n        $ref: '#/components/messages/MyMsgEmail'\n  my_topic_3:\n    publish:\n      message:\n        $ref: '#/components/messages/MyMsgUrl'\n  my_topic_4:\n    publish:\n      message:\n        $ref: '#/components/messages/MyMsgEmail'\n  my_topic_5:\n    publish:\n      message:\n        $ref: '#/components/messages/MyMsgUrl'\ncomponents:\n  messages:\n    MyMsgEmail:\n      payload:\n        example:\n          email: agent-007@sis.gov.uk\n          msg_url:\n            info:\n              mobile: '+385987654321'\n              name: James Bond\n            url: https://sis.gov.uk/agents/007\n        properties:\n          email:\n            example: agent-007@sis.gov.uk\n            format: email\n            title: Email\n            type: string\n          msg_url:\n            allOf:\n            - $ref: '#/components/messages/MyMsgUrl'\n            example:\n              info:\n                mobile: '+385987654321'\n                name: James Bond\n              url: https://sis.gov.uk/agents/007\n            title: Msg Url\n        required:\n        - msg_url\n        - email\n        title: MyMsgEmail\n        type: object\n    MyMsgUrl:\n      payload:\n        example:\n          info:\n            mobile: '+385987654321'\n            name: James Bond\n          url: https://sis.gov.uk/agents/007\n        properties:\n          info:\n            allOf:\n            - $ref: '#/components/schemas/MyInfo'\n            example:\n              mobile: '+385987654321'\n              name: James Bond\n            title: Info\n          url:\n            example: https://sis.gov.uk/agents/007\n            format: uri\n            maxLength: 2083\n            minLength: 1\n            title: Url\n            type: string\n        required:\n        - info\n        - url\n        title: MyMsgUrl\n        type: object\n  schemas:\n    MyInfo:\n      payload:\n        properties:\n          mobile:\n            example: '+385987654321'\n            title: Mobile\n            type: string\n          name:\n            example: James Bond\n            title: Name\n            type: string\n        required:\n        - mobile\n        - name\n        title: MyInfo\n        type: object\n  securitySchemes: {}\n\"\"\"\n</code></pre> <pre><code>d1, d2 = None, None\n\n\nasync def test_me():\n    global d1\n    global d2\n    async with start_test_app() as app:\n        client = TestClient(app._fast_api_app)\n        response = client.get(\"/asyncapi.yml\")\n        assert response.status_code == 200\n        d1 = yaml.safe_load(response.text)\n        d2 = yaml.safe_load(expected)\n        assert d1 == d2, f\"{d1} != {d2}\"\n\n\nasyncio.run(test_me())\nprint(\"ok\")\n</code></pre> <pre><code>[INFO] fast_kafka_api._components.asyncapi: Old async specifications at '/tmp/000_FastKafkaAPI/asyncapi/spec/asyncapi.yml' does not exist.\n[INFO] fast_kafka_api._components.asyncapi: New async specifications generated at: '/tmp/000_FastKafkaAPI/asyncapi/spec/asyncapi.yml'\n[INFO] fast_kafka_api._components.asyncapi: Async docs generated at '/tmp/000_FastKafkaAPI/asyncapi/docs'\n[INFO] fast_kafka_api._components.asyncapi: Output of '$ npx -y -p @asyncapi/generator ag /tmp/000_FastKafkaAPI/asyncapi/spec/asyncapi.yml @asyncapi/html-template -o /tmp/000_FastKafkaAPI/asyncapi/docs --force-write'\n\nDone! \u2728\nCheck out your shiny new generated files at /tmp/000_FastKafkaAPI/asyncapi/docs.\n\n\n[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092'}'\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092'}'\n[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092'}'\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100, 'group_id': 'davor-fast-kafka-api-kafka-1:9092_group'}\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100, 'group_id': 'davor-fast-kafka-api-kafka-1:9092_group'}\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_1'})\n[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_1'}\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_2'})\n[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_2'}\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n[INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: (Re-)joining group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: (Re-)joining group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: Joined group 'davor-fast-kafka-api-kafka-1:9092_group' (generation 38) with member_id aiokafka-0.8.0-643f3079-8880-43c3-89d4-04d536508649\n[INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n[INFO] aiokafka.consumer.group_coordinator: Joined group 'davor-fast-kafka-api-kafka-1:9092_group' (generation 39) with member_id aiokafka-0.8.0-b9d667ec-542d-48c6-8812-bbd9a67b5e1b\n[INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n[INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n[INFO] aiokafka.consumer.group_coordinator: Successfully synced group davor-fast-kafka-api-kafka-1:9092_group with generation 39\n[INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='my_topic_2', partition=0)} for group davor-fast-kafka-api-kafka-1:9092_group\n[INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\nok\n</code></pre> <pre><code># don't wait for specs to be generated (takes 10 sec or so)\nwith unittest.mock.patch(\"__main__.export_async_spec\"):\n\n    # mock up send method of AIOKafkaProducer\n    with mock_AIOKafkaProducer_send() as mock:\n\n        app = create_testing_app()\n\n        @app.produces()\n        async def to_my_test_topic(mobile: str, url: str) -&gt; MyMsgUrl:\n            msg = MyMsgUrl(info=dict(mobile=mobile, name=\"James Bond\"), url=url)\n            return msg\n\n        @app.produces()\n        def to_my_test_topic_2(mobile: str, url: str) -&gt; MyMsgUrl:\n            msg = MyMsgUrl(info=dict(mobile=mobile, name=\"James Bond\"), url=url)\n            return msg\n\n        try:\n            await app._on_startup()\n            await to_my_test_topic(mobile=\"+385912345678\", url=\"https://www.vip.hr\")\n            to_my_test_topic_2(mobile=\"+385987654321\", url=\"https://www.ht.hr\")\n        finally:\n            await app._on_shutdown()\n\n        mock.assert_has_calls(\n            [\n                unittest.mock.call(\n                    \"my_test_topic\",\n                    b'{\"info\": {\"mobile\": \"+385912345678\", \"name\": \"James Bond\"}, \"url\": \"https://www.vip.hr\"}',\n                ),\n                unittest.mock.call(\n                    \"my_test_topic_2\",\n                    b'{\"info\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"}, \"url\": \"https://www.ht.hr\"}',\n                ),\n            ]\n        )\n</code></pre> <pre><code>[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092'}'\n[INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092'}'\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Entering...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting task group\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Starting send_stream\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.start(): Finished.\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Entering...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting send_stream\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Exiting task group\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: _aiokafka_producer_manager(): Finished.\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Stoping producer...\n[INFO] fast_kafka_api._components.aiokafka_producer_manager: AIOKafkaProducerManager.stop(): Finished\n</code></pre> <pre><code># don't wait for specs to be generated (takes 10 sec or so)\nwith unittest.mock.patch(\"__main__.export_async_spec\"):\n\n    app = create_testing_app()\n    fast_task = unittest.mock.Mock()\n    long_task = unittest.mock.Mock()\n\n    @app.run_in_background()\n    async def bg_task():\n        fast_task()\n        await asyncio.sleep(100)\n        long_task()\n\n    fast_task_second = unittest.mock.Mock()\n    long_task_second = unittest.mock.Mock()\n\n    @app.run_in_background()\n    async def bg_task_second():\n        fast_task_second()\n        await asyncio.sleep(100)\n        long_task_second()\n\n    try:\n        await app._on_startup()\n    finally:\n        await app._on_shutdown()\n\n    fast_task.assert_called()\n    long_task.assert_not_called()\n\n    fast_task_second.assert_called()\n    long_task_second.assert_not_called()\n\nprint(\"ok\")\n</code></pre> <pre><code>ok\n</code></pre>"},{"location":"001_ConsumerLoop/","title":"001 ConsumerLoop","text":"<pre><code>from unittest.mock import AsyncMock, MagicMock, Mock, call\n\nfrom fast_kafka_api._components.logger import supress_timestamps\nfrom fast_kafka_api.testing import (\n    create_and_fill_testing_topic,\n    nb_safe_seed,\n    true_after,\n)\n</code></pre> <pre><code>seed = nb_safe_seed(\"_components.aiokafka_consumer_loop\")\n</code></pre> <pre><code># allows async calls in notebooks\n\nimport nest_asyncio\n</code></pre> <pre><code>nest_asyncio.apply()\n</code></pre> <pre><code>supress_timestamps()\nlogger = get_logger(__name__, level=20)\nlogger.info(\"ok\")\n</code></pre> <pre><code>[INFO] __main__: ok\n</code></pre> <pre><code>kafka_server_url = environ[\"KAFKA_HOSTNAME\"]\nkafka_server_port = environ[\"KAFKA_PORT\"]\n\nkafka_config = {\"bootstrap.servers\": f\"{kafka_server_url}:{kafka_server_port}\"}\n</code></pre> <pre><code>class MyMessage(BaseModel):\n    url: HttpUrl = Field(..., example=\"http://www.acme.com\", description=\"Url example\")\n    port: NonNegativeInt = Field(1000)\n</code></pre> <p>source</p>"},{"location":"001_ConsumerLoop/#process_msgs","title":"process_msgs","text":"<pre><code> process_msgs (msgs:Dict[kafka.structs.TopicPartition,List[aiokafka.struct\n               s.ConsumerRecord]], callbacks:Dict[str,Callable[[pydantic.m\n               ain.BaseModel],Optional[Awaitable[NoneType]]]],\n               msg_types:Dict[str,Type[pydantic.main.BaseModel]], process_\n               f:Callable[[Tuple[Callable[[pydantic.main.BaseModel],Awaita\n               ble[NoneType]],pydantic.main.BaseModel]],Awaitable[NoneType\n               ]])\n</code></pre> <p>For each messages msg in msgs, calls process_f with callbacks[topic] and msgs.</p> <p>Params: msgs: a dictionary mapping topic partition to a list of messages, returned by <code>AIOKafkaConsumer.getmany</code>. callbacks: a dictionary mapping topics into a callback functions. msg_types: a dictionary mapping topics into a message type of a message. process_f: a stream processing function registrated by <code>anyio.create_memory_object_stream</code></p> <p>Todo: remove it :)</p> <pre><code>def create_consumer_record(topic: str, partition: int, msg: BaseModel):\n    record = ConsumerRecord(\n        topic=topic,\n        partition=partition,\n        offset=0,\n        timestamp=0,\n        timestamp_type=0,\n        key=None,\n        value=msg.json().encode(\"utf-8\"),\n        checksum=0,\n        serialized_key_size=0,\n        serialized_value_size=0,\n        headers=[],\n    )\n    return record\n</code></pre> <pre><code># Sanity check\n# One msg, one topic, process_f called once with callback and decoded_msg\n\ntopic = \"topic_0\"\npartition = 0\ntopic_part_0_0 = TopicPartition(topic, partition)\nmsg = MyMessage(url=\"http://www.acme.com\", port=22)\nrecord = create_consumer_record(topic=topic, partition=partition, msg=msg)\n\n\nasync def process_f(arg):\n    callback, msg = arg\n    await callback(msg)\n\n\nfor is_async in [False, True]:\n    print(f\"is_async={is_async}\")\n    callback_0 = Mock()\n    await process_msgs(\n        msgs={topic_part_0_0: [record]},\n        callbacks={topic: (asyncer.asyncify(callback_0) if is_async else callback_0)},\n        msg_types={topic: MyMessage},\n        process_f=process_f,\n    )\n\n    #     process_f.assert_called_with((callback_0, msg))\n    callback_0.assert_called_with(msg)\n    assert callback_0.call_count == 1\n</code></pre> <pre><code>is_async=False\nis_async=True\n</code></pre> <pre><code># Sanity check: exception in callback\n# One msg, one topic, process_f called once with callback and decoded_msg\n\ntopic = \"topic_0\"\npartition = 0\ntopic_part_0_0 = TopicPartition(topic, partition)\nmsg = MyMessage(url=\"http://www.acme.com\", port=22)\nrecord = create_consumer_record(topic=topic, partition=partition, msg=msg)\n\n\nasync def process_f(arg):\n    callback, msg = arg\n    await callback(msg)\n\n\nfor is_async in [False, True]:\n    print(f\"is_async={is_async}\")\n    callback_0 = Mock()\n    callback_0.side_effect = Mock(side_effect=Exception(\"Test\"))\n    await process_msgs(\n        msgs={topic_part_0_0: [record]},\n        callbacks={topic: (asyncer.asyncify(callback_0) if is_async else callback_0)},\n        msg_types={topic: MyMessage},\n        process_f=process_f,\n    )\n\n    #     process_f.assert_called_with((callback_0, msg))\n    callback_0.assert_called_with(msg)\n    assert callback_0.call_count == 1\n</code></pre> <pre><code>is_async=False\n[WARNING] __main__: process_msgs(): exception caugth Exception('Test') while awaiting '&lt;function asyncify.&lt;locals&gt;.wrapper&gt;(url=HttpUrl('http://www.acme.com', ) port=22)'\nis_async=True\n[WARNING] __main__: process_msgs(): exception caugth Exception('Test') while awaiting '&lt;function asyncify.&lt;locals&gt;.wrapper&gt;(url=HttpUrl('http://www.acme.com', ) port=22)'\n</code></pre> <pre><code># Check different topics\n\n# Two msg, two topics, process_f called twice with each callback called once\n\ntopic_part_0_0 = TopicPartition(\"topic_0\", 0)\ntopic_part_1_0 = TopicPartition(\"topic_1\", 0)\n\ntopic = \"topic_0\"\npartition = 0\ntopic_part_0_0 = TopicPartition(\"topic_0\", 0)\nmsg = MyMessage(url=\"http://www.acme.com\", port=22)\nrecord = create_consumer_record(topic=topic, partition=partition, msg=msg)\n\ncallback_0 = Mock()\ncallback_1 = AsyncMock()\n\nawait process_msgs(\n    msgs={topic_part_0_0: [record], topic_part_1_0: [record]},\n    callbacks={\"topic_0\": callback_0, \"topic_1\": callback_1},\n    msg_types={\"topic_0\": MyMessage, \"topic_1\": MyMessage},\n    process_f=process_f,\n)\n\ncallback_0.assert_called_once_with(msg)\ncallback_1.assert_awaited_once_with(msg)\ncallback_0.assert_called_once_with(msg)\n</code></pre> <pre><code># Check multiple msgs in same topic\n# Check callback not called if there are no msgs for it in the queue\n\n# Two msg, one topic, one callback called twice, other called nonce, produce and process_f called twice\n\n# Check different topics\n\n# Two msg, two topics, process_f called twice with each callback called once and produce twice\n\ntopic_part_0_0 = TopicPartition(\"topic_0\", 0)\n\ntopic = \"topic_0\"\npartition = 0\ntopic_part_0_0 = TopicPartition(\"topic_0\", 0)\nmsg = MyMessage(url=\"http://www.acme.com\", port=22)\nrecord = create_consumer_record(topic=topic, partition=partition, msg=msg)\n\ncallback_0 = Mock()\ncallback_1 = AsyncMock()\n\nawait process_msgs(\n    msgs={topic_part_0_0: [record, record]},\n    callbacks={\"topic_0\": callback_0, \"topic_1\": callback_1},\n    msg_types={\"topic_0\": MyMessage, \"topic_1\": MyMessage},\n    process_f=process_f,\n)\n\ncallback_0.assert_has_calls([call(msg)] * 2)\ncallback_1.assert_not_awaited()\n</code></pre> <pre><code># Check multiple partitions\n\n# Two msg, one topic, two partitions, one callback called twice, produce and process_f called twice\n\ntopic_part_0_0 = TopicPartition(\"topic_0\", 0)\ntopic_part_0_1 = TopicPartition(\"topic_0\", 1)\n\nmsg = MyMessage(url=\"http://www.acme.com\", port=22)\nrecord = create_consumer_record(topic=topic, partition=partition, msg=msg)\n\ncallback_0 = AsyncMock()\ncallback_1 = Mock()\n\nawait process_msgs(\n    msgs={\n        topic_part_0_0: [create_consumer_record(topic=\"topic_0\", partition=0, msg=msg)],\n        topic_part_0_1: [create_consumer_record(topic=\"topic_0\", partition=1, msg=msg)],\n    },\n    callbacks={\"topic_0\": callback_0, \"topic_1\": callback_1},\n    msg_types={\"topic_0\": MyMessage, \"topic_1\": MyMessage},\n    process_f=process_f,\n)\n\ncallback_0.assert_has_awaits([call(msg)] * 2)\ncallback_1.assert_not_called()\n</code></pre> <p>source</p>"},{"location":"001_ConsumerLoop/#process_message_callback","title":"process_message_callback","text":"<pre><code> process_message_callback (receive_stream:anyio.streams.memory.MemoryObjec\n                           tReceiveStream[typing.Any])\n</code></pre> <pre><code>topic = \"topic_0\"\nmsg = MyMessage(url=\"http://www.acme.com\", port=22)\nrecord = create_consumer_record(topic=topic, partition=partition, msg=msg)\n\nmock_consumer = MagicMock()\nmsgs = {TopicPartition(topic, 0): [record]}\n\nf = asyncio.Future()\nf.set_result(msgs)\nmock_consumer.configure_mock(**{\"getmany.return_value\": f})\nmock_callback = Mock()\n\n\ndef is_shutting_down_f(mock_func):\n    def _is_shutting_down_f():\n        return mock_func.called\n\n    return _is_shutting_down_f\n\n\nfor is_async in [True, False]:\n    await _aiokafka_consumer_loop(\n        consumer=mock_consumer,\n        max_buffer_size=100,\n        callbacks={\n            topic: asyncer.asyncify(mock_callback) if is_async else mock_callback\n        },\n        msg_types={topic: MyMessage},\n        is_shutting_down_f=is_shutting_down_f(mock_consumer.getmany),\n    )\n\n    assert mock_consumer.getmany.call_count == 1\n    mock_callback.assert_called_once_with(msg)\n</code></pre> <p>source</p>"},{"location":"001_ConsumerLoop/#sanitize_kafka_config","title":"sanitize_kafka_config","text":"<pre><code> sanitize_kafka_config (**kwargs)\n</code></pre> <p>Sanitize Kafka config</p> <pre><code>kwargs = {'bootstrap_servers': 'whatever.cloud:9092',\n 'auto_offset_reset': 'earliest',\n 'security_protocol': 'SASL_SSL',\n 'sasl_mechanism': 'PLAIN',\n 'sasl_plain_username': 'username',\n 'sasl_plain_password': 'password',\n 'ssl_context': \"something\"}\n\nassert sanitize_kafka_config(**kwargs)[\"sasl_plain_password\"] == '********'\n</code></pre> <p>source</p>"},{"location":"001_ConsumerLoop/#aiokafka_consumer_loop","title":"aiokafka_consumer_loop","text":"<pre><code> aiokafka_consumer_loop (topics:List[str], bootstrap_servers:str,\n                         auto_offset_reset:str, max_poll_records:int=1000,\n                         timeout_ms:int=100, max_buffer_size:int=10000, ca\n                         llbacks:Dict[str,Callable[[pydantic.main.BaseMode\n                         l],Optional[Awaitable[NoneType]]]], msg_types:Dic\n                         t[str,Type[pydantic.main.BaseModel]],\n                         is_shutting_down_f:Callable[[],bool], **kwargs)\n</code></pre> <p>todo: write docs</p> <pre><code>msgs_sent = 9178\nmsgs = [\n    MyMessage(url=\"http://www.ai.com\", port=port).json().encode(\"utf-8\")\n    for port in range(msgs_sent)\n]\nmsgs_received = 0\n\n\nasync def count_msg(msg: MyMessage):\n    global msgs_received\n    msgs_received = msgs_received + 1\n    if msgs_received % 1000 == 0:\n        logger.info(f\"{msgs_received=}\")\n\n\nasync with create_and_fill_testing_topic(\n    kafka_config=kafka_config, msgs=msgs, seed=seed(1)\n) as topic:\n    await aiokafka_consumer_loop(\n        topics=[topic],\n        bootstrap_servers=kafka_config[\"bootstrap.servers\"],\n        auto_offset_reset=\"earliest\",\n        callbacks={topic: count_msg},\n        msg_types={topic: MyMessage},\n        is_shutting_down_f=true_after(2),\n    )\n\nassert msgs_sent == msgs_received, f\"{msgs_sent} != {msgs_received}\"\n</code></pre> <pre><code>[INFO] fast_kafka_api.testing: create_missing_topics(['my_topic_5696213874']): new_topics = [NewTopic(topic=my_topic_5696213874,num_partitions=3)]\n[INFO] fast_kafka_api.testing: Producer &lt;aiokafka.producer.producer.AIOKafkaProducer object&gt; created.\n[INFO] fast_kafka_api.testing: Producer &lt;aiokafka.producer.producer.AIOKafkaProducer object&gt; started.\n[INFO] fast_kafka_api.testing: Sent messages: len(sent_msgs)=9178\n[INFO] __main__: aiokafka_consumer_loop() starting...\n[INFO] __main__: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 1000}\n[INFO] __main__: aiokafka_consumer_loop(): Consumer started.\n[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_5696213874'})\n[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_5696213874'}\n[INFO] __main__: aiokafka_consumer_loop(): Consumer subscribed.\n[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_5696213874': 3}. \n[INFO] __main__: msgs_received=1000\n[INFO] __main__: msgs_received=2000\n[INFO] __main__: msgs_received=3000\n[INFO] __main__: msgs_received=4000\n[INFO] __main__: msgs_received=5000\n[INFO] __main__: msgs_received=6000\n[INFO] __main__: msgs_received=7000\n[INFO] __main__: msgs_received=8000\n[INFO] __main__: msgs_received=9000\n[INFO] __main__: aiokafka_consumer_loop(): Consumer stopped.\n[INFO] __main__: aiokafka_consumer_loop() finished.\n[INFO] fast_kafka_api.testing: Producer &lt;aiokafka.producer.producer.AIOKafkaProducer object&gt; stoped.\n</code></pre> <pre><code>msgs_sent = 100_000\nmsgs = [\n    MyMessage(url=\"http://www.ai.com\", port=port).json().encode(\"utf-8\")\n    for port in range(msgs_sent)\n]\nmsgs_received = 0\n\n\nasync def count_msg(msg: MyMessage):\n    global msgs_received\n    msgs_received = msgs_received + 1\n    if msgs_received % 1000 == 0:\n        logger.info(f\"{msgs_received=}\")\n\n\ndef _is_shutting_down_f():\n    return msgs_received == msgs_sent\n\n\nasync with create_and_fill_testing_topic(\n    kafka_config=kafka_config, msgs=msgs, seed=seed(3)\n) as topic:\n    start = datetime.now()\n    await aiokafka_consumer_loop(\n        topics=[topic],\n        bootstrap_servers=kafka_config[\"bootstrap.servers\"],\n        auto_offset_reset=\"earliest\",\n        callbacks={topic: count_msg},\n        msg_types={topic: MyMessage},\n        is_shutting_down_f=_is_shutting_down_f,\n    )\n    t = (datetime.now() - start) / timedelta(seconds=1)\n    thrp = msgs_received / t\n\n    print(f\"Messages processed: {msgs_received:,d}\")\n    print(f\"Time              : {t:.2f} s\")\n    print(f\"Throughput.       : {thrp:,.0f} msg/s\")\n</code></pre> <pre><code>[INFO] fast_kafka_api.testing: create_missing_topics(['my_topic_5168585847']): new_topics = [NewTopic(topic=my_topic_5168585847,num_partitions=3)]\n[INFO] fast_kafka_api.testing: Producer &lt;aiokafka.producer.producer.AIOKafkaProducer object&gt; created.\n[INFO] fast_kafka_api.testing: Producer &lt;aiokafka.producer.producer.AIOKafkaProducer object&gt; started.\n[INFO] fast_kafka_api.testing: Sent messages: len(sent_msgs)=100000\n[INFO] __main__: aiokafka_consumer_loop() starting...\n[INFO] __main__: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'davor-fast-kafka-api-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 1000}\n[INFO] __main__: aiokafka_consumer_loop(): Consumer started.\n[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_5168585847'})\n[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_5168585847'}\n[INFO] __main__: aiokafka_consumer_loop(): Consumer subscribed.\n[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_5168585847': 3}. \n[INFO] __main__: msgs_received=1000\n[INFO] __main__: msgs_received=2000\n[INFO] __main__: msgs_received=3000\n[INFO] __main__: msgs_received=4000\n[INFO] __main__: msgs_received=5000\n[INFO] __main__: msgs_received=6000\n[INFO] __main__: msgs_received=7000\n[INFO] __main__: msgs_received=8000\n[INFO] __main__: msgs_received=9000\n[INFO] __main__: msgs_received=10000\n[INFO] __main__: msgs_received=11000\n[INFO] __main__: msgs_received=12000\n[INFO] __main__: msgs_received=13000\n[INFO] __main__: msgs_received=14000\n[INFO] __main__: msgs_received=15000\n[INFO] __main__: msgs_received=16000\n[INFO] __main__: msgs_received=17000\n[INFO] __main__: msgs_received=18000\n[INFO] __main__: msgs_received=19000\n[INFO] __main__: msgs_received=20000\n[INFO] __main__: msgs_received=21000\n[INFO] __main__: msgs_received=22000\n[INFO] __main__: msgs_received=23000\n[INFO] __main__: msgs_received=24000\n[INFO] __main__: msgs_received=25000\n[INFO] __main__: msgs_received=26000\n[INFO] __main__: msgs_received=27000\n[INFO] __main__: msgs_received=28000\n[INFO] __main__: msgs_received=29000\n[INFO] __main__: msgs_received=30000\n[INFO] __main__: msgs_received=31000\n[INFO] __main__: msgs_received=32000\n[INFO] __main__: msgs_received=33000\n[INFO] __main__: msgs_received=34000\n[INFO] __main__: msgs_received=35000\n[INFO] __main__: msgs_received=36000\n[INFO] __main__: msgs_received=37000\n[INFO] __main__: msgs_received=38000\n[INFO] __main__: msgs_received=39000\n[INFO] __main__: msgs_received=40000\n[INFO] __main__: msgs_received=41000\n[INFO] __main__: msgs_received=42000\n[INFO] __main__: msgs_received=43000\n[INFO] __main__: msgs_received=44000\n[INFO] __main__: msgs_received=45000\n[INFO] __main__: msgs_received=46000\n[INFO] __main__: msgs_received=47000\n[INFO] __main__: msgs_received=48000\n[INFO] __main__: msgs_received=49000\n[INFO] __main__: msgs_received=50000\n[INFO] __main__: msgs_received=51000\n[INFO] __main__: msgs_received=52000\n[INFO] __main__: msgs_received=53000\n[INFO] __main__: msgs_received=54000\n[INFO] __main__: msgs_received=55000\n[INFO] __main__: msgs_received=56000\n[INFO] __main__: msgs_received=57000\n[INFO] __main__: msgs_received=58000\n[INFO] __main__: msgs_received=59000\n[INFO] __main__: msgs_received=60000\n[INFO] __main__: msgs_received=61000\n[INFO] __main__: msgs_received=62000\n[INFO] __main__: msgs_received=63000\n[INFO] __main__: msgs_received=64000\n[INFO] __main__: msgs_received=65000\n[INFO] __main__: msgs_received=66000\n[INFO] __main__: msgs_received=67000\n[INFO] __main__: msgs_received=68000\n[INFO] __main__: msgs_received=69000\n[INFO] __main__: msgs_received=70000\n[INFO] __main__: msgs_received=71000\n[INFO] __main__: msgs_received=72000\n[INFO] __main__: msgs_received=73000\n[INFO] __main__: msgs_received=74000\n[INFO] __main__: msgs_received=75000\n[INFO] __main__: msgs_received=76000\n[INFO] __main__: msgs_received=77000\n[INFO] __main__: msgs_received=78000\n[INFO] __main__: msgs_received=79000\n[INFO] __main__: msgs_received=80000\n[INFO] __main__: msgs_received=81000\n[INFO] __main__: msgs_received=82000\n[INFO] __main__: msgs_received=83000\n[INFO] __main__: msgs_received=84000\n[INFO] __main__: msgs_received=85000\n[INFO] __main__: msgs_received=86000\n[INFO] __main__: msgs_received=87000\n[INFO] __main__: msgs_received=88000\n[INFO] __main__: msgs_received=89000\n[INFO] __main__: msgs_received=90000\n[INFO] __main__: msgs_received=91000\n[INFO] __main__: msgs_received=92000\n[INFO] __main__: msgs_received=93000\n[INFO] __main__: msgs_received=94000\n[INFO] __main__: msgs_received=95000\n[INFO] __main__: msgs_received=96000\n[INFO] __main__: msgs_received=97000\n[INFO] __main__: msgs_received=98000\n[INFO] __main__: msgs_received=99000\n[INFO] __main__: msgs_received=100000\n[INFO] __main__: aiokafka_consumer_loop(): Consumer stopped.\n[INFO] __main__: aiokafka_consumer_loop() finished.\nMessages processed: 100,000\nTime              : 3.90 s\nThroughput.       : 25,665 msg/s\n[INFO] fast_kafka_api.testing: Producer &lt;aiokafka.producer.producer.AIOKafkaProducer object&gt; stoped.\n</code></pre>"},{"location":"002_ProducerManager/","title":"002 ProducerManager","text":"<pre><code>import unittest.mock\nfrom os import environ\n\nfrom fast_kafka_api._components.logger import supress_timestamps\nfrom fast_kafka_api.testing import (\n    create_and_fill_testing_topic,\n    nb_safe_seed,\n    true_after,\n)\n</code></pre> <pre><code>seed = nb_safe_seed(\"_components.aiokafka_producer_loop\")\n</code></pre> <pre><code># allows async calls in notebooks\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n</code></pre> <pre><code>supress_timestamps()\nlogger = get_logger(__name__, level=1)\nlogger.info(\"ok\")\n</code></pre> <pre><code>[INFO] __main__: ok\n</code></pre> <pre><code>kafka_server_url = environ[\"KAFKA_HOSTNAME\"]\nkafka_server_port = environ[\"KAFKA_PORT\"]\n\nkafka_config = {\"bootstrap.servers\": f\"{kafka_server_url}:{kafka_server_port}\"}\n</code></pre> <pre><code>@contextmanager\ndef mock_AIOKafkaProducer_send():\n    with unittest.mock.patch(\"__main__.AIOKafkaProducer.send\") as mock:\n\n        async def _f():\n            pass\n\n        mock.return_value = asyncio.create_task(_f())\n\n        yield mock\n</code></pre> <pre><code>num_msgs = 15\ntopic = \"topic\"\nmsg = b\"msg\"\nmsgs = [(topic, msg) for _ in range(num_msgs)]\ncalls = [unittest.mock.call(topic, msg) for _ in range(num_msgs)]\n\nwith mock_AIOKafkaProducer_send() as send_mock:\n    producer = AIOKafkaProducer()\n    async with _aiokafka_producer_manager(producer) as send_stream:\n\n        for msg in msgs:\n            send_stream.send_nowait(msg)\n\n        await producer.stop()\n    #     await producer_loop_generator.__aexit__(None, None, None)\n\n    send_mock.assert_has_calls(calls)\n</code></pre> <pre><code>[INFO] __main__: _aiokafka_producer_manager(): Starting...\n[INFO] __main__: _aiokafka_producer_manager(): Starting task group\n[INFO] __main__: _aiokafka_producer_manager(): Starting send_stream\n[DEBUG] aiokafka.producer.producer: The Kafka producer has closed.\n[INFO] __main__: _aiokafka_producer_manager(): Exiting send_stream\n[INFO] __main__: _aiokafka_producer_manager(): Exiting task group\n[INFO] __main__: _aiokafka_producer_manager(): Finished.\n</code></pre> <p>source</p>"},{"location":"002_ProducerManager/#aiokafkaproducermanager","title":"AIOKafkaProducerManager","text":"<pre><code> AIOKafkaProducerManager\n                          (producer:aiokafka.producer.producer.AIOKafkaPro\n                          ducer, max_buffer_size:int=1000)\n</code></pre> <p>Initialize self. See help(type(self)) for accurate signature.</p> Type Default Details producer AIOKafkaProducer type: ignore max_buffer_size int 1000 <pre><code>producer = AIOKafkaProducer(bootstrap_servers=kafka_config[\"bootstrap.servers\"])\nmanager = AIOKafkaProducerManager(producer)\nawait manager.start()\nmanager.send(\"topic\", b\"msg\")\nawait manager.stop()\nlogger.info(\"Stopped\")\n</code></pre> <pre><code>[INFO] __main__: AIOKafkaProducerManager.start(): Entering...\n[DEBUG] aiokafka.producer.producer: Starting the Kafka producer\n[DEBUG] aiokafka: Attempting to bootstrap via node at tvrtko-fast-kafka-api-kafka-1:9092\n[DEBUG] aiokafka.conn: &lt;AIOKafkaConnection host=tvrtko-fast-kafka-api-kafka-1 port=9092&gt; Request 1: MetadataRequest_v0(topics=[])\n[DEBUG] aiokafka.conn: &lt;AIOKafkaConnection host=tvrtko-fast-kafka-api-kafka-1 port=9092&gt; Response 1: MetadataResponse_v0(brokers=[(node_id=1001, host='75d5a1be66b3', port=9092), (node_id=1003, host='40c27daf393d', port=9092), (node_id=1002, host='681f4568022c', port=9092)], topics=[(error_code=0, topic='my_topic_1', partitions=[(error_code=0, partition=0, leader=1003, replicas=[1003], isr=[1003])]), (error_code=0, topic='my_test_topic_2', partitions=[(error_code=0, partition=0, leader=1002, replicas=[1002], isr=[1002])]), (error_code=0, topic='training_status', partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001, 1002, 1003], isr=[1001, 1002, 1003]), (error_code=0, partition=5, leader=1003, replicas=[1003, 1002, 1001], isr=[1003, 1002, 1001]), (error_code=0, partition=4, leader=1002, replicas=[1002, 1001, 1003], isr=[1002, 1001, 1003]), (error_code=0, partition=1, leader=1002, replicas=[1002, 1003, 1001], isr=[1002, 1003, 1001]), (error_code=0, partition=2, leader=1003, replicas=[1003, 1001, 1002], isr=[1003, 1001, 1002]), (error_code=0, partition=3, leader=1001, replicas=[1001, 1003, 1002], isr=[1001, 1003, 1002])]), (error_code=0, topic='prediction_request', partitions=[(error_code=0, partition=0, leader=1002, replicas=[1002, 1001, 1003], isr=[1002, 1001, 1003]), (error_code=0, partition=5, leader=1001, replicas=[1001, 1002, 1003], isr=[1001, 1002, 1003]), (error_code=0, partition=1, leader=1003, replicas=[1003, 1002, 1001], isr=[1003, 1002, 1001]), (error_code=0, partition=4, leader=1003, replicas=[1003, 1001, 1002], isr=[1003, 1001, 1002]), (error_code=0, partition=2, leader=1001, replicas=[1001, 1003, 1002], isr=[1001, 1003, 1002]), (error_code=0, partition=3, leader=1002, replicas=[1002, 1003, 1001], isr=[1002, 1003, 1001])]), (error_code=0, topic='my_topic_2', partitions=[(error_code=0, partition=0, leader=1003, replicas=[1003], isr=[1003])]), (error_code=0, topic='training_data', partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='topic', partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='prediction_status', partitions=[(error_code=0, partition=0, leader=1002, replicas=[1002, 1003, 1001], isr=[1002, 1003, 1001]), (error_code=0, partition=5, leader=1001, replicas=[1001, 1003, 1002], isr=[1001, 1003, 1002]), (error_code=0, partition=1, leader=1003, replicas=[1003, 1001, 1002], isr=[1003, 1001, 1002]), (error_code=0, partition=4, leader=1003, replicas=[1003, 1002, 1001], isr=[1003, 1002, 1001]), (error_code=0, partition=2, leader=1001, replicas=[1001, 1002, 1003], isr=[1001, 1002, 1003]), (error_code=0, partition=3, leader=1002, replicas=[1002, 1001, 1003], isr=[1002, 1001, 1003])]), (error_code=0, topic='realitime_data', partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='training_request', partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001, 1003, 1002], isr=[1001, 1003, 1002]), (error_code=0, partition=5, leader=1003, replicas=[1003, 1001, 1002], isr=[1003, 1001, 1002]), (error_code=0, partition=1, leader=1002, replicas=[1002, 1001, 1003], isr=[1002, 1001, 1003]), (error_code=0, partition=4, leader=1002, replicas=[1002, 1003, 1001], isr=[1002, 1003, 1001]), (error_code=0, partition=2, leader=1003, replicas=[1003, 1002, 1001], isr=[1003, 1002, 1001]), (error_code=0, partition=3, leader=1001, replicas=[1001, 1002, 1003], isr=[1001, 1002, 1003])])])\n[DEBUG] aiokafka.cluster: Updated cluster metadata to ClusterMetadata(brokers: 3, topics: 10, groups: 0)\n[DEBUG] aiokafka.conn: Closing connection at tvrtko-fast-kafka-api-kafka-1:9092\n[DEBUG] aiokafka: Received cluster metadata: ClusterMetadata(brokers: 3, topics: 10, groups: 0)\n[DEBUG] aiokafka: Initiating connection to node 1003 at 40c27daf393d:9092\n[DEBUG] aiokafka.conn: &lt;AIOKafkaConnection host=40c27daf393d port=9092&gt; Request 1: ApiVersionRequest_v0()\n[DEBUG] aiokafka.conn: &lt;AIOKafkaConnection host=40c27daf393d port=9092&gt; Response 1: ApiVersionResponse_v0(error_code=0, api_versions=[(api_key=0, min_version=0, max_version=9), (api_key=1, min_version=0, max_version=12), (api_key=2, min_version=0, max_version=6), (api_key=3, min_version=0, max_version=11), (api_key=4, min_version=0, max_version=5), (api_key=5, min_version=0, max_version=3), (api_key=6, min_version=0, max_version=7), (api_key=7, min_version=0, max_version=3), (api_key=8, min_version=0, max_version=8), (api_key=9, min_version=0, max_version=7), (api_key=10, min_version=0, max_version=3), (api_key=11, min_version=0, max_version=7), (api_key=12, min_version=0, max_version=4), (api_key=13, min_version=0, max_version=4), (api_key=14, min_version=0, max_version=5), (api_key=15, min_version=0, max_version=5), (api_key=16, min_version=0, max_version=4), (api_key=17, min_version=0, max_version=1), (api_key=18, min_version=0, max_version=3), (api_key=19, min_version=0, max_version=7), (api_key=20, min_version=0, max_version=6), (api_key=21, min_version=0, max_version=2), (api_key=22, min_version=0, max_version=4), (api_key=23, min_version=0, max_version=4), (api_key=24, min_version=0, max_version=3), (api_key=25, min_version=0, max_version=3), (api_key=26, min_version=0, max_version=3), (api_key=27, min_version=0, max_version=1), (api_key=28, min_version=0, max_version=3), (api_key=29, min_version=0, max_version=2), (api_key=30, min_version=0, max_version=2), (api_key=31, min_version=0, max_version=2), (api_key=32, min_version=0, max_version=4), (api_key=33, min_version=0, max_version=2), (api_key=34, min_version=0, max_version=2), (api_key=35, min_version=0, max_version=2), (api_key=36, min_version=0, max_version=2), (api_key=37, min_version=0, max_version=3), (api_key=38, min_version=0, max_version=2), (api_key=39, min_version=0, max_version=2), (api_key=40, min_version=0, max_version=2), (api_key=41, min_version=0, max_version=2), (api_key=42, min_version=0, max_version=2), (api_key=43, min_version=0, max_version=2), (api_key=44, min_version=0, max_version=1), (api_key=45, min_version=0, max_version=0), (api_key=46, min_version=0, max_version=0), (api_key=47, min_version=0, max_version=0), (api_key=48, min_version=0, max_version=1), (api_key=49, min_version=0, max_version=1), (api_key=50, min_version=0, max_version=0), (api_key=51, min_version=0, max_version=0), (api_key=56, min_version=0, max_version=0), (api_key=57, min_version=0, max_version=0), (api_key=60, min_version=0, max_version=0), (api_key=61, min_version=0, max_version=0)])\n[DEBUG] aiokafka.conn: &lt;AIOKafkaConnection host=40c27daf393d port=9092&gt; Request 2: MetadataRequest_v0(topics=[])\n[DEBUG] aiokafka.conn: &lt;AIOKafkaConnection host=40c27daf393d port=9092&gt; Response 2: MetadataResponse_v0(brokers=[(node_id=1001, host='75d5a1be66b3', port=9092), (node_id=1003, host='40c27daf393d', port=9092), (node_id=1002, host='681f4568022c', port=9092)], topics=[(error_code=0, topic='my_topic_1', partitions=[(error_code=0, partition=0, leader=1003, replicas=[1003], isr=[1003])]), (error_code=0, topic='my_test_topic_2', partitions=[(error_code=0, partition=0, leader=1002, replicas=[1002], isr=[1002])]), (error_code=0, topic='training_status', partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001, 1002, 1003], isr=[1001, 1002, 1003]), (error_code=0, partition=5, leader=1003, replicas=[1003, 1002, 1001], isr=[1003, 1002, 1001]), (error_code=0, partition=4, leader=1002, replicas=[1002, 1001, 1003], isr=[1002, 1001, 1003]), (error_code=0, partition=1, leader=1002, replicas=[1002, 1003, 1001], isr=[1002, 1003, 1001]), (error_code=0, partition=2, leader=1003, replicas=[1003, 1001, 1002], isr=[1003, 1001, 1002]), (error_code=0, partition=3, leader=1001, replicas=[1001, 1003, 1002], isr=[1001, 1003, 1002])]), (error_code=0, topic='prediction_request', partitions=[(error_code=0, partition=0, leader=1002, replicas=[1002, 1001, 1003], isr=[1002, 1001, 1003]), (error_code=0, partition=5, leader=1001, replicas=[1001, 1002, 1003], isr=[1001, 1002, 1003]), (error_code=0, partition=1, leader=1003, replicas=[1003, 1002, 1001], isr=[1003, 1002, 1001]), (error_code=0, partition=4, leader=1003, replicas=[1003, 1001, 1002], isr=[1003, 1001, 1002]), (error_code=0, partition=2, leader=1001, replicas=[1001, 1003, 1002], isr=[1001, 1003, 1002]), (error_code=0, partition=3, leader=1002, replicas=[1002, 1003, 1001], isr=[1002, 1003, 1001])]), (error_code=0, topic='my_topic_2', partitions=[(error_code=0, partition=0, leader=1003, replicas=[1003], isr=[1003])]), (error_code=0, topic='training_data', partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='topic', partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='prediction_status', partitions=[(error_code=0, partition=0, leader=1002, replicas=[1002, 1003, 1001], isr=[1002, 1003, 1001]), (error_code=0, partition=5, leader=1001, replicas=[1001, 1003, 1002], isr=[1001, 1003, 1002]), (error_code=0, partition=1, leader=1003, replicas=[1003, 1001, 1002], isr=[1003, 1001, 1002]), (error_code=0, partition=4, leader=1003, replicas=[1003, 1002, 1001], isr=[1003, 1002, 1001]), (error_code=0, partition=2, leader=1001, replicas=[1001, 1002, 1003], isr=[1001, 1002, 1003]), (error_code=0, partition=3, leader=1002, replicas=[1002, 1001, 1003], isr=[1002, 1001, 1003])]), (error_code=0, topic='realitime_data', partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='training_request', partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001, 1003, 1002], isr=[1001, 1003, 1002]), (error_code=0, partition=5, leader=1003, replicas=[1003, 1001, 1002], isr=[1003, 1001, 1002]), (error_code=0, partition=1, leader=1002, replicas=[1002, 1001, 1003], isr=[1002, 1001, 1003]), (error_code=0, partition=4, leader=1002, replicas=[1002, 1003, 1001], isr=[1002, 1003, 1001]), (error_code=0, partition=2, leader=1003, replicas=[1003, 1002, 1001], isr=[1003, 1002, 1001]), (error_code=0, partition=3, leader=1001, replicas=[1001, 1002, 1003], isr=[1001, 1002, 1003])])])\n\n[DEBUG] aiokafka.conn: Closing connection at 40c27daf393d:9092\n[DEBUG] aiokafka.producer.producer: Kafka producer started\n[INFO] __main__: _aiokafka_producer_manager(): Starting...\n[INFO] __main__: _aiokafka_producer_manager(): Starting task group\n[INFO] __main__: _aiokafka_producer_manager(): Starting send_stream\n[INFO] __main__: AIOKafkaProducerManager.start(): Finished.\n[INFO] __main__: AIOKafkaProducerManager.stop(): Entering...\n[INFO] __main__: _aiokafka_producer_manager(): Exiting send_stream\n[INFO] __main__: _aiokafka_producer_manager(): Exiting task group\n[DEBUG] aiokafka.producer.producer: Sending (key=None value=b'msg') to TopicPartition(topic='topic', partition=0)\n[DEBUG] aiokafka: Initiating connection to node 1001 at 75d5a1be66b3:9092\n[DEBUG] aiokafka.conn: &lt;AIOKafkaConnection host=75d5a1be66b3 port=9092&gt; Request 1: ApiVersionRequest_v0()\n[DEBUG] aiokafka.conn: &lt;AIOKafkaConnection host=75d5a1be66b3 port=9092&gt; Response 1: ApiVersionResponse_v0(error_code=0, api_versions=[(api_key=0, min_version=0, max_version=9), (api_key=1, min_version=0, max_version=12), (api_key=2, min_version=0, max_version=6), (api_key=3, min_version=0, max_version=11), (api_key=4, min_version=0, max_version=5), (api_key=5, min_version=0, max_version=3), (api_key=6, min_version=0, max_version=7), (api_key=7, min_version=0, max_version=3), (api_key=8, min_version=0, max_version=8), (api_key=9, min_version=0, max_version=7), (api_key=10, min_version=0, max_version=3), (api_key=11, min_version=0, max_version=7), (api_key=12, min_version=0, max_version=4), (api_key=13, min_version=0, max_version=4), (api_key=14, min_version=0, max_version=5), (api_key=15, min_version=0, max_version=5), (api_key=16, min_version=0, max_version=4), (api_key=17, min_version=0, max_version=1), (api_key=18, min_version=0, max_version=3), (api_key=19, min_version=0, max_version=7), (api_key=20, min_version=0, max_version=6), (api_key=21, min_version=0, max_version=2), (api_key=22, min_version=0, max_version=4), (api_key=23, min_version=0, max_version=4), (api_key=24, min_version=0, max_version=3), (api_key=25, min_version=0, max_version=3), (api_key=26, min_version=0, max_version=3), (api_key=27, min_version=0, max_version=1), (api_key=28, min_version=0, max_version=3), (api_key=29, min_version=0, max_version=2), (api_key=30, min_version=0, max_version=2), (api_key=31, min_version=0, max_version=2), (api_key=32, min_version=0, max_version=4), (api_key=33, min_version=0, max_version=2), (api_key=34, min_version=0, max_version=2), (api_key=35, min_version=0, max_version=2), (api_key=36, min_version=0, max_version=2), (api_key=37, min_version=0, max_version=3), (api_key=38, min_version=0, max_version=2), (api_key=39, min_version=0, max_version=2), (api_key=40, min_version=0, max_version=2), (api_key=41, min_version=0, max_version=2), (api_key=42, min_version=0, max_version=2), (api_key=43, min_version=0, max_version=2), (api_key=44, min_version=0, max_version=1), (api_key=45, min_version=0, max_version=0), (api_key=46, min_version=0, max_version=0), (api_key=47, min_version=0, max_version=0), (api_key=48, min_version=0, max_version=1), (api_key=49, min_version=0, max_version=1), (api_key=50, min_version=0, max_version=0), (api_key=51, min_version=0, max_version=0), (api_key=56, min_version=0, max_version=0), (api_key=57, min_version=0, max_version=0), (api_key=60, min_version=0, max_version=0), (api_key=61, min_version=0, max_version=0)])\n[DEBUG] aiokafka.conn: &lt;AIOKafkaConnection host=75d5a1be66b3 port=9092&gt; Request 2: ProduceRequest_v7(transactional_id=None, required_acks=1, timeout=40000, topics=[(topic='topic', partitions=[(partition=0, messages=bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00;\\xff\\xff\\xff\\xff\\x02\\xcbLP\\xf4\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x85{\\xe8\\x00.\\x00\\x00\\x01\\x85{\\xe8\\x00.\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x12\\x00\\x00\\x00\\x01\\x06msg\\x00'))])])\n[DEBUG] aiokafka.conn: &lt;AIOKafkaConnection host=75d5a1be66b3 port=9092&gt; Response 2: ProduceResponse_v7(topics=[(topic='topic', partitions=[(partition=0, error_code=0, offset=1, timestamp=-1, log_start_offset=0)])], throttle_time_ms=0)\n[INFO] __main__: _aiokafka_producer_manager(): Finished.\n[INFO] __main__: AIOKafkaProducerManager.stop(): Stoping producer...\n[DEBUG] aiokafka.conn: Closing connection at 40c27daf393d:9092\n[DEBUG] aiokafka.conn: Closing connection at 75d5a1be66b3:9092\n[DEBUG] aiokafka.producer.producer: The Kafka producer has closed.\n[INFO] __main__: AIOKafkaProducerManager.stop(): Finished\n[INFO] __main__: Stopped\n</code></pre>"},{"location":"003_AsyncAPI/","title":"003 AsyncAPI","text":"<pre><code>import pytest\nfrom rich.pretty import pprint\n</code></pre> <p>source</p>"},{"location":"003_AsyncAPI/#kafkamessage","title":"KafkaMessage","text":"<pre><code> KafkaMessage ()\n</code></pre> <p>Create a new model by parsing and validating input data from keyword arguments.</p> <p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p> <pre><code>class MyMsg(KafkaMessage):\n    dt: datetime = Field(..., example=datetime.now())\n    td: timedelta = Field(timedelta(days=1, hours=12, minutes=2, seconds=1.2345678))\n\n\nmy_msg = MyMsg(dt=datetime(year=2021, month=12, day=31, hour=23, minute=59, second=58))\npprint(my_msg)\nexpected = '{\"dt\": \"2021-12-31T23:59:58\", \"td\": \"P1DT12H2M1.234568S\"}'\nactual = my_msg.json()\nassert actual == expected, f\"{actual} != {expected}\"\n\nactual = MyMsg.parse_raw(actual)\nassert actual == my_msg\n</code></pre> <pre>MyMsg(\n\u2502   dt=datetime.datetime(2021, 12, 31, 23, 59, 58),\n\u2502   td=datetime.timedelta(days=1, seconds=43321, microseconds=234568)\n)\n</pre> <p>source</p>"},{"location":"003_AsyncAPI/#securityschema","title":"SecuritySchema","text":"<pre><code> SecuritySchema (security_type:__main__.SecurityType,\n                 description:Optional[str]=None, name:Optional[str]=None,\n                 api_key_loc:Optional[__main__.APIKeyLocation]=None,\n                 scheme:Optional[str]=None,\n                 bearerFormat:Optional[str]=None,\n                 flows:Optional[str]=None,\n                 openIdConnectUrl:Optional[str]=None)\n</code></pre> <p>Create a new model by parsing and validating input data from keyword arguments.</p> <p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p> <p>source</p>"},{"location":"003_AsyncAPI/#apikeylocation","title":"APIKeyLocation","text":"<pre><code> APIKeyLocation (value, names=None, module=None, qualname=None, type=None,\n                 start=1)\n</code></pre> <p>An enumeration.</p> <p>source</p>"},{"location":"003_AsyncAPI/#securitytype","title":"SecurityType","text":"<pre><code> SecurityType (value, names=None, module=None, qualname=None, type=None,\n               start=1)\n</code></pre> <p>An enumeration.</p> <pre><code>sec_schema = SecuritySchema(type=\"plain\")\npprint(sec_schema)\n\nactual = sec_schema.json()\nprint(f\"JSON={actual}\")\nassert actual == '{\"type\": \"plain\"}', actual\n\nactual = SecuritySchema.parse_raw(sec_schema.json())\npprint(actual)\nassert actual == sec_schema\n</code></pre> <pre>SecuritySchema(\n\u2502   security_type=&lt;SecurityType.plain: 'plain'&gt;,\n\u2502   description=None,\n\u2502   name=None,\n\u2502   api_key_loc=None,\n\u2502   scheme=None,\n\u2502   bearerFormat=None,\n\u2502   flows=None,\n\u2502   openIdConnectUrl=None\n)\n</pre> <pre><code>JSON={\"type\": \"plain\"}\n</code></pre> <pre>SecuritySchema(\n\u2502   security_type=&lt;SecurityType.plain: 'plain'&gt;,\n\u2502   description=None,\n\u2502   name=None,\n\u2502   api_key_loc=None,\n\u2502   scheme=None,\n\u2502   bearerFormat=None,\n\u2502   flows=None,\n\u2502   openIdConnectUrl=None\n)\n</pre> <p>source</p>"},{"location":"003_AsyncAPI/#kafkabroker","title":"KafkaBroker","text":"<pre><code> KafkaBroker (url:str, description:str='Kafka broker', port:str='9092',\n              protocol:str='kafka',\n              security:Optional[__main__.SecuritySchema]=None)\n</code></pre> <p>Kafka broker</p> <pre><code>kafka_broker = KafkaBroker(url=\"kafka\")\npprint(kafka_broker)\n\nexpected = '{\"url\": \"kafka\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}'\nprint(kafka_broker.json())\nassert kafka_broker.json() == expected\n\n# serialization/deserialization test\nactual = KafkaBroker.parse_raw(kafka_broker.json())\nassert actual == kafka_broker\n</code></pre> <pre>KafkaBroker(url='kafka', description='Kafka broker', port='9092', protocol='kafka', security=None)\n</pre> <pre><code>{\"url\": \"kafka\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}\n</code></pre> <pre><code>sec_kafka_broker = KafkaBroker(\n    url=\"kafka\", protocol=\"kafka-secure\", security=SecuritySchema(type=\"plain\")\n)\npprint(sec_kafka_broker)\n\nexpected = '{\"url\": \"kafka\", \"description\": \"Kafka broker\", \"protocol\": \"kafka-secure\", \"security\": {\"type\": \"plain\"}, \"variables\": {\"port\": {\"default\": \"9092\"}}}'\nactual = sec_kafka_broker.json()\nprint(f\"JSON={actual}\")\nassert actual == expected\n\n# serialization/deserialization test\nactual = KafkaBroker.parse_raw(sec_kafka_broker.json())\nassert actual == sec_kafka_broker\n</code></pre> <pre>KafkaBroker(\n\u2502   url='kafka',\n\u2502   description='Kafka broker',\n\u2502   port='9092',\n\u2502   protocol='kafka-secure',\n\u2502   security=SecuritySchema(\n\u2502   \u2502   security_type=&lt;SecurityType.plain: 'plain'&gt;,\n\u2502   \u2502   description=None,\n\u2502   \u2502   name=None,\n\u2502   \u2502   api_key_loc=None,\n\u2502   \u2502   scheme=None,\n\u2502   \u2502   bearerFormat=None,\n\u2502   \u2502   flows=None,\n\u2502   \u2502   openIdConnectUrl=None\n\u2502   )\n)\n</pre> <pre><code>JSON={\"url\": \"kafka\", \"description\": \"Kafka broker\", \"protocol\": \"kafka-secure\", \"security\": {\"type\": \"plain\"}, \"variables\": {\"port\": {\"default\": \"9092\"}}}\n</code></pre> <p>source</p>"},{"location":"003_AsyncAPI/#kafkaserviceinfo","title":"KafkaServiceInfo","text":"<pre><code> KafkaServiceInfo (title:str='Title', version:str='0.0.1',\n                   description:str='Description of the service',\n                   contact:__main__.ContactInfo)\n</code></pre> <p>Create a new model by parsing and validating input data from keyword arguments.</p> <p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p> <p>source</p>"},{"location":"003_AsyncAPI/#contactinfo","title":"ContactInfo","text":"<pre><code> ContactInfo (name:str, url:pydantic.networks.HttpUrl, email:str)\n</code></pre> <p>Create a new model by parsing and validating input data from keyword arguments.</p> <p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p> <pre><code>my_contact = ContactInfo(\n    name=\"ACME\", url=\"https://www.acme.com\", email=\"noreply@acme.com\"\n)\nservice_info = KafkaServiceInfo(contact=my_contact)\npprint(service_info)\n</code></pre> <pre>KafkaServiceInfo(\n\u2502   title='Title',\n\u2502   version='0.0.1',\n\u2502   description='Description of the service',\n\u2502   contact=ContactInfo(name='ACME', url=HttpUrl('https://www.acme.com', ), email='noreply@acme.com')\n)\n</pre> <p>source</p>"},{"location":"003_AsyncAPI/#kafkabrokers","title":"KafkaBrokers","text":"<pre><code> KafkaBrokers (brokers:Dict[str,__main__.KafkaBroker])\n</code></pre> <p>Create a new model by parsing and validating input data from keyword arguments.</p> <p>Raises ValidationError if the input data cannot be parsed to form a valid model.</p> <pre><code>kafka_brokers = KafkaBrokers(brokers={\"dev\": kafka_broker, \"staging\": sec_kafka_broker})\npprint(kafka_brokers)\n\nexpected = '{\"brokers\": {\"dev\": {\"url\": \"kafka\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}, \"staging\": {\"url\": \"kafka\", \"description\": \"Kafka broker\", \"protocol\": \"kafka-secure\", \"security\": {\"type\": \"plain\"}, \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\n\nactual = kafka_brokers.json()\nprint(f\"JSON={actual}\")\nassert actual == expected, actual\n\nactual = KafkaBrokers.parse_raw(kafka_brokers.json())\npprint(actual)\nassert actual == kafka_brokers\n</code></pre> <pre>KafkaBrokers(\n\u2502   brokers={\n\u2502   \u2502   'dev': KafkaBroker(url='kafka', description='Kafka broker', port='9092', protocol='kafka', security=None),\n\u2502   \u2502   'staging': KafkaBroker(\n\u2502   \u2502   \u2502   url='kafka',\n\u2502   \u2502   \u2502   description='Kafka broker',\n\u2502   \u2502   \u2502   port='9092',\n\u2502   \u2502   \u2502   protocol='kafka-secure',\n\u2502   \u2502   \u2502   security=SecuritySchema(\n\u2502   \u2502   \u2502   \u2502   security_type=&lt;SecurityType.plain: 'plain'&gt;,\n\u2502   \u2502   \u2502   \u2502   description=None,\n\u2502   \u2502   \u2502   \u2502   name=None,\n\u2502   \u2502   \u2502   \u2502   api_key_loc=None,\n\u2502   \u2502   \u2502   \u2502   scheme=None,\n\u2502   \u2502   \u2502   \u2502   bearerFormat=None,\n\u2502   \u2502   \u2502   \u2502   flows=None,\n\u2502   \u2502   \u2502   \u2502   openIdConnectUrl=None\n\u2502   \u2502   \u2502   )\n\u2502   \u2502   )\n\u2502   }\n)\n</pre> <pre><code>JSON={\"brokers\": {\"dev\": {\"url\": \"kafka\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}, \"staging\": {\"url\": \"kafka\", \"description\": \"Kafka broker\", \"protocol\": \"kafka-secure\", \"security\": {\"type\": \"plain\"}, \"variables\": {\"port\": {\"default\": \"9092\"}}}}}\n</code></pre> <pre>KafkaBrokers(\n\u2502   brokers={\n\u2502   \u2502   'dev': KafkaBroker(url='kafka', description='Kafka broker', port='9092', protocol='kafka', security=None),\n\u2502   \u2502   'staging': KafkaBroker(\n\u2502   \u2502   \u2502   url='kafka',\n\u2502   \u2502   \u2502   description='Kafka broker',\n\u2502   \u2502   \u2502   port='9092',\n\u2502   \u2502   \u2502   protocol='kafka-secure',\n\u2502   \u2502   \u2502   security=SecuritySchema(\n\u2502   \u2502   \u2502   \u2502   security_type=&lt;SecurityType.plain: 'plain'&gt;,\n\u2502   \u2502   \u2502   \u2502   description=None,\n\u2502   \u2502   \u2502   \u2502   name=None,\n\u2502   \u2502   \u2502   \u2502   api_key_loc=None,\n\u2502   \u2502   \u2502   \u2502   scheme=None,\n\u2502   \u2502   \u2502   \u2502   bearerFormat=None,\n\u2502   \u2502   \u2502   \u2502   flows=None,\n\u2502   \u2502   \u2502   \u2502   openIdConnectUrl=None\n\u2502   \u2502   \u2502   )\n\u2502   \u2502   )\n\u2502   }\n)\n</pre> <pre><code>brokers_json = '{\"brokers\": {\"dev\": {\"url\": \"kafka\", \"description\": \"Kafka broker\", \"protocol\": \"kafka\", \"variables\": {\"port\": {\"default\": \"9092\"}}}, \"staging\": {\"url\": \"kafka\", \"description\": \"Kafka broker\", \"protocol\": \"kafka-secure\", \"security\": {\"type\": \"plain\"}, \"variables\": {\"port\": {\"default\": \"9092\"}}}}}'\nkafka_brokers = KafkaBrokers.parse_raw(brokers_json)\npprint(kafka_brokers)\n\nmy_contact = ContactInfo(\n    name=\"ACME\", url=\"https://www.acme.com\", email=\"noreply@acme.com\"\n)\nkafka_service_info = KafkaServiceInfo(contact=my_contact)\npprint(kafka_service_info)\n\n\nclass MyInfo(KafkaMessage):\n    mobile: str = Field(..., example=\"+385987654321\")\n    name: str = Field(..., example=\"James Bond\")\n\n\nclass MyMsgUrl(KafkaMessage):\n    info: MyInfo = Field(..., example=dict(mobile=\"+385987654321\", name=\"James Bond\"))\n    url: HttpUrl = Field(..., example=\"https://sis.gov.uk/agents/007\")\n\n\nclass MyMsgEmail(KafkaMessage):\n    msg_url: MyMsgUrl = Field(\n        ...,\n        example=dict(\n            info=dict(mobile=\"+385987654321\", name=\"James Bond\"),\n            url=\"https://sis.gov.uk/agents/007\",\n        ),\n    )\n    email: EmailStr = Field(..., example=\"agent-007@sis.gov.uk\")\n\n\ndef on_my_topic_one(msg: MyMsgUrl):\n    raise NotImplemented\n\n\nasync def on_my_topic_2(msg: MyMsgEmail):\n    raise NotImplemented\n\n\nasync def to_my_topic_3(msg) -&gt; MyMsgUrl:\n    raise NotImplemented\n\n\nasync def to_my_topic_4(msg) -&gt; MyMsgEmail:\n    raise NotImplemented\n\n\nconsumers = {\"my_topic_1\": on_my_topic_one, \"my_topic_2\": on_my_topic_2}\nproducers = {\"my_topic_3\": to_my_topic_3, \"my_topic_4\": to_my_topic_4}\n\npprint(dict(consumers=consumers, producers=producers))\nassert set(consumers.keys()) == set([\"my_topic_1\", \"my_topic_2\"])\nassert set(producers.keys()) == set([\"my_topic_3\", \"my_topic_4\"])\n\n# print(f\"kafka_service_info={kafka_service_info}\")\n\n# print(f\"kafka_brokers={kafka_brokers}\")\n</code></pre> <pre>KafkaBrokers(\n\u2502   brokers={\n\u2502   \u2502   'dev': KafkaBroker(url='kafka', description='Kafka broker', port='9092', protocol='kafka', security=None),\n\u2502   \u2502   'staging': KafkaBroker(\n\u2502   \u2502   \u2502   url='kafka',\n\u2502   \u2502   \u2502   description='Kafka broker',\n\u2502   \u2502   \u2502   port='9092',\n\u2502   \u2502   \u2502   protocol='kafka-secure',\n\u2502   \u2502   \u2502   security=SecuritySchema(\n\u2502   \u2502   \u2502   \u2502   security_type=&lt;SecurityType.plain: 'plain'&gt;,\n\u2502   \u2502   \u2502   \u2502   description=None,\n\u2502   \u2502   \u2502   \u2502   name=None,\n\u2502   \u2502   \u2502   \u2502   api_key_loc=None,\n\u2502   \u2502   \u2502   \u2502   scheme=None,\n\u2502   \u2502   \u2502   \u2502   bearerFormat=None,\n\u2502   \u2502   \u2502   \u2502   flows=None,\n\u2502   \u2502   \u2502   \u2502   openIdConnectUrl=None\n\u2502   \u2502   \u2502   )\n\u2502   \u2502   )\n\u2502   }\n)\n</pre> <pre>KafkaServiceInfo(\n\u2502   title='Title',\n\u2502   version='0.0.1',\n\u2502   description='Description of the service',\n\u2502   contact=ContactInfo(name='ACME', url=HttpUrl('https://www.acme.com', ), email='noreply@acme.com')\n)\n</pre> <pre>{\n\u2502   'consumers': {\n\u2502   \u2502   'my_topic_1': &lt;function on_my_topic_one at 0x7fb854e53130&gt;,\n\u2502   \u2502   'my_topic_2': &lt;function on_my_topic_2 at 0x7fb854e53010&gt;\n\u2502   },\n\u2502   'producers': {\n\u2502   \u2502   'my_topic_3': &lt;function to_my_topic_3 at 0x7fb854b17400&gt;,\n\u2502   \u2502   'my_topic_4': &lt;function to_my_topic_4 at 0x7fb854b17490&gt;\n\u2502   }\n}\n</pre> <pre><code>expected = MyMsgUrl\nactual = _get_msg_cls_for_producer(to_my_topic_3)\ndisplay(actual)\nassert actual == expected\n</code></pre> <pre><code>__main__.MyMsgUrl\n</code></pre> <pre><code>expected = MyMsgUrl\nactual = _get_msg_cls_for_consumer(on_my_topic_one)\ndisplay(actual)\nassert actual == expected\n</code></pre> <pre><code>__main__.MyMsgUrl\n</code></pre> <pre><code>expected = {\"subscribe\": {\"message\": {\"$ref\": \"#/components/messages/MyMsgEmail\"}}}\n\nactual = _get_topic_dict(on_my_topic_2, \"subscribe\")\npprint(actual)\n\nassert actual == expected\n</code></pre> <pre>{'subscribe': {'message': {'$ref': '#/components/messages/MyMsgEmail'}}}\n</pre> <pre><code>expected = {\"publish\": {\"message\": {\"$ref\": \"#/components/messages/MyMsgEmail\"}}}\n\nactual = _get_topic_dict(to_my_topic_4, \"publish\")\npprint(actual)\n\nassert actual == expected\n</code></pre> <pre>{'publish': {'message': {'$ref': '#/components/messages/MyMsgEmail'}}}\n</pre> <pre><code>expected = {\n    \"my_topic_1\": {\n        \"subscribe\": {\"message\": {\"$ref\": \"#/components/messages/MyMsgUrl\"}}\n    },\n    \"my_topic_2\": {\n        \"subscribe\": {\"message\": {\"$ref\": \"#/components/messages/MyMsgEmail\"}}\n    },\n    \"my_topic_3\": {\"publish\": {\"message\": {\"$ref\": \"#/components/messages/MyMsgUrl\"}}},\n    \"my_topic_4\": {\n        \"publish\": {\"message\": {\"$ref\": \"#/components/messages/MyMsgEmail\"}}\n    },\n}\nactual = _get_channels_schema(consumers, producers)\npprint(actual)\n\nassert actual == expected\n</code></pre> <pre>{\n\u2502   'my_topic_1': {'subscribe': {'message': {'$ref': '#/components/messages/MyMsgUrl'}}},\n\u2502   'my_topic_2': {'subscribe': {'message': {'$ref': '#/components/messages/MyMsgEmail'}}},\n\u2502   'my_topic_3': {'publish': {'message': {'$ref': '#/components/messages/MyMsgUrl'}}},\n\u2502   'my_topic_4': {'publish': {'message': {'$ref': '#/components/messages/MyMsgEmail'}}}\n}\n</pre> <pre><code>expected = {\n    \"definitions\": {\n        \"MyInfo\": {\n            \"title\": \"MyInfo\",\n            \"type\": \"object\",\n            \"properties\": {\n                \"mobile\": {\n                    \"title\": \"Mobile\",\n                    \"example\": \"+385987654321\",\n                    \"type\": \"string\",\n                },\n                \"name\": {\"title\": \"Name\", \"example\": \"James Bond\", \"type\": \"string\"},\n            },\n            \"required\": [\"mobile\", \"name\"],\n        },\n        \"MyMsgUrl\": {\n            \"title\": \"MyMsgUrl\",\n            \"type\": \"object\",\n            \"properties\": {\n                \"info\": {\n                    \"title\": \"Info\",\n                    \"example\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"},\n                    \"allOf\": [{\"$ref\": \"#/definitions/MyInfo\"}],\n                },\n                \"url\": {\n                    \"title\": \"Url\",\n                    \"example\": \"https://sis.gov.uk/agents/007\",\n                    \"minLength\": 1,\n                    \"maxLength\": 2083,\n                    \"format\": \"uri\",\n                    \"type\": \"string\",\n                },\n            },\n            \"required\": [\"info\", \"url\"],\n        },\n        \"MyMsgEmail\": {\n            \"title\": \"MyMsgEmail\",\n            \"type\": \"object\",\n            \"properties\": {\n                \"msg_url\": {\n                    \"title\": \"Msg Url\",\n                    \"example\": {\n                        \"info\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"},\n                        \"url\": \"https://sis.gov.uk/agents/007\",\n                    },\n                    \"allOf\": [{\"$ref\": \"#/definitions/MyMsgUrl\"}],\n                },\n                \"email\": {\n                    \"title\": \"Email\",\n                    \"example\": \"agent-007@sis.gov.uk\",\n                    \"type\": \"string\",\n                    \"format\": \"email\",\n                },\n            },\n            \"required\": [\"msg_url\", \"email\"],\n        },\n    }\n}\n\nmsg_definitions = _get_kafka_msg_definitions(consumers, producers)\nassert msg_definitions == expected\n</code></pre> <pre><code>expected = {\n    \"msg_url\": {\n        \"info\": {\"name\": \"James Bond\", \"mobile\": \"+385987654321\"},\n        \"url\": \"https://sis.gov.uk/agents/007\",\n    },\n    \"email\": \"agent-007@sis.gov.uk\",\n}\n\nactual = _get_example(MyMsgEmail)\npprint(actual)\n\nassert actual == expected\n</code></pre> <pre>{\n\u2502   'msg_url': {\n\u2502   \u2502   'info': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   'url': 'https://sis.gov.uk/agents/007'\n\u2502   },\n\u2502   'email': 'agent-007@sis.gov.uk'\n}\n</pre> <pre><code>expected = {\n    \"MyInfo\": {\n        \"payload\": {\n            \"title\": \"MyInfo\",\n            \"type\": \"object\",\n            \"properties\": {\n                \"mobile\": {\n                    \"title\": \"Mobile\",\n                    \"example\": \"+385987654321\",\n                    \"type\": \"string\",\n                },\n                \"name\": {\"title\": \"Name\", \"example\": \"James Bond\", \"type\": \"string\"},\n            },\n            \"required\": [\"mobile\", \"name\"],\n        }\n    },\n    \"MyMsgUrl\": {\n        \"payload\": {\n            \"title\": \"MyMsgUrl\",\n            \"type\": \"object\",\n            \"properties\": {\n                \"info\": {\n                    \"title\": \"Info\",\n                    \"example\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"},\n                    \"allOf\": [{\"$ref\": \"#/definitions/MyInfo\"}],\n                },\n                \"url\": {\n                    \"title\": \"Url\",\n                    \"example\": \"https://sis.gov.uk/agents/007\",\n                    \"minLength\": 1,\n                    \"maxLength\": 2083,\n                    \"format\": \"uri\",\n                    \"type\": \"string\",\n                },\n            },\n            \"required\": [\"info\", \"url\"],\n            \"example\": {\n                \"info\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"},\n                \"url\": \"https://sis.gov.uk/agents/007\",\n            },\n        }\n    },\n    \"MyMsgEmail\": {\n        \"payload\": {\n            \"title\": \"MyMsgEmail\",\n            \"type\": \"object\",\n            \"properties\": {\n                \"msg_url\": {\n                    \"title\": \"Msg Url\",\n                    \"example\": {\n                        \"info\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"},\n                        \"url\": \"https://sis.gov.uk/agents/007\",\n                    },\n                    \"allOf\": [{\"$ref\": \"#/definitions/MyMsgUrl\"}],\n                },\n                \"email\": {\n                    \"title\": \"Email\",\n                    \"example\": \"agent-007@sis.gov.uk\",\n                    \"type\": \"string\",\n                    \"format\": \"email\",\n                },\n            },\n            \"required\": [\"msg_url\", \"email\"],\n            \"example\": {\n                \"msg_url\": {\n                    \"info\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"},\n                    \"url\": \"https://sis.gov.uk/agents/007\",\n                },\n                \"email\": \"agent-007@sis.gov.uk\",\n            },\n        }\n    },\n}\n\nactual = _get_msg_definitions_with_examples(consumers, producers)\npprint(actual)\nassert actual == expected\n</code></pre> <pre>{\n\u2502   'MyInfo': {\n\u2502   \u2502   'payload': {\n\u2502   \u2502   \u2502   'title': 'MyInfo',\n\u2502   \u2502   \u2502   'type': 'object',\n\u2502   \u2502   \u2502   'properties': {\n\u2502   \u2502   \u2502   \u2502   'mobile': {'title': 'Mobile', 'example': '+385987654321', 'type': 'string'},\n\u2502   \u2502   \u2502   \u2502   'name': {'title': 'Name', 'example': 'James Bond', 'type': 'string'}\n\u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   'required': ['mobile', 'name']\n\u2502   \u2502   }\n\u2502   },\n\u2502   'MyMsgUrl': {\n\u2502   \u2502   'payload': {\n\u2502   \u2502   \u2502   'title': 'MyMsgUrl',\n\u2502   \u2502   \u2502   'type': 'object',\n\u2502   \u2502   \u2502   'properties': {\n\u2502   \u2502   \u2502   \u2502   'info': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'title': 'Info',\n\u2502   \u2502   \u2502   \u2502   \u2502   'example': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   \u2502   \u2502   \u2502   'allOf': [{'$ref': '#/definitions/MyInfo'}]\n\u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   'url': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'title': 'Url',\n\u2502   \u2502   \u2502   \u2502   \u2502   'example': 'https://sis.gov.uk/agents/007',\n\u2502   \u2502   \u2502   \u2502   \u2502   'minLength': 1,\n\u2502   \u2502   \u2502   \u2502   \u2502   'maxLength': 2083,\n\u2502   \u2502   \u2502   \u2502   \u2502   'format': 'uri',\n\u2502   \u2502   \u2502   \u2502   \u2502   'type': 'string'\n\u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   'required': ['info', 'url'],\n\u2502   \u2502   \u2502   'example': {\n\u2502   \u2502   \u2502   \u2502   'info': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   \u2502   \u2502   'url': 'https://sis.gov.uk/agents/007'\n\u2502   \u2502   \u2502   }\n\u2502   \u2502   }\n\u2502   },\n\u2502   'MyMsgEmail': {\n\u2502   \u2502   'payload': {\n\u2502   \u2502   \u2502   'title': 'MyMsgEmail',\n\u2502   \u2502   \u2502   'type': 'object',\n\u2502   \u2502   \u2502   'properties': {\n\u2502   \u2502   \u2502   \u2502   'msg_url': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'title': 'Msg Url',\n\u2502   \u2502   \u2502   \u2502   \u2502   'example': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'info': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'url': 'https://sis.gov.uk/agents/007'\n\u2502   \u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   \u2502   'allOf': [{'$ref': '#/definitions/MyMsgUrl'}]\n\u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   'email': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'title': 'Email',\n\u2502   \u2502   \u2502   \u2502   \u2502   'example': 'agent-007@sis.gov.uk',\n\u2502   \u2502   \u2502   \u2502   \u2502   'type': 'string',\n\u2502   \u2502   \u2502   \u2502   \u2502   'format': 'email'\n\u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   'required': ['msg_url', 'email'],\n\u2502   \u2502   \u2502   'example': {\n\u2502   \u2502   \u2502   \u2502   'msg_url': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'info': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   \u2502   \u2502   \u2502   'url': 'https://sis.gov.uk/agents/007'\n\u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   'email': 'agent-007@sis.gov.uk'\n\u2502   \u2502   \u2502   }\n\u2502   \u2502   }\n\u2502   }\n}\n</pre> <p>expected = { \u201cstaging_default_security\u201d: {\u201ctype\u201d: \u201cplain\u201d}, \u201cproduction_default_security\u201d: {\u201ctype\u201d: \u201cplain\u201d}, }</p> <p>brokers = KafkaBrokers( brokers={ \u201cdev\u201d: KafkaBroker( url=\u201clocalhost\u201d, description=\u201cdev\u201d, port=\u201c9092\u201d, ) } )</p> <p>display(brokers)</p> <p>actual = _get_security_schemes(brokers) pprint(actual) assert actual == expected, actual</p> <pre><code>components = _get_components_schema(consumers, producers, kafka_brokers)\npprint(components)\n</code></pre> <pre>{\n\u2502   'messages': {\n\u2502   \u2502   'MyMsgUrl': {\n\u2502   \u2502   \u2502   'payload': {\n\u2502   \u2502   \u2502   \u2502   'title': 'MyMsgUrl',\n\u2502   \u2502   \u2502   \u2502   'type': 'object',\n\u2502   \u2502   \u2502   \u2502   'properties': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'info': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'title': 'Info',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'example': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'allOf': [{'$ref': '#/components/schemas/MyInfo'}]\n\u2502   \u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   \u2502   'url': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'title': 'Url',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'example': 'https://sis.gov.uk/agents/007',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'minLength': 1,\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'maxLength': 2083,\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'format': 'uri',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'type': 'string'\n\u2502   \u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   'required': ['info', 'url'],\n\u2502   \u2502   \u2502   \u2502   'example': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'info': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   \u2502   \u2502   \u2502   'url': 'https://sis.gov.uk/agents/007'\n\u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   }\n\u2502   \u2502   },\n\u2502   \u2502   'MyMsgEmail': {\n\u2502   \u2502   \u2502   'payload': {\n\u2502   \u2502   \u2502   \u2502   'title': 'MyMsgEmail',\n\u2502   \u2502   \u2502   \u2502   'type': 'object',\n\u2502   \u2502   \u2502   \u2502   'properties': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'msg_url': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'title': 'Msg Url',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'example': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'info': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'url': 'https://sis.gov.uk/agents/007'\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'allOf': [{'$ref': '#/components/messages/MyMsgUrl'}]\n\u2502   \u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   \u2502   'email': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'title': 'Email',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'example': 'agent-007@sis.gov.uk',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'type': 'string',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'format': 'email'\n\u2502   \u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   'required': ['msg_url', 'email'],\n\u2502   \u2502   \u2502   \u2502   'example': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'msg_url': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'info': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'url': 'https://sis.gov.uk/agents/007'\n\u2502   \u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   \u2502   'email': 'agent-007@sis.gov.uk'\n\u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   }\n\u2502   \u2502   }\n\u2502   },\n\u2502   'schemas': {\n\u2502   \u2502   'MyInfo': {\n\u2502   \u2502   \u2502   'payload': {\n\u2502   \u2502   \u2502   \u2502   'title': 'MyInfo',\n\u2502   \u2502   \u2502   \u2502   'type': 'object',\n\u2502   \u2502   \u2502   \u2502   'properties': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'mobile': {'title': 'Mobile', 'example': '+385987654321', 'type': 'string'},\n\u2502   \u2502   \u2502   \u2502   \u2502   'name': {'title': 'Name', 'example': 'James Bond', 'type': 'string'}\n\u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   'required': ['mobile', 'name']\n\u2502   \u2502   \u2502   }\n\u2502   \u2502   }\n\u2502   },\n\u2502   'securitySchemes': {'staging_default_security': {'type': 'plain'}}\n}\n</pre> <pre><code>expected = {\n    \"dev\": {\n        \"url\": \"kafka\",\n        \"description\": \"Kafka broker\",\n        \"protocol\": \"kafka\",\n        \"variables\": {\"port\": {\"default\": \"9092\"}},\n    },\n    \"staging\": {\n        \"url\": \"kafka\",\n        \"description\": \"Kafka broker\",\n        \"protocol\": \"kafka-secure\",\n        \"security\": [{\"staging_default_security\": []}],\n        \"variables\": {\"port\": {\"default\": \"9092\"}},\n    },\n}\n\nactual = _get_servers_schema(kafka_brokers)\npprint(actual)\nassert actual == expected, actual\n</code></pre> <pre>{\n\u2502   'dev': {\n\u2502   \u2502   'url': 'kafka',\n\u2502   \u2502   'description': 'Kafka broker',\n\u2502   \u2502   'protocol': 'kafka',\n\u2502   \u2502   'variables': {'port': {'default': '9092'}}\n\u2502   },\n\u2502   'staging': {\n\u2502   \u2502   'url': 'kafka',\n\u2502   \u2502   'description': 'Kafka broker',\n\u2502   \u2502   'protocol': 'kafka-secure',\n\u2502   \u2502   'security': [{'staging_default_security': []}],\n\u2502   \u2502   'variables': {'port': {'default': '9092'}}\n\u2502   }\n}\n</pre> <pre><code>expected = {\n    \"asyncapi\": \"2.5.0\",\n    \"info\": {\n        \"title\": \"Title\",\n        \"version\": \"0.0.1\",\n        \"description\": \"Description of the service\",\n        \"contact\": {\n            \"name\": \"ACME\",\n            \"url\": \"https://www.acme.com\",\n            \"email\": \"noreply@acme.com\",\n        },\n    },\n    \"servers\": {\n        \"dev\": {\n            \"url\": \"kafka\",\n            \"description\": \"Kafka broker\",\n            \"protocol\": \"kafka\",\n            \"variables\": {\"port\": {\"default\": \"9092\"}},\n        },\n        \"staging\": {\n            \"url\": \"kafka\",\n            \"description\": \"Kafka broker\",\n            \"protocol\": \"kafka-secure\",\n            \"security\": [{\"staging_default_security\": []}],\n            \"variables\": {\"port\": {\"default\": \"9092\"}},\n        },\n    },\n    \"channels\": {\n        \"my_topic_1\": {\n            \"subscribe\": {\"message\": {\"$ref\": \"#/components/messages/MyMsgUrl\"}}\n        },\n        \"my_topic_2\": {\n            \"subscribe\": {\"message\": {\"$ref\": \"#/components/messages/MyMsgEmail\"}}\n        },\n        \"my_topic_3\": {\n            \"publish\": {\"message\": {\"$ref\": \"#/components/messages/MyMsgUrl\"}}\n        },\n        \"my_topic_4\": {\n            \"publish\": {\"message\": {\"$ref\": \"#/components/messages/MyMsgEmail\"}}\n        },\n    },\n    \"components\": {\n        \"securitySchemes\": {\"staging_default_security\": {\"type\": \"plain\"}},\n        \"messages\": {\n            \"MyMsgUrl\": {\n                \"payload\": {\n                    \"title\": \"MyMsgUrl\",\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"info\": {\n                            \"title\": \"Info\",\n                            \"example\": {\n                                \"mobile\": \"+385987654321\",\n                                \"name\": \"James Bond\",\n                            },\n                            \"allOf\": [{\"$ref\": \"#/components/schemas/MyInfo\"}],\n                        },\n                        \"url\": {\n                            \"title\": \"Url\",\n                            \"example\": \"https://sis.gov.uk/agents/007\",\n                            \"minLength\": 1,\n                            \"maxLength\": 2083,\n                            \"format\": \"uri\",\n                            \"type\": \"string\",\n                        },\n                    },\n                    \"required\": [\"info\", \"url\"],\n                    \"example\": {\n                        \"info\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"},\n                        \"url\": \"https://sis.gov.uk/agents/007\",\n                    },\n                }\n            },\n            \"MyMsgEmail\": {\n                \"payload\": {\n                    \"title\": \"MyMsgEmail\",\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"msg_url\": {\n                            \"title\": \"Msg Url\",\n                            \"example\": {\n                                \"info\": {\n                                    \"mobile\": \"+385987654321\",\n                                    \"name\": \"James Bond\",\n                                },\n                                \"url\": \"https://sis.gov.uk/agents/007\",\n                            },\n                            \"allOf\": [{\"$ref\": \"#/components/messages/MyMsgUrl\"}],\n                        },\n                        \"email\": {\n                            \"title\": \"Email\",\n                            \"example\": \"agent-007@sis.gov.uk\",\n                            \"type\": \"string\",\n                            \"format\": \"email\",\n                        },\n                    },\n                    \"required\": [\"msg_url\", \"email\"],\n                    \"example\": {\n                        \"msg_url\": {\n                            \"info\": {\"mobile\": \"+385987654321\", \"name\": \"James Bond\"},\n                            \"url\": \"https://sis.gov.uk/agents/007\",\n                        },\n                        \"email\": \"agent-007@sis.gov.uk\",\n                    },\n                }\n            },\n        },\n        \"schemas\": {\n            \"MyInfo\": {\n                \"payload\": {\n                    \"title\": \"MyInfo\",\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"mobile\": {\n                            \"title\": \"Mobile\",\n                            \"example\": \"+385987654321\",\n                            \"type\": \"string\",\n                        },\n                        \"name\": {\n                            \"title\": \"Name\",\n                            \"example\": \"James Bond\",\n                            \"type\": \"string\",\n                        },\n                    },\n                    \"required\": [\"mobile\", \"name\"],\n                }\n            }\n        },\n    },\n}\nasyncapi_schema = _get_asyncapi_schema(\n    consumers, producers, kafka_brokers, kafka_service_info\n)\npprint(asyncapi_schema)\nassert asyncapi_schema == expected\n</code></pre> <pre>{\n\u2502   'asyncapi': '2.5.0',\n\u2502   'info': {\n\u2502   \u2502   'title': 'Title',\n\u2502   \u2502   'version': '0.0.1',\n\u2502   \u2502   'description': 'Description of the service',\n\u2502   \u2502   'contact': {'name': 'ACME', 'url': 'https://www.acme.com', 'email': 'noreply@acme.com'}\n\u2502   },\n\u2502   'servers': {\n\u2502   \u2502   'dev': {\n\u2502   \u2502   \u2502   'url': 'kafka',\n\u2502   \u2502   \u2502   'description': 'Kafka broker',\n\u2502   \u2502   \u2502   'protocol': 'kafka',\n\u2502   \u2502   \u2502   'variables': {'port': {'default': '9092'}}\n\u2502   \u2502   },\n\u2502   \u2502   'staging': {\n\u2502   \u2502   \u2502   'url': 'kafka',\n\u2502   \u2502   \u2502   'description': 'Kafka broker',\n\u2502   \u2502   \u2502   'protocol': 'kafka-secure',\n\u2502   \u2502   \u2502   'security': [{'staging_default_security': []}],\n\u2502   \u2502   \u2502   'variables': {'port': {'default': '9092'}}\n\u2502   \u2502   }\n\u2502   },\n\u2502   'channels': {\n\u2502   \u2502   'my_topic_1': {'subscribe': {'message': {'$ref': '#/components/messages/MyMsgUrl'}}},\n\u2502   \u2502   'my_topic_2': {'subscribe': {'message': {'$ref': '#/components/messages/MyMsgEmail'}}},\n\u2502   \u2502   'my_topic_3': {'publish': {'message': {'$ref': '#/components/messages/MyMsgUrl'}}},\n\u2502   \u2502   'my_topic_4': {'publish': {'message': {'$ref': '#/components/messages/MyMsgEmail'}}}\n\u2502   },\n\u2502   'components': {\n\u2502   \u2502   'messages': {\n\u2502   \u2502   \u2502   'MyMsgUrl': {\n\u2502   \u2502   \u2502   \u2502   'payload': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'title': 'MyMsgUrl',\n\u2502   \u2502   \u2502   \u2502   \u2502   'type': 'object',\n\u2502   \u2502   \u2502   \u2502   \u2502   'properties': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'info': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'title': 'Info',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'example': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'allOf': [{'$ref': '#/components/schemas/MyInfo'}]\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'url': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'title': 'Url',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'example': 'https://sis.gov.uk/agents/007',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'minLength': 1,\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'maxLength': 2083,\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'format': 'uri',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'type': 'string'\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   \u2502   'required': ['info', 'url'],\n\u2502   \u2502   \u2502   \u2502   \u2502   'example': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'info': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'url': 'https://sis.gov.uk/agents/007'\n\u2502   \u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   'MyMsgEmail': {\n\u2502   \u2502   \u2502   \u2502   'payload': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'title': 'MyMsgEmail',\n\u2502   \u2502   \u2502   \u2502   \u2502   'type': 'object',\n\u2502   \u2502   \u2502   \u2502   \u2502   'properties': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'msg_url': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'title': 'Msg Url',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'example': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'info': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'url': 'https://sis.gov.uk/agents/007'\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'allOf': [{'$ref': '#/components/messages/MyMsgUrl'}]\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'email': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'title': 'Email',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'example': 'agent-007@sis.gov.uk',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'type': 'string',\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'format': 'email'\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   \u2502   'required': ['msg_url', 'email'],\n\u2502   \u2502   \u2502   \u2502   \u2502   'example': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'msg_url': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'info': {'mobile': '+385987654321', 'name': 'James Bond'},\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'url': 'https://sis.gov.uk/agents/007'\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'email': 'agent-007@sis.gov.uk'\n\u2502   \u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   }\n\u2502   \u2502   },\n\u2502   \u2502   'schemas': {\n\u2502   \u2502   \u2502   'MyInfo': {\n\u2502   \u2502   \u2502   \u2502   'payload': {\n\u2502   \u2502   \u2502   \u2502   \u2502   'title': 'MyInfo',\n\u2502   \u2502   \u2502   \u2502   \u2502   'type': 'object',\n\u2502   \u2502   \u2502   \u2502   \u2502   'properties': {\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'mobile': {'title': 'Mobile', 'example': '+385987654321', 'type': 'string'},\n\u2502   \u2502   \u2502   \u2502   \u2502   \u2502   'name': {'title': 'Name', 'example': 'James Bond', 'type': 'string'}\n\u2502   \u2502   \u2502   \u2502   \u2502   },\n\u2502   \u2502   \u2502   \u2502   \u2502   'required': ['mobile', 'name']\n\u2502   \u2502   \u2502   \u2502   }\n\u2502   \u2502   \u2502   }\n\u2502   \u2502   },\n\u2502   \u2502   'securitySchemes': {'staging_default_security': {'type': 'plain'}}\n\u2502   }\n}\n</pre> <p>source</p>"},{"location":"003_AsyncAPI/#yaml_file_cmp","title":"yaml_file_cmp","text":"<pre><code> yaml_file_cmp (file_1:Union[pathlib.Path,str],\n                file_2:Union[pathlib.Path,str])\n</code></pre> <pre><code>with tempfile.TemporaryDirectory() as d:\n    try:\n        asyncapi_path = Path(d).parent / \"003_AsyncAPI\" / \"asyncapi\"\n        if asyncapi_path.exists():\n            shutil.rmtree(asyncapi_path)\n        spec_path = Path(asyncapi_path) / \"spec\" / \"asyncapi.yml\"\n\n        is_spec_built = _generate_async_spec(\n            consumers=consumers,\n            producers=producers,\n            kafka_brokers=kafka_brokers,\n            kafka_service_info=kafka_service_info,\n            spec_path=spec_path,\n            force_rebuild=False,\n        )\n        assert is_spec_built\n        assert (Path(asyncapi_path) / \"spec\" / \"asyncapi.yml\").exists()\n\n        is_spec_built = _generate_async_spec(\n            consumers=consumers,\n            producers=producers,\n            kafka_brokers=kafka_brokers,\n            kafka_service_info=kafka_service_info,\n            spec_path=spec_path,\n            force_rebuild=False,\n        )\n        assert not is_spec_built\n        assert (Path(asyncapi_path) / \"spec\" / \"asyncapi.yml\").exists()\n\n        is_spec_built = _generate_async_spec(\n            consumers=consumers,\n            producers=producers,\n            kafka_brokers=kafka_brokers,\n            kafka_service_info=kafka_service_info,\n            spec_path=spec_path,\n            force_rebuild=True,\n        )\n        assert is_spec_built\n        assert (Path(asyncapi_path) / \"spec\" / \"asyncapi.yml\").exists()\n\n    finally:\n        shutil.rmtree(asyncapi_path)\n</code></pre> <pre><code>[INFO] __main__: Old async specifications at '/tmp/003_AsyncAPI/asyncapi/spec/asyncapi.yml' does not exist.\n[INFO] __main__: New async specifications generated at: '/tmp/003_AsyncAPI/asyncapi/spec/asyncapi.yml'\n[INFO] __main__: Keeping the old async specifications at: '/tmp/003_AsyncAPI/asyncapi/spec/asyncapi.yml'\n[INFO] __main__: New async specifications generated at: '/tmp/003_AsyncAPI/asyncapi/spec/asyncapi.yml'\n</code></pre> <pre><code>with tempfile.TemporaryDirectory() as d:\n    try:\n        asyncapi_path = Path(d).parent / \"003_AsyncAPI\" / \"asyncapi\"\n        if asyncapi_path.exists():\n            shutil.rmtree(asyncapi_path)\n        spec_path = Path(asyncapi_path) / \"spec\" / \"asyncapi.yml\"\n        docs_path = Path(asyncapi_path) / \"docs\"\n\n        is_spec_built = _generate_async_spec(\n            consumers=consumers,\n            producers=producers,\n            kafka_brokers=kafka_brokers,\n            kafka_service_info=kafka_service_info,\n            spec_path=spec_path,\n            force_rebuild=False,\n        )\n\n        _generate_async_docs(\n            spec_path=spec_path,\n            docs_path=docs_path,\n        )\n        assert docs_path.exists()\n    finally:\n        shutil.rmtree(asyncapi_path)\n</code></pre> <pre><code>[INFO] __main__: Old async specifications at '/tmp/003_AsyncAPI/asyncapi/spec/asyncapi.yml' does not exist.\n[INFO] __main__: New async specifications generated at: '/tmp/003_AsyncAPI/asyncapi/spec/asyncapi.yml'\n[INFO] __main__: Async docs generated at '/tmp/003_AsyncAPI/asyncapi/docs'\n[INFO] __main__: Output of '$ npx -y -p @asyncapi/generator ag /tmp/003_AsyncAPI/asyncapi/spec/asyncapi.yml @asyncapi/html-template -o /tmp/003_AsyncAPI/asyncapi/docs --force-write'\n\nDone! \u2728\nCheck out your shiny new generated files at /tmp/003_AsyncAPI/asyncapi/docs.\n</code></pre> <p>source</p>"},{"location":"003_AsyncAPI/#export_async_spec","title":"export_async_spec","text":"<pre><code> export_async_spec (consumers:Dict[str,Callable[[pydantic.main.BaseModel],\n                    Optional[Awaitable[NoneType]]]], producers:Dict[str,Ca\n                    llable[...,Union[Awaitable[pydantic.main.BaseModel],py\n                    dantic.main.BaseModel]]],\n                    kafka_brokers:__main__.KafkaBrokers,\n                    kafka_service_info:__main__.KafkaServiceInfo,\n                    asyncapi_path:Union[pathlib.Path,str],\n                    force_rebuild:bool=False)\n</code></pre> <p>Export async specification to a given path</p> <p>Params: path: path where the specification will be exported. If parent subdirectories do not exist, they will be created.</p> <pre><code>with tempfile.TemporaryDirectory() as d:\n    try:\n        asyncapi_path = Path(d).parent / \"003_AsyncAPI\" / \"asyncapi\"\n        if asyncapi_path.exists():\n            shutil.rmtree(asyncapi_path)\n\n        export_async_spec(\n            consumers=consumers,\n            producers=producers,\n            kafka_brokers=kafka_brokers,\n            kafka_service_info=kafka_service_info,\n            asyncapi_path=asyncapi_path,\n            force_rebuild=False,\n        )\n        #         !ls -al {asyncapi_path}\n        assert (Path(asyncapi_path) / \"spec\" / \"asyncapi.yml\").exists()\n        assert (Path(asyncapi_path) / \"docs\" / \"index.html\").exists()\n    finally:\n        shutil.rmtree(asyncapi_path)\n</code></pre> <pre><code>[INFO] __main__: Old async specifications at '/tmp/003_AsyncAPI/asyncapi/spec/asyncapi.yml' does not exist.\n[INFO] __main__: New async specifications generated at: '/tmp/003_AsyncAPI/asyncapi/spec/asyncapi.yml'\n[INFO] __main__: Async docs generated at '/tmp/003_AsyncAPI/asyncapi/docs'\n[INFO] __main__: Output of '$ npx -y -p @asyncapi/generator ag /tmp/003_AsyncAPI/asyncapi/spec/asyncapi.yml @asyncapi/html-template -o /tmp/003_AsyncAPI/asyncapi/docs --force-write'\n\nDone! \u2728\nCheck out your shiny new generated files at /tmp/003_AsyncAPI/asyncapi/docs.\n</code></pre>"},{"location":"004_CLI/","title":"004 CLI","text":"<pre><code>import os\nfrom contextlib import contextmanager\nfrom tempfile import TemporaryDirectory\n\nimport nbformat\nfrom nbconvert import PythonExporter\nfrom typer.testing import CliRunner\n</code></pre> <pre><code>@contextmanager\ndef cwd(path: Union[str, Path]) -&gt; None:\n    org_cwd = os.getcwd()\n    try:\n        os.chdir(path)\n        yield\n    finally:\n        os.chdir(org_cwd)\n\n\nwith cwd(\"/tmp\"):\n    assert os.getcwd() == \"/tmp\"\nassert os.getcwd() != \"/tmp\"\nos.getcwd()\n</code></pre> <pre><code>'/work/fast-kafka-api/nbs'\n</code></pre> <pre><code>def generate_app_src(out_path: Union[Path, str]) -&gt; None:\n    path = Path(\"099_Test_Service.ipynb\")\n    if not path.exists():\n        path = Path(\"..\") / \"099_Test_Service.ipynb\"\n    if not path.exists():\n        raise ValueError(f\"Path '{path.resolve()}' does not exists.\")\n\n    with open(path, \"r\") as f:\n        notebook = nbformat.reads(f.read(), nbformat.NO_CONVERT)\n        exporter = PythonExporter()\n        source, _ = exporter.from_notebook_node(notebook)\n\n    with open(out_path, \"w\") as f:\n        f.write(source)\n</code></pre> <pre><code>with TemporaryDirectory() as d:\n    generate_app_src((Path(d) / \"main.py\"))\n    !ls -al {d}\n    !cat {d}/main.py | grep @app\n</code></pre> <pre><code>total 20\ndrwx------ 2 davor davor  4096 Jan  9 17:29 .\ndrwxrwxrwt 1 root  root   4096 Jan  9 17:29 ..\n-rw-rw-r-- 1 davor davor 11955 Jan  9 17:29 main.py\n</code></pre>"},{"location":"004_CLI/#importfromstringerror","title":"ImportFromStringError","text":"<p>Common base class for all non-exit exceptions.</p> <pre><code>with TemporaryDirectory() as d:\n    src_path = Path(d) / \"main.py\"\n    generate_app_src(src_path)\n    with cwd(d):\n        rest_app = _import_from_string(f\"{src_path.stem}:rest_app\")\n        kafka_app = _import_from_string(f\"{src_path.stem}:kafka_app\")\n        assert isinstance(rest_app, FastAPI)\n        assert isinstance(kafka_app, FastKafkaAPI)\n</code></pre> <pre><code>[INFO] main: check\n</code></pre> <pre><code>runner = CliRunner()\n</code></pre>"},{"location":"004_CLI/#generate_docs","title":"generate_docs","text":"<pre><code> generate_docs (root_path:str=&lt;typer.models.OptionInfo object at\n                0x7f5a0ed97f70&gt;, app:str=&lt;typer.models.ArgumentInfo object\n                at 0x7f5a0ed97ee0&gt;)\n</code></pre>"},{"location":"004_CLI/#run","title":"run","text":"<pre><code> run (root_path:str=&lt;typer.models.OptionInfo object at 0x7f5a0ed97760&gt;)\n</code></pre> <pre><code>result = runner.invoke(_app, [\"--help\"])\n</code></pre> <pre>                                                                                                                   Usage: root [OPTIONS] COMMAND [ARGS]...                                                                           \n</pre> <pre>\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --install-completion          Install completion for the current shell.                                         \u2502\n\u2502 --show-completion             Show completion for the current shell, to copy it or customize the installation.  \u2502\n\u2502 --help                        Show this message and exit.                                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 generate-docs           Creates documentation for a Fast Kafka API application                                  \u2502\n\u2502 run                     Runs Fast Kafka API application using uvicorn                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre><code>result = runner.invoke(_app, [\"generate-docs\", \"--help\"])\n</code></pre> <pre>                                                                                                                   Usage: root generate-docs [OPTIONS] APP                                                                           \n</pre> <pre> Creates documentation for a Fast Kafka API application                                                            \n\n</pre> <pre>\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    app      TEXT  input in the form of 'path:app', where **path** is the path to a python file and **app** is \u2502\n\u2502                     an object of type **FastKafkaAPI**.                                                         \u2502\n\u2502                     [default: None]                                                                             \u2502\n\u2502                     [required]                                                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --root-path        TEXT  root path under which documentation will be create [default: .]                        \u2502\n\u2502 --help               Show this message and exit.                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre><code>with TemporaryDirectory() as d:\n    src_path = Path(d) / \"main.py\"\n    generate_app_src(src_path)\n    print(f\"{src_path=}\")\n    with cwd(d):\n        !ls -al {d}\n        import_str = f\"{src_path.stem}:kafka_app\"\n\n        result = runner.invoke(_app, [\"generate-docs\", import_str])\n        typer.echo(result.output)\n        assert result.exit_code == 0\n\n        result = runner.invoke(_app, [\"generate-docs\", import_str])\n        typer.echo(result.output)\n        assert result.exit_code == 0\n</code></pre> <pre><code>src_path=PosixPath('/tmp/tmp0cf2sbsw/main.py')\ntotal 20\ndrwx------ 2 davor davor  4096 Jan  9 17:30 .\ndrwxrwxrwt 1 root  root   4096 Jan  9 17:30 ..\n-rw-rw-r-- 1 davor davor 11955 Jan  9 17:30 main.py\n[INFO] fast_kafka_api._components.asyncapi: Old async specifications at '/tmp/tmp0cf2sbsw/asyncapi/spec/asyncapi.yml' does not exist.\n[INFO] fast_kafka_api._components.asyncapi: New async specifications generated at: 'asyncapi/spec/asyncapi.yml'\n[INFO] fast_kafka_api._components.asyncapi: Async docs generated at 'asyncapi/docs'\n[INFO] fast_kafka_api._components.asyncapi: Output of '$ npx -y -p @asyncapi/generator ag asyncapi/spec/asyncapi.yml @asyncapi/html-template -o asyncapi/docs --force-write'\n\nDone! \u2728\nCheck out your shiny new generated files at /tmp/tmp0cf2sbsw/asyncapi/docs.\n\n\n\n[INFO] fast_kafka_api._components.asyncapi: Keeping the old async specifications at: 'asyncapi/spec/asyncapi.yml'\n[INFO] fast_kafka_api._components.asyncapi: Skipping generating async documentation in '/tmp/tmp0cf2sbsw/asyncapi/docs'\n</code></pre> <pre><code>result = runner.invoke(_app, [\"run\", \"--help\"])\n</code></pre> <pre>                                                                                                                   Usage: root run [OPTIONS]                                                                                         \n</pre> <pre> Runs Fast Kafka API application using uvicorn                                                                     \n\n</pre> <pre>\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --root-path        TEXT  [default: .]                                                                           \u2502\n\u2502 --help               Show this message and exit.                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre><code>result = runner.invoke(_app, [\"run\"])\nprint(result.output)\nassert result.return_value != 0\n</code></pre> <pre><code>Unexpected internal error:\n</code></pre>"},{"location":"099_Test_Service/","title":"099 Test Service","text":"<pre><code>import json\nimport os\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom enum import Enum\nfrom os import environ\nfrom pathlib import Path\nfrom typing import *\n\nimport httpx\nimport yaml\nfrom confluent_kafka import Consumer, Producer\nfrom fastapi import FastAPI, Depends, HTTPException, Request, Response, status\nfrom fastapi.openapi.docs import get_redoc_html, get_swagger_ui_html\nfrom fastapi.openapi.utils import get_openapi\nfrom fastapi.responses import FileResponse, RedirectResponse\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom fastapi.staticfiles import StaticFiles\nfrom pydantic import BaseModel, EmailStr, Field, HttpUrl, NonNegativeInt, validator\n\nfrom fast_kafka_api._components.logger import get_logger\nfrom fast_kafka_api.application import (\n    ConsumeCallable,\n    FastKafkaAPI,\n    KafkaMessage,\n    ProduceCallable,\n)\n</code></pre> <pre><code>import asyncio\nimport tempfile\nfrom datetime import timedelta\n\nimport nest_asyncio\nimport uvicorn\nfrom fastapi.testclient import TestClient\nfrom starlette.datastructures import Headers\n</code></pre> <pre><code>logger = get_logger(__name__)\n</code></pre> <pre><code>logger.info(\"check\")\n</code></pre> <pre><code>[INFO] __main__: check\n</code></pre> <pre><code>class ModelType(str, Enum):\n    churn = \"churn\"\n    propensity_to_buy = \"propensity_to_buy\"\n\n\nclass ModelTrainingRequest(BaseModel):\n    AccountId: NonNegativeInt = Field(\n        ..., example=202020, description=\"ID of an account\"\n    )\n    ModelName: ModelType = Field(..., example=\"churn\", description=\"ID of an account\")\n    total_no_of_records: NonNegativeInt = Field(\n        ...,\n        example=1_000_000,\n        description=\"total number of records (rows) to be ingested\",\n    )\n\n\nclass EventData(BaseModel):\n\"\"\"\n    A sequence of events for a fixed account_id\n    \"\"\"\n\n    AccountId: NonNegativeInt = Field(\n        ..., example=202020, description=\"ID of an account\"\n    )\n    Application: Optional[str] = Field(\n        None,\n        example=\"DriverApp\",\n        description=\"Name of the application in case there is more than one for the AccountId\",\n    )\n    DefinitionId: str = Field(\n        ...,\n        example=\"appLaunch\",\n        description=\"name of the event\",\n        min_length=1,\n    )\n    OccurredTime: datetime = Field(\n        ...,\n        example=\"2021-03-28T00:34:08\",\n        description=\"local time of the event\",\n    )\n    OccurredTimeTicks: NonNegativeInt = Field(\n        ...,\n        example=1616891648496,\n        description=\"local time of the event as the number of ticks\",\n    )\n    PersonId: NonNegativeInt = Field(\n        ..., example=12345678, description=\"ID of a person\"\n    )\n\n\nclass RealtimeData(BaseModel):\n    event_data: EventData = Field(\n        ...,\n        example=dict(\n            AccountId=202020,\n            Application=\"DriverApp\",\n            DefinitionId=\"appLaunch\",\n            OccurredTime=\"2021-03-28T00:34:08\",\n            OccurredTimeTicks=1616891648496,\n            PersonId=12345678,\n        ),\n        description=\"realtime event data\",\n    )\n    make_prediction: bool = Field(\n        ..., example=True, description=\"trigger prediction message in prediction topic\"\n    )\n\n\nclass TrainingDataStatus(BaseModel):\n    AccountId: NonNegativeInt = Field(\n        ..., example=202020, description=\"ID of an account\"\n    )\n    no_of_records: NonNegativeInt = Field(\n        ...,\n        example=12_345,\n        description=\"number of records (rows) ingested\",\n    )\n    total_no_of_records: NonNegativeInt = Field(\n        ...,\n        example=1_000_000,\n        description=\"total number of records (rows) to be ingested\",\n    )\n\n\nclass TrainingModelStatus(BaseModel):\n    AccountId: NonNegativeInt = Field(\n        ..., example=202020, description=\"ID of an account\"\n    )\n    current_step: NonNegativeInt = Field(\n        ...,\n        example=0,\n        description=\"number of records (rows) ingested\",\n    )\n    current_step_percentage: float = Field(\n        ...,\n        example=0.21,\n        description=\"the percentage of the current step completed\",\n    )\n    total_no_of_steps: NonNegativeInt = Field(\n        ...,\n        example=1_000_000,\n        description=\"total number of steps for training the model\",\n    )\n\n\nclass ModelMetrics(BaseModel):\n\"\"\"The standard metrics for classification models.\n\n    The most important metrics is AUC for unbalanced classes such as churn. Metrics such as\n    accuracy are not very useful since they are easily maximized by outputting the most common\n    class all the time.\n    \"\"\"\n\n    AccountId: NonNegativeInt = Field(\n        ..., example=202020, description=\"ID of an account\"\n    )\n    Application: Optional[str] = Field(\n        None,\n        example=\"DriverApp\",\n        description=\"Name of the application in case there is more than one for the AccountId\",\n    )\n    timestamp: datetime = Field(\n        ...,\n        example=\"2021-03-28T00:34:08\",\n        description=\"UTC time when the model was trained\",\n    )\n    model_type: ModelType = Field(\n        ...,\n        example=\"churn\",\n        description=\"Name of the model used (churn, propensity to buy)\",\n    )\n    auc: float = Field(\n        ..., example=0.91, description=\"Area under ROC curve\", ge=0.0, le=1.0\n    )\n    f1: float = Field(..., example=0.89, description=\"F-1 score\", ge=0.0, le=1.0)\n    precission: float = Field(\n        ..., example=0.84, description=\"precission\", ge=0.0, le=1.0\n    )\n    recall: float = Field(..., example=0.82, description=\"recall\", ge=0.0, le=1.0)\n    accuracy: float = Field(..., example=0.82, description=\"accuracy\", ge=0.0, le=1.0)\n\n\nclass Prediction(BaseModel):\n    AccountId: NonNegativeInt = Field(\n        ..., example=202020, description=\"ID of an account\"\n    )\n    Application: Optional[str] = Field(\n        None,\n        example=\"DriverApp\",\n        description=\"Name of the application in case there is more than one for the AccountId\",\n    )\n    PersonId: NonNegativeInt = Field(\n        ..., example=12345678, description=\"ID of a person\"\n    )\n    prediction_time: datetime = Field(\n        ...,\n        example=\"2021-03-28T00:34:08\",\n        description=\"UTC time of prediction\",\n    )\n    model_type: ModelType = Field(\n        ...,\n        example=\"churn\",\n        description=\"Name of the model used (churn, propensity to buy)\",\n    )\n    score: float = Field(\n        ...,\n        example=0.4321,\n        description=\"Prediction score (e.g. the probability of churn in the next 28 days)\",\n        ge=0.0,\n        le=1.0,\n    )\n</code></pre> <pre><code>_total_no_of_records = 0\n_no_of_records_received = 0\n\n\ndef create_ws_server(assets_path: Path = Path(\"./assets\")) -&gt; Tuple[FastAPI, FastKafkaAPI]:\n    title = \"Example for FastKafkaAPI\"\n    description = \"A simple example on how to use FastKafkaAPI\"\n    version = \"0.0.1\"\n    openapi_url = \"/openapi.json\"\n    favicon_url = \"/assets/images/favicon.ico\"\n\n    contact = dict(name=\"airt.ai\", url=\"https://airt.ai\", email=\"info@airt.ai\")\n\n    kafka_brokers = {\n        \"localhost\": {\n            \"url\": \"kafka\",\n            \"description\": \"local development kafka\",\n            \"port\": 9092,\n        },\n        \"staging\": {\n            \"url\": \"kafka.staging.acme.com\",\n            \"description\": \"staging kafka\",\n            \"port\": 9092,\n            \"protocol\": \"kafka-secure\",\n            \"security\": {\"type\": \"plain\"},\n        },\n        \"production\": {\n            \"url\": \"kafka.infobip.acme.com\",\n            \"description\": \"production kafka\",\n            \"port\": 9092,\n            \"protocol\": \"kafka-secure\",\n            \"security\": {\"type\": \"plain\"},\n        },\n    }\n\n    kafka_server_url = environ[\"KAFKA_HOSTNAME\"]\n    kafka_server_port = environ[\"KAFKA_PORT\"]\n    kafka_config = {\n        \"bootstrap_servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n        \"group_id\": f\"{kafka_server_url}:{kafka_server_port}_group\",\n        \"auto_offset_reset\": \"earliest\",\n    }\n    if \"KAFKA_API_KEY\" in environ:\n        kafka_config = {\n            **kafka_config,\n            **{\n                \"security_protocol\": \"SASL_SSL\",\n                \"sasl_mechanisms\": \"PLAIN\",\n                \"sasl_username\": environ[\"KAFKA_API_KEY\"],\n                \"sasl_password\": environ[\"KAFKA_API_SECRET\"],\n            },\n        }\n\n    rest_app = FastAPI(\n        title=title,\n        contact=contact,\n        description=description,\n        version=version,\n        docs_url=None,\n        redoc_url=None,\n    )\n\n    kafka_app = FastKafkaAPI(\n        fast_api_app=rest_app,\n        title=title,\n        contact=contact,\n        description=description,\n        version=version,\n        kafka_brokers=kafka_brokers,\n        **kafka_config\n    )\n\n    @rest_app.get(\"/docs\", include_in_schema=False)\n    def overridden_swagger():\n        return get_swagger_ui_html(\n            openapi_url=openapi_url,\n            title=title,\n            swagger_favicon_url=favicon_url,\n        )\n\n    @rest_app.get(\"/redoc\", include_in_schema=False)\n    def overridden_redoc():\n        return get_redoc_html(\n            openapi_url=openapi_url,\n            title=title,\n            redoc_favicon_url=favicon_url,\n        )\n\n    @rest_app.post(\"/from_kafka_start\")\n    async def from_kafka_start(training_request: ModelTrainingRequest):\n        global _total_no_of_records\n        global _no_of_records_received\n\n        _total_no_of_records = training_request.total_no_of_records\n        _no_of_records_received = 0\n\n    @rest_app.get(\"/from_kafka_end\")\n    async def from_kafka_end():\n        pass\n\n    @kafka_app.consumes()  # type: ignore\n    async def on_training_data(msg: EventData):\n        # ToDo: this is not showing up in logs\n        logger.debug(f\"msg={msg}\")\n        global _total_no_of_records\n        global _no_of_records_received\n        _no_of_records_received = _no_of_records_received + 1\n\n        if _no_of_records_received % 100 == 0:\n            training_data_status = TrainingDataStatus(\n                AccountId=EventData.AccountId,\n                no_of_records=_no_of_records_received,\n                total_no_of_records=_total_no_of_records,\n            )\n            app.produce(\"training_data_status\", training_data_status)\n\n    @kafka_app.consumes()  # type: ignore\n    async def on_realitime_data(msg: RealtimeData):\n        pass\n\n    @kafka_app.produces()  # type: ignore\n    async def to_training_data_status(msg: TrainingDataStatus) -&gt; TrainingDataStatus:\n        logger.debug(f\"on_training_data_status(msg={msg}, kafka_msg={kafka_msg})\")\n        return msg\n\n    @kafka_app.produces()  # type: ignore\n    async def to_training_model_status(msg: str) -&gt; TrainingModelStatus:\n        logger.debug(f\"on_training_model_status(msg={msg}, kafka_msg={kafka_msg})\")\n        return TrainingModelStatus()\n\n    @kafka_app.produces()  # type: ignore\n    async def to_model_metrics(msg: ModelMetrics) -&gt; ModelMetrics:\n        logger.debug(f\"on_training_model_status(msg={msg}, kafka_msg={kafka_msg})\")\n        return msg\n\n    @kafka_app.produces()  # type: ignore\n    async def to_prediction(msg: Prediction) -&gt; Prediction:\n        logger.debug(f\"on_realtime_data_status(msg={msg},, kafka_msg={kafka_msg})\")\n        return msg\n\n    return rest_app, kafka_app\n</code></pre> <pre><code>def create_fastapi_app(assets_path: Path = Path(\"../assets\")) -&gt; FastKafkaAPI:\n    assets_path = assets_path.resolve()\n    rest_app, kafka_app = create_ws_server(assets_path=assets_path)\n    return rest_app, kafka_app\n</code></pre> <pre><code># Path(\"/tmp/099_Test_Service\").mkdir(exist_ok=True)\n# os.chdir(\"/tmp/099_Test_Service\")\n</code></pre> <pre><code>assets_path: Path = Path(\"../assets\")\nrest_app, kafka_app = create_fastapi_app(assets_path=assets_path)\n</code></pre> <pre><code>async def start_fastapi_server(\n    host: str = \"0.0.0.0\",\n    port: int = 6006,\n    rest_app=rest_app,\n):\n    config = uvicorn.Config(rest_app, host=\"0.0.0.0\", port=6006, log_level=\"info\")\n    server = uvicorn.Server(config)\n    await server.serve()\n</code></pre> <pre><code># await start_fastapi_server()\n</code></pre> <pre><code>INFO:     Started server process [21039]\nINFO:     Waiting for application startup.\n\n[INFO] fast_kafka_api._components.asyncapi: New async specifications generated at: 'asyncapi/spec/asyncapi.yml'\n[INFO] fast_kafka_api._components.asyncapi: Async docs generated at 'asyncapi/docs'\n[INFO] fast_kafka_api._components.asyncapi: Output of '$ npx -y -p @asyncapi/generator ag asyncapi/spec/asyncapi.yml @asyncapi/html-template -o asyncapi/docs --force-write'\n\nDone! \u2728\nCheck out your shiny new generated files at /work/fast-kafka-api/nbs/asyncapi/docs.\n\n\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting..\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting..\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created.\n\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:6006 (Press CTRL+C to quit)\n\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'realitime_data'})\n[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'realitime_data'}\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'training_data'})\n[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'training_data'}\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n[WARNING] aiokafka.cluster: Topic realitime_data is not available during auto-create initialization\n[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'realitime_data': 0}. \n[WARNING] aiokafka.cluster: Topic training_data is not available during auto-create initialization\n[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'training_data': 0}.\n\nINFO:     Shutting down\nINFO:     Waiting for application shutdown.\n\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n\nINFO:     Application shutdown complete.\nINFO:     Finished server process [21039]\n</code></pre>"},{"location":"999_Test_Utils/","title":"999 Test Utils","text":"<pre><code># allows async calls in notebooks\n\nimport nest_asyncio\n\nnest_asyncio.apply()\n</code></pre> <pre><code>logger = get_logger(__name__, level=20)\nlogger.debug(\"ok\")\n</code></pre> <p>source</p>"},{"location":"999_Test_Utils/#true_after","title":"true_after","text":"<pre><code> true_after (seconds:float)\n</code></pre> <p>Function returning True after a given number of seconds</p> <pre><code>f = true_after(1.1)\nassert not f()\ntime.sleep(1)\nassert not f()\ntime.sleep(0.1)\nassert f()\n</code></pre> <p>source</p>"},{"location":"999_Test_Utils/#create_missing_topics","title":"create_missing_topics","text":"<pre><code> create_missing_topics (admin:confluent_kafka.admin.AdminClient,\n                        topic_names:List[str],\n                        num_partitions:Optional[int]=None,\n                        replication_factor:Optional[int]=None, **kwargs)\n</code></pre> Type Default Details admin AdminClient type: ignore topic_names typing.List[str] num_partitions typing.Optional[int] None replication_factor typing.Optional[int] None kwargs Returns None <pre><code># Check if topics are created\n\nkafka_admin = AdminClient(kafka_config)\ntopics = [\"A\", \"B\", \"C\"]\ncreate_missing_topics(kafka_admin, topics)\n\nexisting_topics = kafka_admin.list_topics().topics.keys()\nassert set([\"A\", \"B\", \"C\"]) &lt;= existing_topics\n\n# Cleanup\n[\n    await asyncio.wrap_future(topic, loop=None)\n    for topic in kafka_admin.delete_topics(topics=topics).values()\n]\n</code></pre> <pre><code>23-01-10 09:11:57.028 [INFO] __main__: create_missing_topics(['A', 'B', 'C']): new_topics = [NewTopic(topic=A,num_partitions=3), NewTopic(topic=B,num_partitions=3), NewTopic(topic=C,num_partitions=3)]\n\n[None, None, None]\n</code></pre> <p>source</p>"},{"location":"999_Test_Utils/#create_testing_topic","title":"create_testing_topic","text":"<pre><code> create_testing_topic (kafka_config:Dict[str,Any], topic_prefix:str,\n                       seed:Optional[int]=None)\n</code></pre> <pre><code>kafka_admin = AdminClient(kafka_config)\n\nwith create_testing_topic(kafka_config, \"my_topic_\", 1) as topic:\n    # Check if topic is created and exists in topic list\n    existing_topics = kafka_admin.list_topics().topics.keys()\n    assert topic in existing_topics\n\n# Check if topic is deleted after exiting context\nexisting_topics = kafka_admin.list_topics().topics.keys()\nassert topic not in existing_topics\n</code></pre> <pre><code>23-01-10 09:11:58.135 [INFO] __main__: create_missing_topics(['my_topic_9167024629']): new_topics = [NewTopic(topic=my_topic_9167024629,num_partitions=3)]\n</code></pre> <p>source</p>"},{"location":"999_Test_Utils/#create_and_fill_testing_topic","title":"create_and_fill_testing_topic","text":"<pre><code> create_and_fill_testing_topic (msgs:List[bytes],\n                                kafka_config:Dict[str,str]={'bootstrap.ser\n                                vers': 'localhost:9092'}, seed:int)\n</code></pre> <pre><code>msgs_sent = 317\nmsgs = [f\"Hello world {i:05d}\".encode(\"utf-8\") for i in range(msgs_sent)]\n\nasync with create_and_fill_testing_topic(msgs, seed=1) as topic:\n    consumer = AIOKafkaConsumer(\n        topic,\n        bootstrap_servers=kafka_config[\"bootstrap.servers\"],\n        auto_offset_reset=\"earliest\",\n        max_poll_records=100,\n    )\n    logger.info(f\"Consumer {consumer} created.\")\n    await consumer.start()\n    logger.info(f\"Consumer {consumer} started.\")\n    is_shutting_down_f = true_after(5)\n    msgs_received = 0\n    try:\n        while True:\n            msgs = await consumer.getmany(timeout_ms=100)\n            for k, v in msgs.items():\n                msgs_received = msgs_received + len(v)\n            if is_shutting_down_f():\n                break\n\n    finally:\n        assert msgs_received == msgs_sent\n        print(f\"Total messages received: {msgs_received}\")\n        await consumer.stop()\n        logger.info(f\"Consumer {consumer} stopped.\")\n</code></pre> <pre><code>23-01-10 09:12:00.202 [INFO] __main__: create_missing_topics(['my_topic_9167024629']): new_topics = [NewTopic(topic=my_topic_9167024629,num_partitions=3)]\n23-01-10 09:12:01.210 [INFO] __main__: Producer &lt;aiokafka.producer.producer.AIOKafkaProducer object&gt; created.\n23-01-10 09:12:01.235 [INFO] __main__: Producer &lt;aiokafka.producer.producer.AIOKafkaProducer object&gt; started.\n23-01-10 09:12:01.271 [INFO] __main__: Sent messages: len(sent_msgs)=317\n23-01-10 09:12:01.273 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_9167024629'})\n23-01-10 09:12:01.278 [INFO] __main__: Consumer &lt;aiokafka.consumer.consumer.AIOKafkaConsumer object&gt; created.\n23-01-10 09:12:01.290 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_9167024629': 3}. \n23-01-10 09:12:01.295 [INFO] __main__: Consumer &lt;aiokafka.consumer.consumer.AIOKafkaConsumer object&gt; started.\nTotal messages received: 317\n23-01-10 09:12:06.361 [INFO] __main__: Consumer &lt;aiokafka.consumer.consumer.AIOKafkaConsumer object&gt; stopped.\n23-01-10 09:12:06.364 [INFO] __main__: Producer &lt;aiokafka.producer.producer.AIOKafkaProducer object&gt; stoped.\n</code></pre> <pre><code># TODO: Send repeatedly?\n</code></pre> <p>source</p>"},{"location":"999_Test_Utils/#nb_safe_seed","title":"nb_safe_seed","text":"<pre><code> nb_safe_seed (s:str)\n</code></pre> <p>Gets a unique seed function for a notebook</p> <p>Params: s: name of the notebook used to initialize the seed function</p> <p>Returns: A unique seed function</p> <pre><code>seed = nb_safe_seed(\"999_test_utils\")\n\nassert seed() == seed(0)\nassert seed() + 1 == seed(1)\n</code></pre> <p>source</p>"},{"location":"999_Test_Utils/#mock_aiokafkaproducer_send","title":"mock_AIOKafkaProducer_send","text":"<pre><code> mock_AIOKafkaProducer_send ()\n</code></pre> <p>Mocks send method of AIOKafkaProducer</p> <p>source</p>"},{"location":"999_Test_Utils/#change_dir","title":"change_dir","text":"<pre><code> change_dir (d:str)\n</code></pre> <pre><code># TODO: tests\nwith TemporaryDirectory() as d:\n    original_wd = os.getcwd()\n    assert original_wd != d\n    with change_dir(d):\n        assert os.getcwd() == d\n    assert os.getcwd() == original_wd\n</code></pre> <p>source</p>"},{"location":"999_Test_Utils/#run_script_and_cancel","title":"run_script_and_cancel","text":"<pre><code> run_script_and_cancel (script:str, script_file:str, cmd:str,\n                        cancel_after:int)\n</code></pre> <pre><code>cmd = \"python3 -m test.py\"\n\n# Check exit code 0\nscript = \"exit(0)\"\n\nexit_code, output = run_script_and_cancel(\n    script=script, script_file=\"test.py\", cmd=cmd, cancel_after=1\n)\n\nassert exit_code == 0\nassert output.decode(\"utf-8\") == \"\"\n\n\n# Check exit code 1\nscript = \"exit(1)\"\n\nexit_code, output = run_script_and_cancel(\n    script=script, script_file=\"test.py\", cmd=cmd, cancel_after=1\n)\n\nassert exit_code == 1\nassert output.decode(\"utf-8\") == \"\"\n\n\n# Check exit code 0 and output to stdout and stderr\nscript = \"\"\"\nimport sys\nsys.stderr.write(\"hello from stderr\\\\n\")\nsys.stderr.flush()\nprint(\"hello, exiting with exit code 0\")\nexit(0)\n\"\"\"\n\nexit_code, output = run_script_and_cancel(\n    script=script, script_file=\"test.py\", cmd=cmd, cancel_after=1\n)\n\nassert exit_code == 0, exit_code\nassert output.decode(\"utf-8\") == \"hello from stderr\\nhello, exiting with exit code 0\\n\", output.decode(\"utf-8\")\n\n\n# Check random exit code and output\nscript = \"\"\"\nprint(\"hello\\\\nexiting with exit code 143\")\nexit(143)\n\"\"\"\n\nexit_code, output = run_script_and_cancel(\n    script=script, script_file=\"test.py\", cmd=cmd, cancel_after=1\n)\n\nassert exit_code == 143\nassert output.decode(\"utf-8\") == \"hello\\nexiting with exit code 143\\n\"\n\nprint(\"ok\")\n</code></pre> <pre><code>ok\n</code></pre>"},{"location":"CHANGELOG/","title":"Release notes","text":""},{"location":"CHANGELOG/#004","title":"0.0.4","text":"<ul> <li>inheritance from FastAPI removed</li> </ul>"},{"location":"CHANGELOG/#003","title":"0.0.3","text":"<p>Documentation polishing and Semgrep scan added</p>"},{"location":"CHANGELOG/#002","title":"0.0.2","text":""},{"location":"CHANGELOG/#bugs-squashed","title":"Bugs Squashed","text":"<ul> <li>Documentation build failing in CI (#21)</li> </ul>"},{"location":"CHANGELOG/#001","title":"0.0.1","text":"<p>Initial release</p>"},{"location":"Logger/","title":"Logger","text":"<p>source</p>"},{"location":"Logger/#get_default_logger_configuration","title":"get_default_logger_configuration","text":"<pre><code> get_default_logger_configuration (level:int=20)\n</code></pre> <p>Return the common configurations for the logger</p> <p>Args: level: Logger level to set</p> <p>Returns: A dict with default logger configuration</p> <p>source</p>"},{"location":"Logger/#supress_timestamps","title":"supress_timestamps","text":"<pre><code> supress_timestamps (flag:bool=True)\n</code></pre> <p>Supress logger timestamp</p> <p>Args: flag: If not set, then the default value True will be used to supress the timestamp from the logger messages</p> <p>Example on how to use <code>get_default_logger_configuration</code> function</p> <pre><code># collapse_output\n\nget_default_logger_configuration()\n</code></pre> <pre><code>{'version': 1,\n 'disable_existing_loggers': False,\n 'formatters': {'standard': {'format': '%(asctime)s.%(msecs)03d [%(levelname)s] %(name)s: %(message)s',\n   'datefmt': '%y-%m-%d %H:%M:%S'}},\n 'handlers': {'default': {'level': 20,\n   'formatter': 'standard',\n   'class': 'logging.StreamHandler',\n   'stream': 'ext://sys.stdout'}},\n 'loggers': {'': {'handlers': ['default'], 'level': 20}}}\n</code></pre> <p>source</p>"},{"location":"Logger/#get_logger","title":"get_logger","text":"<pre><code> get_logger (name:str, level:int=20, add_spaces:bool=True)\n</code></pre> <p>Return the logger class with default logging configuration.</p> <p>Args: name: Pass the name variable as name while calling level: Used to configure logging, default value <code>logging.INFO</code> logs info messages and up. add_spaces:</p> <p>Returns: The logging.Logger class with default/custom logging configuration</p> <pre><code>logger = get_logger(__name__)\nlogger.info(\"hello\")\nlogger = get_logger(__name__)\nlogger.info(\"hello\")\n\n\ndef f():\n    logger.info(\"hello\")\n\n\nf()\n</code></pre> <pre><code>23-01-06 08:28:06.781 [INFO] __main__: hello\n23-01-06 08:28:06.782 [INFO] __main__: hello\n23-01-06 08:28:06.782 [INFO] __main__: hello\n</code></pre> <p>Example on how to use <code>get_logger</code> function</p> <pre><code># collapse_output\n\nlogger = get_logger(__name__)\n\nlogger.debug(\"Debug\")\nlogger.info(\"info\")\nlogger.warning(\"Warning\")\nlogger.error(\"Error\")\nlogger.critical(\"Critical\")\n</code></pre> <pre><code>23-01-06 08:28:06.787 [INFO] __main__: info\n23-01-06 08:28:06.788 [WARNING] __main__: Warning\n23-01-06 08:28:06.789 [ERROR] __main__: Error\n23-01-06 08:28:06.789 [CRITICAL] __main__: Critical\n</code></pre> <pre><code># collapse_output\n\nsupress_timestamps()\nlogger = get_logger(__name__)\n\nlogger.debug(\"Debug\")\nlogger.info(\"info\")\nlogger.warning(\"Warning\")\nlogger.error(\"Error\")\nlogger.critical(\"Critical\")\n</code></pre> <pre><code>[INFO] __main__: info\n[WARNING] __main__: Warning\n[ERROR] __main__: Error\n[CRITICAL] __main__: Critical\n</code></pre> <p>source</p>"},{"location":"Logger/#set_level","title":"set_level","text":"<pre><code> set_level (level:int)\n</code></pre> <p>Set logger level</p> <p>Args: level: Logger level to set</p> <pre><code>level = logging.ERROR\n\nset_level(level)\n\n# Checking if the logger is set back to logging.WARNING in dev mode\nprint(logger.getEffectiveLevel())\nassert logger.getEffectiveLevel() == level\n\nlogger.debug(\"This is a debug message\")\nlogger.info(\"This is an info\")\nlogger.warning(\"This is a warning\")\nlogger.error(\"This is an error\")\n</code></pre> <pre><code>40\n[ERROR] __main__: This is an error\n</code></pre> <pre><code># Reset log level back to info\nlevel = logging.INFO\n\nset_level(level)\nlogger.info(\"something\")\n</code></pre> <pre><code>[INFO] __main__: something\n</code></pre>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>FastKafkaAPI</li> <li>Guides<ul> <li>Intro</li> <li>First Steps</li> </ul> </li> <li>API<ul> <li>fast_kafka_api.application</li> <li>fast_kafka_api.testing</li> </ul> </li> <li>CLI<ul> <li>fast-kafka-api</li> </ul> </li> <li>Releases</li> </ul>"},{"location":"fast_kafka_api_api_docs/fast_kafka_api/application/","title":"fast_kafka_api.application","text":""},{"location":"fast_kafka_api_api_docs/fast_kafka_api/application/#fast_kafka_api.application.FastKafkaAPI","title":"<code> FastKafkaAPI        </code>","text":"Source code in <code>fast_kafka_api/application.py</code> <pre><code>class FastKafkaAPI:\n    @delegates(  # type: ignore\n        _get_func_with_combined_sig(\n            [AIOKafkaConsumer.__init__, AIOKafkaProducer.__init__]\n        )\n    )\n    def __init__(\n        self,\n        fast_api_app: FastAPI,\n        *,\n        asyncapi_route: Optional[str] = \"/asyncapi\",\n        title: Optional[str] = None,\n        description: Optional[str] = None,\n        version: Optional[str] = None,\n        contact: Optional[Dict[str, str]] = None,\n        kafka_brokers: Optional[Dict[str, Any]] = None,\n        root_path: Optional[Union[Path, str]] = None,\n        **kwargs,\n    ):\n\"\"\"Combined REST and Kafka service\n\n        Params:\n            fast_api_app: the FastAPI app, if None, one will be created\n            asyncapi_route: the route to where to mount the documentation. If **None**, the docs will not be mounted.\n            title: optional title for the documentation. If None, the title of passed fast_api_app will be used\n            description: optional description for the documentation. If None, the description of passed fast_api_app will be used\n            version: optional version for the documentation. If None, the version of passed fast_api_app will be used\n            contact: optional contact for the documentation. If None, the contact of passed fast_api_app will be used\n            kafka_brokers: dictionary describing kafka brokers used for generating documentation\n            root_path: path to where documentation will be created\n        \"\"\"\n        self._fast_api_app = fast_api_app\n\n        # this is neede for documentation generation\n        self._title = title if title else fast_api_app.title\n        self._description = description if description else fast_api_app.description\n        self._version = version if version else fast_api_app.version\n        if contact is not None:\n            self._contact_info = _get_contact_info(**contact)\n        elif fast_api_app.contact is not None:\n            if isinstance(fast_api_app.contact, str):\n                self._contact_info = _get_contact_info(name=fast_api_app.contact)\n            else:\n                self._contact_info = _get_contact_info(**fast_api_app.contact)\n        else:\n            self._contact_info = _get_contact_info()\n\n        self._kafka_service_info = KafkaServiceInfo(\n            title=self._title,\n            version=self._version,\n            description=self._description,\n            contact=self._contact_info,\n        )\n        self._kafka_brokers = _get_kafka_brokers(kafka_brokers)\n\n        self._root_path = Path(\".\") if root_path is None else Path(root_path)\n\n        # this is used as default parameters for creating AIOProducer and AIOConsumer objects\n        self._kafka_config = _get_kafka_config(**kwargs)\n\n        #\n        self._consumers_store: Dict[str, Tuple[ConsumeCallable, Dict[str, Any]]] = {}\n\n        self._producers_list: List[  # type: ignore\n            Union[AIOKafkaProducer, AIOKafkaProducerManager]\n        ] = []\n        self._producers_store: Dict[  # type: ignore\n            str, Tuple[ProduceCallable, AIOKafkaProducer, Dict[str, Any]]\n        ] = {}\n\n        # background tasks\n        self._scheduled_bg_tasks: List[Callable[..., Coroutine[Any, Any, Any]]] = []\n        self._bg_task_group_generator: Optional[anyio.abc.TaskGroup] = None\n        self._bg_tasks_group: Optional[anyio.abc.TaskGroup]\n\n        # todo: use this for errrors\n        self._on_error_topic: Optional[str] = None\n\n        self._asyncapi_path = self._root_path / \"asyncapi\"\n        (self._asyncapi_path / \"docs\").mkdir(exist_ok=True, parents=True)\n        (self._asyncapi_path / \"spec\").mkdir(exist_ok=True, parents=True)\n        self._fast_api_app.mount(\n            \"/asyncapi\",\n            StaticFiles(directory=self._asyncapi_path / \"docs\"),\n            name=\"asyncapi\",\n        )\n\n        self._is_shutting_down: bool = False\n        self._kafka_consumer_tasks: List[asyncio.Task[Any]] = []\n        self._kafka_producer_tasks: List[asyncio.Task[Any]] = []\n\n        @self._fast_api_app.get(\"/\", include_in_schema=False)\n        def redirect_root_to_asyncapi():\n            return RedirectResponse(\"/asyncapi\")\n\n        @self._fast_api_app.get(\"/asyncapi\", include_in_schema=False)\n        async def redirect_asyncapi_docs():\n            return RedirectResponse(\"/asyncapi/index.html\")\n\n        @self._fast_api_app.get(\"/asyncapi.yml\", include_in_schema=False)\n        async def download_asyncapi_yml():\n            return FileResponse(self._asyncapi_path / \"spec\" / \"asyncapi.yml\")\n\n        @self._fast_api_app.on_event(\"startup\")\n        async def on_startup(app=self):\n            await app._on_startup()\n\n        @self._fast_api_app.on_event(\"shutdown\")\n        async def on_shutdown(app=self):\n            await app._on_shutdown()\n\n    async def _on_startup(self) -&gt; None:\n        raise NotImplementedError\n\n    async def _on_shutdown(self) -&gt; None:\n        raise NotImplementedError\n\n    def consumes(\n        self,\n        topic: Optional[str] = None,\n        *,\n        prefix: str = \"on_\",\n        **kwargs: Dict[str, Any],\n    ) -&gt; ConsumeCallable:\n        raise NotImplementedError\n\n    def produces(  # type: ignore\n        self,\n        topic: Optional[str] = None,\n        *,\n        prefix: str = \"to_\",\n        producer: Optional[AIOKafkaProducer] = None,\n        **kwargs: Dict[str, Any],\n    ) -&gt; ProduceCallable:\n        raise NotImplementedError\n\n    def run_in_background(\n        self,\n    ) -&gt; Callable[[], Any]:\n        raise NotImplementedError\n\n    def _populate_consumers(\n        self,\n        is_shutting_down_f: Callable[[], bool],\n    ) -&gt; None:\n        raise NotImplementedError\n\n    async def _populate_producers(self) -&gt; None:\n        raise NotImplementedError\n\n    def generate_async_spec(self) -&gt; None:\n        raise NotImplementedError\n\n    async def _shutdown_consumers(self) -&gt; None:\n        raise NotImplementedError\n\n    async def _shutdown_producers(self) -&gt; None:\n        raise NotImplementedError\n\n    async def _populate_bg_tasks(self) -&gt; None:\n        raise NotImplementedError\n\n    async def _shutdown_bg_tasks(self) -&gt; None:\n        raise NotImplementedError\n</code></pre>"},{"location":"fast_kafka_api_api_docs/fast_kafka_api/application/#fast_kafka_api.application.FastKafkaAPI.__init__","title":"<code>__init__(self, fast_api_app, *, asyncapi_route='/asyncapi', title=None, description=None, version=None, contact=None, kafka_brokers=None, root_path=None, loop=None, bootstrap_servers='localhost', client_id='aiokafka-0.8.0', group_id=None, key_deserializer=None, value_deserializer=None, fetch_max_wait_ms=500, fetch_max_bytes=52428800, fetch_min_bytes=1, max_partition_fetch_bytes=1048576, request_timeout_ms=40000, retry_backoff_ms=100, auto_offset_reset='latest', enable_auto_commit=True, auto_commit_interval_ms=5000, check_crcs=True, metadata_max_age_ms=300000, partition_assignment_strategy=(&lt;class 'kafka.coordinator.assignors.roundrobin.RoundRobinPartitionAssignor'&gt;,), max_poll_interval_ms=300000, rebalance_timeout_ms=None, session_timeout_ms=10000, heartbeat_interval_ms=3000, consumer_timeout_ms=200, max_poll_records=None, ssl_context=None, security_protocol='PLAINTEXT', api_version='auto', exclude_internal_topics=True, connections_max_idle_ms=540000, isolation_level='read_uncommitted', sasl_mechanism='PLAIN', sasl_plain_password=None, sasl_plain_username=None, sasl_kerberos_service_name='kafka', sasl_kerberos_domain_name=None, sasl_oauth_token_provider=None, acks=&lt;object object at 0x7ff4e36ab2c0&gt;, key_serializer=None, value_serializer=None, compression_type=None, max_batch_size=16384, partitioner=&lt;kafka.partitioner.default.DefaultPartitioner object at 0x7ff4e35398e0&gt;, max_request_size=1048576, linger_ms=0, send_backoff_ms=100, enable_idempotence=False, transactional_id=None, transaction_timeout_ms=60000)</code>  <code>special</code>","text":"<p>Combined REST and Kafka service</p> <p>Parameters:</p> Name Type Description Default <code>fast_api_app</code> <code>FastAPI</code> <p>the FastAPI app, if None, one will be created</p> required <code>asyncapi_route</code> <code>Optional[str]</code> <p>the route to where to mount the documentation. If None, the docs will not be mounted.</p> <code>'/asyncapi'</code> <code>title</code> <code>Optional[str]</code> <p>optional title for the documentation. If None, the title of passed fast_api_app will be used</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>optional description for the documentation. If None, the description of passed fast_api_app will be used</p> <code>None</code> <code>version</code> <code>Optional[str]</code> <p>optional version for the documentation. If None, the version of passed fast_api_app will be used</p> <code>None</code> <code>contact</code> <code>Optional[Dict[str, str]]</code> <p>optional contact for the documentation. If None, the contact of passed fast_api_app will be used</p> <code>None</code> <code>kafka_brokers</code> <code>Optional[Dict[str, Any]]</code> <p>dictionary describing kafka brokers used for generating documentation</p> <code>None</code> <code>root_path</code> <code>Union[pathlib.Path, str]</code> <p>path to where documentation will be created</p> <code>None</code> Source code in <code>fast_kafka_api/application.py</code> <pre><code>@delegates(  # type: ignore\n    _get_func_with_combined_sig(\n        [AIOKafkaConsumer.__init__, AIOKafkaProducer.__init__]\n    )\n)\ndef __init__(\n    self,\n    fast_api_app: FastAPI,\n    *,\n    asyncapi_route: Optional[str] = \"/asyncapi\",\n    title: Optional[str] = None,\n    description: Optional[str] = None,\n    version: Optional[str] = None,\n    contact: Optional[Dict[str, str]] = None,\n    kafka_brokers: Optional[Dict[str, Any]] = None,\n    root_path: Optional[Union[Path, str]] = None,\n    **kwargs,\n):\n\"\"\"Combined REST and Kafka service\n\n    Params:\n        fast_api_app: the FastAPI app, if None, one will be created\n        asyncapi_route: the route to where to mount the documentation. If **None**, the docs will not be mounted.\n        title: optional title for the documentation. If None, the title of passed fast_api_app will be used\n        description: optional description for the documentation. If None, the description of passed fast_api_app will be used\n        version: optional version for the documentation. If None, the version of passed fast_api_app will be used\n        contact: optional contact for the documentation. If None, the contact of passed fast_api_app will be used\n        kafka_brokers: dictionary describing kafka brokers used for generating documentation\n        root_path: path to where documentation will be created\n    \"\"\"\n    self._fast_api_app = fast_api_app\n\n    # this is neede for documentation generation\n    self._title = title if title else fast_api_app.title\n    self._description = description if description else fast_api_app.description\n    self._version = version if version else fast_api_app.version\n    if contact is not None:\n        self._contact_info = _get_contact_info(**contact)\n    elif fast_api_app.contact is not None:\n        if isinstance(fast_api_app.contact, str):\n            self._contact_info = _get_contact_info(name=fast_api_app.contact)\n        else:\n            self._contact_info = _get_contact_info(**fast_api_app.contact)\n    else:\n        self._contact_info = _get_contact_info()\n\n    self._kafka_service_info = KafkaServiceInfo(\n        title=self._title,\n        version=self._version,\n        description=self._description,\n        contact=self._contact_info,\n    )\n    self._kafka_brokers = _get_kafka_brokers(kafka_brokers)\n\n    self._root_path = Path(\".\") if root_path is None else Path(root_path)\n\n    # this is used as default parameters for creating AIOProducer and AIOConsumer objects\n    self._kafka_config = _get_kafka_config(**kwargs)\n\n    #\n    self._consumers_store: Dict[str, Tuple[ConsumeCallable, Dict[str, Any]]] = {}\n\n    self._producers_list: List[  # type: ignore\n        Union[AIOKafkaProducer, AIOKafkaProducerManager]\n    ] = []\n    self._producers_store: Dict[  # type: ignore\n        str, Tuple[ProduceCallable, AIOKafkaProducer, Dict[str, Any]]\n    ] = {}\n\n    # background tasks\n    self._scheduled_bg_tasks: List[Callable[..., Coroutine[Any, Any, Any]]] = []\n    self._bg_task_group_generator: Optional[anyio.abc.TaskGroup] = None\n    self._bg_tasks_group: Optional[anyio.abc.TaskGroup]\n\n    # todo: use this for errrors\n    self._on_error_topic: Optional[str] = None\n\n    self._asyncapi_path = self._root_path / \"asyncapi\"\n    (self._asyncapi_path / \"docs\").mkdir(exist_ok=True, parents=True)\n    (self._asyncapi_path / \"spec\").mkdir(exist_ok=True, parents=True)\n    self._fast_api_app.mount(\n        \"/asyncapi\",\n        StaticFiles(directory=self._asyncapi_path / \"docs\"),\n        name=\"asyncapi\",\n    )\n\n    self._is_shutting_down: bool = False\n    self._kafka_consumer_tasks: List[asyncio.Task[Any]] = []\n    self._kafka_producer_tasks: List[asyncio.Task[Any]] = []\n\n    @self._fast_api_app.get(\"/\", include_in_schema=False)\n    def redirect_root_to_asyncapi():\n        return RedirectResponse(\"/asyncapi\")\n\n    @self._fast_api_app.get(\"/asyncapi\", include_in_schema=False)\n    async def redirect_asyncapi_docs():\n        return RedirectResponse(\"/asyncapi/index.html\")\n\n    @self._fast_api_app.get(\"/asyncapi.yml\", include_in_schema=False)\n    async def download_asyncapi_yml():\n        return FileResponse(self._asyncapi_path / \"spec\" / \"asyncapi.yml\")\n\n    @self._fast_api_app.on_event(\"startup\")\n    async def on_startup(app=self):\n        await app._on_startup()\n\n    @self._fast_api_app.on_event(\"shutdown\")\n    async def on_shutdown(app=self):\n        await app._on_shutdown()\n</code></pre>"},{"location":"fast_kafka_api_api_docs/fast_kafka_api/application/#fast_kafka_api.application.FastKafkaAPI.consumes","title":"<code>consumes(self, topic=None, *, prefix='on_', **kwargs)</code>","text":"<p>Decorator registering the callback called when a message is received in a topic.</p> <p>This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>Optional[str]</code> <p>Kafka topic that the consumer will subscribe to and execute the decorated function when it receives a message from the topic, default: None If the topic is not specified, topic name will be inferred from the decorated function name by stripping the defined prefix</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Prefix stripped from the decorated function to define a topic name if the topic argument is not passed, default: \"on_\" If the decorated function name is not prefixed with the defined prefix and topic argument is not passed, then this method will throw ValueError</p> <code>'on_'</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Keyword arguments that will be passed to AIOKafkaConsumer, used to configure the consumer</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable[[Callable[[pydantic.main.BaseModel], Optional[Awaitable[NoneType]]]], Callable[[pydantic.main.BaseModel], Optional[Awaitable[NoneType]]]]</code> <p>A function returning the same function</p> <p>Throws</p> <p>ValueError</p> Source code in <code>fast_kafka_api/application.py</code> <pre><code>@patch  # type: ignore\ndef consumes(\n    self: FastKafkaAPI,\n    topic: Optional[str] = None,\n    *,\n    prefix: str = \"on_\",\n    **kwargs: Dict[str, Any],\n) -&gt; Callable[[ConsumeCallable], ConsumeCallable]:\n\"\"\"Decorator registering the callback called when a message is received in a topic.\n\n    This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.\n\n    Params:\n        topic: Kafka topic that the consumer will subscribe to and execute the decorated function when it receives a message from the topic, default: None\n            If the topic is not specified, topic name will be inferred from the decorated function name by stripping the defined prefix\n        prefix: Prefix stripped from the decorated function to define a topic name if the topic argument is not passed, default: \"on_\"\n            If the decorated function name is not prefixed with the defined prefix and topic argument is not passed, then this method will throw ValueError\n        **kwargs: Keyword arguments that will be passed to AIOKafkaConsumer, used to configure the consumer\n\n    Returns:\n        A function returning the same function\n\n    Throws:\n        ValueError\n\n    \"\"\"\n\n    def _decorator(\n        on_topic: ConsumeCallable,\n        topic: Optional[str] = topic,\n        kwargs: Dict[str, Any] = kwargs,\n    ) -&gt; ConsumeCallable:\n        topic_resolved: str = (\n            _get_topic_name(topic_callable=on_topic, prefix=prefix)\n            if topic is None\n            else topic\n        )\n\n        self._consumers_store[topic_resolved] = (on_topic, kwargs)\n\n        return on_topic\n\n    return _decorator\n</code></pre>"},{"location":"fast_kafka_api_api_docs/fast_kafka_api/application/#fast_kafka_api.application.FastKafkaAPI.produces","title":"<code>produces(self, topic=None, *, prefix='to_', producer=None, **kwargs)</code>","text":"<p>Decorator registering the callback called when delivery report for a produced message is received</p> <p>This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>Optional[str]</code> <p>Kafka topic that the producer will send returned values from the decorated function to, default: None If the topic is not specified, topic name will be inferred from the decorated function name by stripping the defined prefix</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Prefix stripped from the decorated function to define a topic name if the topic argument is not passed, default: \"to_\" If the decorated function name is not prefixed with the defined prefix and topic argument is not passed, then this method will throw ValueError</p> <code>'to_'</code> <code>producer</code> <code>AIOKafkaProducer</code> <code>None</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Keyword arguments that will be passed to AIOKafkaProducer, used to configure the producer</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable[[Callable[..., Union[Awaitable[pydantic.main.BaseModel], pydantic.main.BaseModel]]], Callable[..., Union[Awaitable[pydantic.main.BaseModel], pydantic.main.BaseModel]]]</code> <p>A function returning the same function</p> <p>Throws</p> <p>ValueError</p> Source code in <code>fast_kafka_api/application.py</code> <pre><code>@patch  # type: ignore\ndef produces(\n    self: FastKafkaAPI,\n    topic: Optional[str] = None,\n    *,\n    prefix: str = \"to_\",\n    producer: AIOKafkaProducer = None,\n    **kwargs: Dict[str, Any],\n) -&gt; Callable[[ProduceCallable], ProduceCallable]:\n\"\"\"Decorator registering the callback called when delivery report for a produced message is received\n\n    This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.\n\n    Params:\n        topic: Kafka topic that the producer will send returned values from the decorated function to, default: None\n            If the topic is not specified, topic name will be inferred from the decorated function name by stripping the defined prefix\n        prefix: Prefix stripped from the decorated function to define a topic name if the topic argument is not passed, default: \"to_\"\n            If the decorated function name is not prefixed with the defined prefix and topic argument is not passed, then this method will throw ValueError\n        producer:\n        **kwargs: Keyword arguments that will be passed to AIOKafkaProducer, used to configure the producer\n\n    Returns:\n        A function returning the same function\n\n    Throws:\n        ValueError\n\n    \"\"\"\n\n    def _decorator(\n        on_topic: ProduceCallable,\n        topic: Optional[str] = topic,\n        kwargs: Dict[str, Any] = kwargs,\n    ) -&gt; ProduceCallable:\n        topic_resolved: str = (\n            _get_topic_name(topic_callable=on_topic, prefix=prefix)\n            if topic is None\n            else topic\n        )\n\n        self._producers_store[topic_resolved] = (on_topic, producer, kwargs)\n\n        return produce_decorator(self, on_topic, topic_resolved)\n\n    return _decorator\n</code></pre>"},{"location":"fast_kafka_api_api_docs/fast_kafka_api/application/#fast_kafka_api.application.FastKafkaAPI.run_in_background","title":"<code>run_in_background(self)</code>","text":"<p>Decorator to schedule a task to be run in the background.</p> <p>This decorator is used to schedule a task to be run in the background when the app's <code>_on_startup</code> event is triggered.</p> <p>Returns:</p> Type Description <code>Callable[None, None]</code> <p>A decorator function that takes a background task as an input and stores it to be run in the backround.</p> Source code in <code>fast_kafka_api/application.py</code> <pre><code>@patch  # type: ignore\ndef run_in_background(\n    self: FastKafkaAPI,\n) -&gt; Callable[\n    [Callable[..., Coroutine[Any, Any, Any]]], Callable[..., Coroutine[Any, Any, Any]]\n]:\n\"\"\"\n    Decorator to schedule a task to be run in the background.\n\n    This decorator is used to schedule a task to be run in the background when the app's `_on_startup` event is triggered.\n\n    Returns:\n        Callable[None, None]: A decorator function that takes a background task as an input and stores it to be run in the backround.\n    \"\"\"\n\n    def _decorator(\n        bg_task: Callable[..., Coroutine[Any, Any, Any]]\n    ) -&gt; Callable[..., Coroutine[Any, Any, Any]]:\n\"\"\"\n        Store the background task.\n\n        Args:\n            bg_task (Callable[[], None]): The background task to be run asynchronously.\n\n        Returns:\n            Callable[[], None]: Original background task.\n        \"\"\"\n        self._scheduled_bg_tasks.append(bg_task)\n\n        return bg_task\n\n    return _decorator\n</code></pre>"},{"location":"fast_kafka_api_api_docs/fast_kafka_api/testing/","title":"fast_kafka_api.testing","text":""},{"location":"fast_kafka_api_api_docs/fast_kafka_api/testing/#fast_kafka_api.testing.mock_AIOKafkaProducer_send","title":"<code>mock_AIOKafkaProducer_send()</code>","text":"<p>Mocks send method of AIOKafkaProducer</p> Source code in <code>fast_kafka_api/testing.py</code> <pre><code>@contextmanager\ndef mock_AIOKafkaProducer_send() -&gt; Generator[unittest.mock.Mock, None, None]:\n\"\"\"Mocks **send** method of **AIOKafkaProducer**\"\"\"\n    with unittest.mock.patch(\"__main__.AIOKafkaProducer.send\") as mock:\n\n        async def _f():\n            pass\n\n        mock.return_value = asyncio.create_task(_f())\n\n        yield mock\n</code></pre>"},{"location":"fast_kafka_api_api_docs/fast_kafka_api/testing/#fast_kafka_api.testing.nb_safe_seed","title":"<code>nb_safe_seed(s)</code>","text":"<p>Gets a unique seed function for a notebook</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>name of the notebook used to initialize the seed function</p> required <p>Returns:</p> Type Description <code>Callable[[int], int]</code> <p>A unique seed function</p> Source code in <code>fast_kafka_api/testing.py</code> <pre><code>def nb_safe_seed(s: str) -&gt; Callable[[int], int]:\n\"\"\"Gets a unique seed function for a notebook\n\n    Params:\n        s: name of the notebook used to initialize the seed function\n\n    Returns:\n        A unique seed function\n    \"\"\"\n    init_seed = int(hashlib.sha256(s.encode(\"utf-8\")).hexdigest(), 16) % (10**8)\n\n    def _get_seed(x: int = 0, *, init_seed: int = init_seed) -&gt; int:\n        return init_seed + x\n\n    return _get_seed\n</code></pre>"},{"location":"fast_kafka_api_api_docs/fast_kafka_api/testing/#fast_kafka_api.testing.true_after","title":"<code>true_after(seconds)</code>","text":"<p>Function returning True after a given number of seconds</p> Source code in <code>fast_kafka_api/testing.py</code> <pre><code>def true_after(seconds: float) -&gt; Callable[[], bool]:\n\"\"\"Function returning True after a given number of seconds\"\"\"\n    t = datetime.now()\n\n    def _true_after(seconds: float = seconds, t: datetime = t) -&gt; bool:\n        return (datetime.now() - t) &gt; timedelta(seconds=seconds)\n\n    return _true_after\n</code></pre>"},{"location":"fast_kafka_api_cli_docs/fast-kafka-api/","title":"<code>fast-kafka-api</code>","text":"<p>Usage:</p> <pre><code>$ fast-kafka-api [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>generate-docs</code>: Creates documentation for a Fast Kafka API...</li> <li><code>run</code>: Runs Fast Kafka API application using uvicorn</li> </ul>"},{"location":"fast_kafka_api_cli_docs/fast-kafka-api/#fast-kafka-api-generate-docs","title":"<code>fast-kafka-api generate-docs</code>","text":"<p>Creates documentation for a Fast Kafka API application </p> <p>Usage:</p> <pre><code>$ fast-kafka-api generate-docs [OPTIONS] APP\n</code></pre> <p>Arguments:</p> <ul> <li><code>APP</code>: input in the form of 'path:app', where path is the path to a python file and app is an object of type FastKafkaAPI.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--root-path TEXT</code>: root path under which documentation will be create  [default: .]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"fast_kafka_api_cli_docs/fast-kafka-api/#fast-kafka-api-run","title":"<code>fast-kafka-api run</code>","text":"<p>Runs Fast Kafka API application using uvicorn</p> <p>Usage:</p> <pre><code>$ fast-kafka-api run [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--root-path TEXT</code>: [default: .]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"guides/Guide_01_Intro/","title":"Intro","text":"<p>This tutorial will show you how to use FastKafkaAPI, step by step.</p> <p>The goal of FastKafkaAPI is to simplify the use of Apache Kafka in Python inspired by FastAPI look and feel.</p> <p>In this Intro tutorial we\u2019ll go trough the basic requirements to run the demos presented in future steps.</p>"},{"location":"guides/Guide_01_Intro/#installing-fastkafkaapi","title":"Installing FastKafkaAPI","text":"<p>First step is to install FastKafkaAPI</p> <pre><code>$ pip install fast-kafka-api\n</code></pre>"},{"location":"guides/Guide_01_Intro/#preparing-a-kafka-broker","title":"Preparing a Kafka broker","text":"<p>Next step is to prepare the Kafka environment, our consumers and producers will need some channel of communication.</p> <p>Hey, your first info!</p> <p>If you already have an instance of Kafka running that you can connect to for demo purposes, feel free to skip this step. </p> <p>To go through the tutorial, we recommend that you use dockerized Kafka brokers, if you have Docker and docker-compose installed the setup should take you no time (if we exclude the container download times).</p> <p>Listen! This is important.</p> <p>To be able to setup this configuration you need to have Docker and docker-compose installed</p> <p>See here for more info on Docker and docker compose</p> <p>To setup the recommended environment, first, create a new folder wher you want to save your demo files (e.g.\u00a0fast_kafka_api_demo). Inside the new folder create a new YAML file named kafka_demo.yml and copy the following configuration into it:</p> <pre><code>version: \"3\"\nservices:\nzookeeper:\nimage: wurstmeister/zookeeper\nhostname: zookeeper\ncontainer_name: zookeeper\nnetworks:\n- fast-kafka-api-network\nports:\n- \"2181:2181\"\n- \"22:22\"\n- \"2888:2888\"\n- \"3888:3888\"\nkafka:\nimage: wurstmeister/kafka\ncontainer_name: kafka\nports:\n- \"9093:9093\"\nenvironment:\nHOSTNAME_COMMAND: \"docker info | grep ^Name: | cut -d' ' -f 2\"\nKAFKA_ZOOKEEPER_CONNECT: \"zookeeper:2181\"\nKAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTER:PLAINTEXT,INSIDE:PLAINTEXT\nKAFKA_ADVERTISED_LISTENERS: INTER://:9092,INSIDE://localhost:9093\nKAFKA_LISTENERS: INTER://_{HOSTNAME_COMMAND}:9092,INSIDE://:9093\nKAFKA_INTER_BROKER_LISTENER_NAME: INTER\nKAFKA_CREATE_TOPICS: \"hello:1:1\"\nvolumes:\n- /var/run/docker.sock:/var/run/docker.sock\ndepends_on:\n- zookeeper\nhealthcheck:\ntest: [ \"CMD\", \"kafka-topics.sh\", \"--list\", \"--zookeeper\", \"zookeeper:2181\" ]\ninterval: 5s\ntimeout: 10s\nretries: 5\nnetworks:\n- fast-kafka-api-network\nnetworks:\nfast-kafka-api-network:\nname: \"fast-kafka-api-network\"\n</code></pre> <p>This configuration will start a single instance of Zookeeper, single instance of Kafka broker and create a \u2018hello\u2019 topic (quite enough for a start). To start the configuration, run:</p> <pre><code>$ docker-compose -f kafka_demo.yaml up -d --wait\n</code></pre> <p>This will start the necessary containers and wait till they report that they are Healthy. After the command finishes, you are good to go to try out the FastKafkaAPI capabilities! </p>"},{"location":"guides/Guide_01_Intro/#running-the-code","title":"Running the code","text":"<p>After installing FastKafkaAPI and initialising the Kafka broker you can proceed to the \u2018First Steps\u2019 part of the tutorial. There, you will write your first Kafka client and producer apps, run them, and interact with them.</p> <p>You are highly encouraged to follow along the tutorials not just by reading trough them but by implementing the code examples in your own environment. This will not only help you remember the use cases better but also, hopefully, demonstrate to you the ease of use of this library.</p>"},{"location":"guides/Guide_02_First_Steps/","title":"First Steps","text":""},{"location":"guides/Guide_02_First_Steps/#creating-a-simple-kafka-consumer-app","title":"Creating a simple Kafka consumer app","text":"<p>For our first demo we will create the simplest possible Kafka consumer and run it using uvicorn.</p> <p>The consumer will:</p> <ol> <li> <p>Connect to the Kafka Broker we setup in the Intro guide</p> </li> <li> <p>Listen to the hello topic</p> </li> <li> <p>Write any message received from the hello topic to stdout</p> </li> </ol> <p>To create the consumer, first, create a file named hello_kafka_consumer.py and copy the following code to it:</p> <pre><code>from os import environ\n\nfrom fastapi import FastAPI\n\nfrom fast_kafka_api.application import FastKafkaAPI\nfrom pydantic import BaseModel, Field\n\nkafka_server_url = environ[\"KAFKA_HOSTNAME\"]\nkafka_server_port = environ[\"KAFKA_PORT\"]\n\nkafka_config = {\n        \"bootstrap_servers\": f\"{kafka_server_url}:{kafka_server_port}\",\n    }\n\nclass HelloKafkaMsg(BaseModel):\n    msg: str = Field(\n        ...,\n        example=\"Hello\",\n        description=\"Demo hello world message\",\n    )\n\napp = FastAPI()\nkafka_app = FastKafkaAPI(\n    fast_api_app=app,\n    **kafka_config,\n)\n\n@app.get(\"/hello\")\nasync def hello() -&gt; str:\n    return \"hello\"\n\n@kafka_app.consumes()\nasync def on_hello(msg: HelloKafkaMsg):\n    print(f\"Got data, msg={msg.msg}\")\n</code></pre> <p>Kafka configuration</p> <p>This consumer script uses KAFKA_HOSTNAME and KAFKA_PORT environment vars, so make sure that you have exported them into your environment before running the following comand (e.g. in shell, for KAFKA_HOSTNAME, run: 'export KAFKA_HOSTNAME=kafka').</p> <p>To run this consumer, in your terminal, run:</p> <pre><code>python3 -m uvicorn hello_kafka_consumer:app --host 0.0.0.0 --port 6006\n</code></pre> <p>After running the command, you should see something similar to the ouput below:</p> <pre><code>INFO:     Started server process [4418]\nINFO:     Waiting for application startup.\n[INFO] fast_kafka_api._components.asyncapi: Old async specifications at '/tmp/tmpyz5_oy_5/asyncapi/spec/asyncapi.yml' does not exist.\n[INFO] fast_kafka_api._components.asyncapi: New async specifications generated at: 'asyncapi/spec/asyncapi.yml'\n[INFO] fast_kafka_api._components.asyncapi: Async docs generated at 'asyncapi/docs'\n[INFO] fast_kafka_api._components.asyncapi: Output of '$ npx -y -p @asyncapi/generator ag asyncapi/spec/asyncapi.yml @asyncapi/html-template -o asyncapi/docs --force-write'\n\nDone! \u2728\nCheck out your shiny new generated files at /tmp/tmpyz5_oy_5/asyncapi/docs.\n\n\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting..\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:6006 (Press CTRL+C to quit)\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n[INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'hello'})\n[INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'hello'}\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n[INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'hello': 1}. \nINFO:     Shutting down\nINFO:     Waiting for application shutdown.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n[INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [4418]\n</code></pre> <p>Now you can interact with your consumer, by sending the messages to the subscribed \u2018hello\u2019 topic, don\u2019t worry, we will cover this in the next step of this guide.</p>"},{"location":"guides/Guide_02_First_Steps/#sending-first-message-to-your-consumer","title":"Sending first message to your consumer","text":"<p>After we have created and run our first consumer, we should send a message to it, to make sure it is working properly.</p> <p>If you are using the Kafka setup as described in the Intro guide, you can follow the steps listed here to send a message to the hello topic.</p> <p>First, connect to your running kafka broker by running:</p> <pre><code>docker run -it kafka /bin/bash\n</code></pre> <p>Then, when connected to the container, run:</p> <pre><code>kafka-console-producer.sh --bootstrap-server=localhost:9092 --topic=hello\n</code></pre> <p>This will open an interactive connection to the hello topic, now you can write your mesages to the topic and they will be consumed by our consumer.</p> <p>In the shell, type:</p> <pre><code>{\"msg\":\"hello\"}\n</code></pre> <p>and press enter. This will send a hello message to the topic which will be read by our running consumer and outputed to stdout.</p> <p>Check the output of your consumer (terminal where you run the uvicorn command) and confirm that your consumer has read the Kafka message. You shoud see something like this:</p> <pre><code>Got data, msg=hello\n</code></pre>"},{"location":"guides/Guide_02_First_Steps/#creating-a-hello-kafka-producer","title":"Creating a hello Kafka producer","text":"<p>Consuming messages is only a part of this Library functionality, the other big part is producing the messages. So, let\u2019s create our first kafka producer which will send it\u2019s greetings to our consumer periodically.</p> <p>The producer will:</p> <ol> <li>Connect to the Kafka Broker we setup in the Intro guide</li> <li>Connect to the hello topic</li> <li>Periodically send a message to the hello world topic</li> </ol> <p>To create the producer, first, create a file named hello_kafka_producer.py and copy the following code to it:</p> <pre><code>from os import environ\n\nfrom fastapi import FastAPI\n\nimport asyncio\nfrom pydantic import BaseModel, Field\n\nfrom fast_kafka_api.application import FastKafkaAPI\nfrom fast_kafka_api._components.logger import get_logger\n\nkafka_server_url = environ[\"KAFKA_HOSTNAME\"]\nkafka_server_port = environ[\"KAFKA_PORT\"]\n\nkafka_config = {\n        \"bootstrap_servers\": f\"{kafka_server_url}:{kafka_server_port}\"\n    }\n\nclass HelloKafkaMsg(BaseModel):\n    msg: str = Field(\n        ...,\n        example=\"Hello\",\n        description=\"Demo hello world message\",\n    )\n\napp = FastAPI()\nkafka_app = FastKafkaAPI(\n    fast_api_app=app,\n    **kafka_config,\n)\n\n@app.get(\"/hello\")\nasync def hello() -&gt; str:\n    return \"hello\"\n\nlogger = get_logger(__name__)\n\n@kafka_app.produces()\nasync def to_hello(msg: HelloKafkaMsg) -&gt; HelloKafkaMsg:\n    logger.info(f\"Producing: {msg}\")\n    return msg\n\n@kafka_app.run_in_background()\nasync def hello_every_second():\n    while(True):\n        await to_hello(HelloKafkaMsg(msg=\"hello\"))\n        await asyncio.sleep(1)\n</code></pre> <p>Kafka configuration</p> <p>This producer script uses KAFKA_HOSTNAME and KAFKA_PORT environment vars, so make sure that you have exported them into your environment before running the following comand (e.g. in shell, for KAFKA_HOSTNAME, run: 'export KAFKA_HOSTNAME=kafka').</p> <p>To run this producer, in your terminal, run:</p> <pre><code>python3 -m uvicorn hello_kafka_producer:app --host 0.0.0.0 --port 6006\n</code></pre> <p>After running the command, you should see something similar to the ouput below:</p> <pre><code>INFO:     Started server process [4498]\nINFO:     Waiting for application startup.\n[INFO] fast_kafka_api._components.asyncapi: Old async specifications at '/tmp/tmp18my68qp/asyncapi/spec/asyncapi.yml' does not exist.\n[INFO] fast_kafka_api._components.asyncapi: New async specifications generated at: 'asyncapi/spec/asyncapi.yml'\n[INFO] fast_kafka_api._components.asyncapi: Async docs generated at 'asyncapi/docs'\n[INFO] fast_kafka_api._components.asyncapi: Output of '$ npx -y -p @asyncapi/generator ag asyncapi/spec/asyncapi.yml @asyncapi/html-template -o asyncapi/docs --force-write'\n\nDone! \u2728\nCheck out your shiny new generated files at /tmp/tmp18my68qp/asyncapi/docs.\n\n\n[INFO] hello_kafka_producer: Producing: msg='hello'\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:6006 (Press CTRL+C to quit)\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\n[INFO] hello_kafka_producer: Producing: msg='hello'\nINFO:     Shutting down\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\nINFO:     Finished server process [4498]\n</code></pre> <p>Now, while the producer is running, it will send a HelloKafkaMsg every second to the hello kafka topic. If your consumer is still running, you should see the messages appear in its log.</p>"},{"location":"guides/Guide_02_First_Steps/#recap","title":"Recap","text":"<p>In this guide we have:</p> <ol> <li>Created a simple Kafka consumer using FastKafkaAPI</li> <li>Sent a message to our consumer trough Kafka</li> <li>Created a simple Kafka producer using FastKafkaAPI</li> </ol>"}]}